{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "this is the implementation of the github repo:\n",
        "https://github.com/mcyhx/PointNetwork_Summarization\n",
        "\n",
        "the code is not updated and i think it was for chinese corpus\n",
        "\n",
        "run on collab some imports changed\n",
        "\n",
        "results:\n",
        "\n",
        "source:  \n",
        "\n",
        "Will: hey babe, what do you want for dinner tonight?\n",
        "Emma:  gah, don't even worry about it tonight\n",
        "Will: what do you mean? everything ok?\n",
        "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
        "Will: Well what time will you be home?\n",
        "Emma: soon, hopefully\n",
        "Will: you sure? Maybe you want me to pick you up?\n",
        "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
        "Will: Alright, love you. \n",
        "Emma: love you too.  \n",
        "\n",
        "target:  Emma will be home soon and she will let Will know. \n",
        "\n",
        "greedy:  <UNK> <UNK> and <UNK> are going to the cinema to see the <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
        "\n",
        "beam:  Jane is going to be home in 5 minutes to go to the cinema to go to the cinema to see the cinema. worry will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. about \n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCRZ7wgzFrfw"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GwiicJs6z97B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Attention, Concatenate, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5J-7-Ga5BozC"
      },
      "outputs": [],
      "source": [
        "username = \"nouralmulhem\"\n",
        "email = \"nouralmulhem@gmail.com\"\n",
        "token = \"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\"\n",
        "repo = \"Soul-AI\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb4vTZocB6P5",
        "outputId": "86fffbe0-433d-4ead-91e4-0fe7a5beeabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Soul-AI'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 102 (delta 39), reused 89 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 16.60 MiB | 14.75 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.name {username}\n",
        "!git config --global user.email {email}\n",
        "!git clone https://{token}@github.com/Better-Call-Soul/{repo}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "oC4riNU20AGP"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Specify the file path\n",
        "file_path1 = '/content/Soul-AI/data/raw/summarization/samsum/samsum-train.csv'\n",
        "file_path2 = '/content/Soul-AI/data/raw/summarization/samsum/samsum-test.csv'\n",
        "file_path3 = '/content/Soul-AI/data/raw/summarization/samsum/samsum-validation.csv'\n",
        "\n",
        "def re_write(input_file, file_name):\n",
        "  dialogs = []\n",
        "  summarizations = []\n",
        "  pair = []\n",
        "\n",
        "  # Open the CSV file\n",
        "  with open(input_file, 'r') as file:\n",
        "      # Create a CSV reader object\n",
        "      csv_reader = csv.reader(file)\n",
        "\n",
        "      next(csv_reader)\n",
        "\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in csv_reader:\n",
        "          # Each row is a list representing the columns in that row\n",
        "          dialogs.append(row[1])\n",
        "          summarizations.append(row[2])\n",
        "          pair.append(row[1] + \"<sep>\" + row[2])\n",
        "\n",
        "  np.savetxt(file_name, pair, fmt='%s')\n",
        "\n",
        "\n",
        "re_write(file_path1, \"train.txt\")\n",
        "re_write(file_path2, \"test.txt\")\n",
        "re_write(file_path3, \"val.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd7R3VnymMu5",
        "outputId": "1ab8c261-fd45-44d8-f65f-1d6d9ea12773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coverage: True\n",
            "Finetune: False\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "class Config(object):\n",
        "  # General\n",
        "  hidden_size: int = 512\n",
        "  dec_hidden_size: Optional[int] = 512\n",
        "  embed_size: int = 512\n",
        "  pointer = True\n",
        "\n",
        "  # Data\n",
        "  max_vocab_size = 20000\n",
        "  embed_file: Optional[str] = None  # use pre-trained embeddings\n",
        "  source = 'train'    # use value: train or  big_samples\n",
        "  data_path: str = '/content/{}.txt'.format(source)\n",
        "  val_data_path: Optional[str] = '/content/val.txt'\n",
        "  test_data_path: Optional[str] = '/content/test.txt'\n",
        "  stop_word_file = '/content/HIT_stop_words.txt'\n",
        "  max_src_len: int = 300  # exclusive of special tokens such as EOS\n",
        "  max_tgt_len: int = 100  # exclusive of special tokens such as EOS\n",
        "  truncate_src: bool = True\n",
        "  truncate_tgt: bool = True\n",
        "  min_dec_steps: int = 30\n",
        "  max_dec_steps: int = 500\n",
        "  enc_rnn_dropout: float = 0.5\n",
        "  enc_attn: bool = True\n",
        "  dec_attn: bool = True\n",
        "  dec_in_dropout = 0\n",
        "  dec_rnn_dropout = 0\n",
        "  dec_out_dropout = 0\n",
        "\n",
        "\n",
        "  # Training\n",
        "  trunc_norm_init_std = 1e-4\n",
        "  eps = 1e-31\n",
        "  learning_rate = 0.001\n",
        "  lr_decay = 0.0\n",
        "  initial_accumulator_value = 0.1\n",
        "  epochs = 8\n",
        "  batch_size = 128 # changed from 32 to 128\n",
        "  coverage = True\n",
        "  fine_tune = False\n",
        "  scheduled_sampling = False\n",
        "  weight_tying = False\n",
        "  max_grad_norm = 2.0\n",
        "  is_cuda = True\n",
        "  DEVICE = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
        "  LAMBDA = 1\n",
        "\n",
        "  print('coverage:',coverage)\n",
        "  print('Finetune:',fine_tune)\n",
        "  if pointer:\n",
        "      if coverage:\n",
        "          if fine_tune:\n",
        "              model_name = 'ft_pgn'\n",
        "          else:\n",
        "              model_name = 'cov_pgn'\n",
        "      elif scheduled_sampling:\n",
        "          model_name = 'ss_pgn'\n",
        "      elif weight_tying:\n",
        "          model_name = 'wt_pgn'\n",
        "      else:\n",
        "          if source == 'big_samples':\n",
        "              model_name = 'pgn_big_samples'\n",
        "\n",
        "  else:\n",
        "      model_name = 'baseline'\n",
        "\n",
        "  encoder_save_name = '/content/saved_model/' + model_name + '/encoder.pt'\n",
        "  decoder_save_name = '/content/saved_model/' + model_name + '/decoder.pt'\n",
        "  attention_save_name = '/content/saved_model/' + model_name + '/attention.pt'\n",
        "  reduce_state_save_name = '/content/saved_model/' + model_name + '/reduce_state.pt'\n",
        "  losses_path = '/content/saved_model/' + model_name + '/val_losses.pkl'\n",
        "  log_path = '/content/runs/' + model_name\n",
        "\n",
        "\n",
        "  # Beam search\n",
        "  beam_size: int = 3\n",
        "  alpha = 0.2\n",
        "  beta = 0.2\n",
        "  gamma = 2000\n",
        "\n",
        "\n",
        "config = Config()\n",
        "print(config.is_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "a0fBdwz93RPS"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    PAD = 0\n",
        "    SOS = 1\n",
        "    EOS = 2\n",
        "    UNK = 3\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = Counter()\n",
        "        self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.index2word = self.reserved[:]\n",
        "        self.embeddings = None\n",
        "\n",
        "    def add_words(self, words):\n",
        "        \"\"\"Add a new token to the vocab and do mapping between word and index.\n",
        "\n",
        "        Args:\n",
        "            words (list): The list of tokens to be added.\n",
        "        \"\"\"\n",
        "        for word in words:\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = len(self.index2word)\n",
        "                self.index2word.append(word)\n",
        "        self.word2count.update(words)\n",
        "\n",
        "    def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n",
        "        num_embeddings = 0\n",
        "        vocab_size = len(self)\n",
        "        with open(file_path, 'rb') as f:\n",
        "            for line in f:\n",
        "                line = line.split()\n",
        "                word = line[0].decode('utf-8')\n",
        "                idx = self.word2index.get(word)\n",
        "                if idx is not None:\n",
        "                    vec = np.array(line[1:], dtype=dtype)\n",
        "                    if self.embeddings is None:\n",
        "                        n_dims = len(vec)\n",
        "                        self.embeddings = np.random.normal(\n",
        "                            np.zeros((vocab_size, n_dims))).astype(dtype)\n",
        "                        self.embeddings[self.PAD] = np.zeros(n_dims)\n",
        "                    self.embeddings[idx] = vec\n",
        "                    num_embeddings += 1\n",
        "        return num_embeddings\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if type(item) is int:\n",
        "            return self.index2word[item]\n",
        "        return self.word2index.get(item, self.UNK)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Returns the total size of the vocabulary\"\"\"\n",
        "        return len(self.index2word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "hEEDbTRe4LEM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import heapq\n",
        "import random\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "\n",
        "\n",
        "\n",
        "def timer(module):\n",
        "    \"\"\"Decorator function for a timer.\n",
        "\n",
        "    Args:\n",
        "        module (str): Description of the function being timed.\n",
        "    \"\"\"\n",
        "    def wrapper(func):\n",
        "        \"\"\"Wrapper of the timer function.\n",
        "\n",
        "        Args:\n",
        "            func (function): The function to be timed.\n",
        "        \"\"\"\n",
        "        def cal_time(*args, **kwargs):\n",
        "            \"\"\"The timer function.\n",
        "\n",
        "            Returns:\n",
        "                res (any): The returned value of the function being timed.\n",
        "            \"\"\"\n",
        "            t1 = time.time()\n",
        "            res = func(*args, **kwargs)\n",
        "            t2 = time.time()\n",
        "            cost_time = t2 - t1\n",
        "            print(f'{cost_time} secs used for ', module)\n",
        "            return res\n",
        "        return cal_time\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def count_words(counter, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence:\n",
        "            counter[word] += 1\n",
        "\n",
        "\n",
        "def sort_batch_by_len(data_batch):\n",
        "    res = {'x': [],\n",
        "           'y': [],\n",
        "           'x_len': [],\n",
        "           'y_len': [],\n",
        "           'OOV': [],\n",
        "           'len_OOV': []}\n",
        "    for i in range(len(data_batch)):\n",
        "        res['x'].append(data_batch[i]['x'])\n",
        "        res['y'].append(data_batch[i]['y'])\n",
        "        res['x_len'].append(len(data_batch[i]['x']))\n",
        "        res['y_len'].append(len(data_batch[i]['y']))\n",
        "        res['OOV'].append(data_batch[i]['OOV'])\n",
        "        res['len_OOV'].append(data_batch[i]['len_OOV'])\n",
        "\n",
        "    # Sort indices of data in batch by lengths.\n",
        "    sorted_indices = np.array(res['x_len']).argsort()[::-1].tolist()\n",
        "\n",
        "    data_batch = {\n",
        "        name: [_tensor[i] for i in sorted_indices]\n",
        "        for name, _tensor in res.items()\n",
        "    }\n",
        "    return data_batch\n",
        "\n",
        "\n",
        "def outputids2words(id_list, source_oovs, vocab):\n",
        "    \"\"\"\n",
        "        Maps output ids to words, including mapping in-source OOVs from\n",
        "        their temporary ids to the original OOV string (applicable in\n",
        "        pointer-generator mode).\n",
        "        Args:\n",
        "            id_list: list of ids (integers)\n",
        "            vocab: Vocabulary object\n",
        "            source_oovs:\n",
        "                list of OOV words (strings) in the order corresponding to\n",
        "                their temporary source OOV ids (that have been assigned in\n",
        "                pointer-generator mode), or None (in baseline mode)\n",
        "        Returns:\n",
        "            words: list of words (strings)\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    for i in id_list:\n",
        "        try:\n",
        "            w = vocab.index2word[i]  # might be [UNK]\n",
        "        except IndexError:  # w is OOV\n",
        "            assert_msg = \"Error: cannot find the ID the in the vocabulary.\"\n",
        "            assert source_oovs is not None, assert_msg\n",
        "            source_oov_idx = i - vocab.size()\n",
        "            try:\n",
        "                w = source_oovs[source_oov_idx]\n",
        "            except ValueError:  # i doesn't correspond to an source oov\n",
        "                raise ValueError(\n",
        "                    'Error: model produced word ID %i corresponding to source OOV %i \\\n",
        "                     but this example only has %i source OOVs'\n",
        "                    % (i, source_oov_idx, len(source_oovs)))\n",
        "        words.append(w)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def source2ids(source_words, vocab):\n",
        "    \"\"\"Map the source words to their ids and return a list of OOVs in the source.\n",
        "    Args:\n",
        "        source_words: list of words (strings)\n",
        "        vocab: Vocabulary object\n",
        "    Returns:\n",
        "        ids:\n",
        "        A list of word ids (integers); OOVs are represented by their temporary\n",
        "        source OOV number. If the vocabulary size is 50k and the source has 3\n",
        "        OOVs tokens, then these temporary OOV numbers will be 50000, 50001,\n",
        "        50002.\n",
        "    oovs:\n",
        "        A list of the OOV words in the source (strings), in the order\n",
        "        corresponding to their temporary source OOV numbers.\n",
        "    \"\"\"\n",
        "    ids = []\n",
        "    oovs = []\n",
        "    unk_id = vocab.UNK\n",
        "    for w in source_words:\n",
        "        i = vocab[w]\n",
        "        if i == unk_id:  # If w is OOV\n",
        "            if w not in oovs:  # Add to list of OOVs\n",
        "                oovs.append(w)\n",
        "            # This is 0 for the first source OOV, 1 for the second source OOV\n",
        "            oov_num = oovs.index(w)\n",
        "            # This is e.g. 20000 for the first source OOV, 50001 for the second\n",
        "            ids.append(vocab.size() + oov_num)\n",
        "        else:\n",
        "            ids.append(i)\n",
        "    return ids, oovs\n",
        "\n",
        "\n",
        "def abstract2ids(abstract_words, vocab, source_oovs):\n",
        "    \"\"\"Map tokens in the abstract (reference) to ids.\n",
        "       OOV tokens in the source will be remained.\n",
        "\n",
        "    Args:\n",
        "        abstract_words (list): Tokens in the reference.\n",
        "        vocab (vocab.Vocab): The vocabulary.\n",
        "        source_oovs (list): OOV tokens in the source.\n",
        "\n",
        "    Returns:\n",
        "        list: The reference with tokens mapped into ids.\n",
        "    \"\"\"\n",
        "    ids = []\n",
        "    unk_id = vocab.UNK\n",
        "    for w in abstract_words:\n",
        "        i = vocab[w]\n",
        "        if i == unk_id:  # If w is an OOV word\n",
        "            if w in source_oovs:  # If w is an in-source OOV\n",
        "                # Map to its temporary source OOV number\n",
        "                vocab_idx = vocab.size() + source_oovs.index(w)\n",
        "                ids.append(vocab_idx)\n",
        "            else:  # If w is an out-of-source OOV\n",
        "                ids.append(unk_id)  # Map to the UNK token id\n",
        "        else:\n",
        "            ids.append(i)\n",
        "    return ids\n",
        "\n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self,\n",
        "                 tokens,\n",
        "                 log_probs,\n",
        "                 decoder_states,\n",
        "                 coverage_vector):\n",
        "        self.tokens = tokens\n",
        "        self.log_probs = log_probs\n",
        "        self.decoder_states = decoder_states\n",
        "        self.coverage_vector = coverage_vector\n",
        "\n",
        "    def extend(self,\n",
        "               token,\n",
        "               log_prob,\n",
        "               decoder_states,\n",
        "               coverage_vector):\n",
        "        return Beam(tokens=self.tokens + [token],\n",
        "                    log_probs=self.log_probs + [log_prob],\n",
        "                    decoder_states=decoder_states,\n",
        "                    coverage_vector=coverage_vector)\n",
        "\n",
        "    def seq_score(self):\n",
        "        \"\"\"\n",
        "        This function calculate the score of the current sequence.\n",
        "        The scores are calculated according to the definitions in\n",
        "        https://opennmt.net/OpenNMT/translation/beam_search/.\n",
        "        1. Lenth normalization is used to normalize the cumulative score\n",
        "        of a whole sequence.\n",
        "        2. Coverage normalization is used to favor the sequences that fully\n",
        "        cover the information in the source. (In this case, it serves different\n",
        "        purpose from the coverage mechanism defined in PGN.)\n",
        "        3. Alpha and beta are hyperparameters that used to control the\n",
        "        strengths of ln and cn.\n",
        "        \"\"\"\n",
        "        len_Y = len(self.tokens)\n",
        "        # Lenth normalization\n",
        "        ln = (5+len_Y)**config.alpha / (5+1)**config.alpha\n",
        "        cn = config.beta * torch.sum(  # Coverage normalization\n",
        "            torch.log(\n",
        "                config.eps +\n",
        "                torch.where(\n",
        "                    self.coverage_vector < 1.0,\n",
        "                    self.coverage_vector,\n",
        "                    torch.ones((1, self.coverage_vector.shape[1])).to(torch.device(config.DEVICE))\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        score = sum(self.log_probs) / ln + cn\n",
        "        return score\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.seq_score() < other.seq_score()\n",
        "\n",
        "    def __le__(self, other):\n",
        "        return self.seq_score() <= other.seq_score()\n",
        "\n",
        "\n",
        "def add2heap(heap, item, k):\n",
        "    \"\"\"Maintain a heap with k nodes and the smallest one as root.\n",
        "\n",
        "    Args:\n",
        "        heap (list): The list to heapify.\n",
        "        item (tuple):\n",
        "            The tuple as item to store.\n",
        "            Comparsion will be made according to values in the first position.\n",
        "            If there is a tie, values in the second position will be compared,\n",
        "            and so on.\n",
        "        k (int): The capacity of the heap.\n",
        "    \"\"\"\n",
        "    if len(heap) < k:\n",
        "        heapq.heappush(heap, item)\n",
        "    else:\n",
        "        heapq.heappushpop(heap, item)\n",
        "\n",
        "\n",
        "def replace_oovs(in_tensor, vocab):\n",
        "    \"\"\"Replace oov tokens in a tensor with the <UNK> token.\n",
        "\n",
        "    Args:\n",
        "        in_tensor (Tensor): The tensor before replacement.\n",
        "        vocab (vocab.Vocab): The vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The tensor after replacement.\n",
        "    \"\"\"\n",
        "    oov_token = torch.full(in_tensor.shape, vocab.UNK).long().to(config.DEVICE)\n",
        "    out_tensor = torch.where(in_tensor > len(vocab) - 1, oov_token, in_tensor)\n",
        "    return out_tensor\n",
        "\n",
        "\n",
        "class ScheduledSampler():\n",
        "    def __init__(self, phases):\n",
        "        self.phases = phases\n",
        "        self.scheduled_probs = [i / (self.phases - 1) for i in range(self.phases)]\n",
        "\n",
        "    def teacher_forcing(self, phase):\n",
        "        \"\"\"According to a certain probability to choose whether to execute teacher_forcing\n",
        "\n",
        "        Args:\n",
        "            phase (int): probability level  if phase = 0, 100% teacher_forcing ,phase = self.phases - 1, 0% teacher_forcing\n",
        "\n",
        "        Returns:\n",
        "            bool: teacher_forcing or not\n",
        "        \"\"\"\n",
        "        sampling_prob = random.random()\n",
        "        if sampling_prob >= self.scheduled_probs[phase]:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "def config_info(config):\n",
        "    \"\"\"get some config information\n",
        "\n",
        "    Args:\n",
        "        config (model): define in  model/config.py\n",
        "    Returns:\n",
        "        string: config information\n",
        "    \"\"\"\n",
        "    info = 'model_name = {}, pointer = {}, coverage = {}, fine_tune = {}, scheduled_sampling = {}, weight_tying = {},' +\\\n",
        "          'source = {}  '\n",
        "    return (info.format(config.model_name, config.pointer, config.coverage, config.fine_tune, config.scheduled_sampling,\n",
        "                      config.weight_tying, config.source))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "S4j1TgAa4GxX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pathlib\n",
        "from collections import Counter\n",
        "from typing import Callable\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "\n",
        "# from utils import simple_tokenizer, count_words, sort_batch_by_len, source2ids, abstract2ids\n",
        "# from vocab import Vocab\n",
        "# import config\n",
        "\n",
        "\n",
        "class PairDataset(object):\n",
        "    \"\"\"The class represents source-reference pairs.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 filename,\n",
        "                 tokenize: Callable = simple_tokenizer,\n",
        "                 max_src_len: int = None,\n",
        "                 max_tgt_len: int = None,\n",
        "                 truncate_src: bool = False,\n",
        "                 truncate_tgt: bool = False):\n",
        "        print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n",
        "        self.filename = filename\n",
        "        self.pairs = []\n",
        "\n",
        "        with open(filename, 'rt', encoding='utf-8') as f:\n",
        "            next(f)\n",
        "            for i, line in enumerate(f):\n",
        "                # Split the source and reference by the <sep> tag.\n",
        "                pair = line.strip().split('<sep>')\n",
        "                if len(pair) != 2:\n",
        "                    # print(\"Line %d of %s is malformed.\" % (i, filename))\n",
        "                    # print(line)\n",
        "                    continue\n",
        "                src = tokenize(pair[0])\n",
        "                if max_src_len and len(src) > max_src_len:\n",
        "                    if truncate_src:\n",
        "                        src = src[:max_src_len]\n",
        "                    else:\n",
        "                        continue\n",
        "                tgt = tokenize(pair[1])\n",
        "                if max_tgt_len and len(tgt) > max_tgt_len:\n",
        "                    if truncate_tgt:\n",
        "                        tgt = tgt[:max_tgt_len]\n",
        "                    else:\n",
        "                        continue\n",
        "                self.pairs.append((src, tgt))\n",
        "        print(\"%d pairs.\" % len(self.pairs))\n",
        "\n",
        "    def build_vocab(self, embed_file: str = None) -> Vocab:\n",
        "        \"\"\"Build the vocabulary for the data set.\n",
        "\n",
        "        Args:\n",
        "            embed_file (str, optional):\n",
        "            The file path of the pre-trained embedding word vector.\n",
        "            Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            vocab.Vocab: The vocab object.\n",
        "        \"\"\"\n",
        "        # word frequency\n",
        "        word_counts = Counter()\n",
        "        count_words(word_counts,\n",
        "                    [src + tgr for src, tgr in self.pairs])\n",
        "        vocab = Vocab()\n",
        "        # Filter the vocabulary by keeping only the top k tokens in terms of\n",
        "        # word frequncy in the data set, where k is the maximum vocab size set\n",
        "        # in \"config.py\".\n",
        "        for word, count in word_counts.most_common(config.max_vocab_size):\n",
        "            vocab.add_words([word])\n",
        "        if embed_file is not None:\n",
        "            count = vocab.load_embeddings(embed_file)\n",
        "            print(\"%d pre-trained embeddings loaded.\" % count)\n",
        "\n",
        "        return vocab\n",
        "\n",
        "\n",
        "class SampleDataset(Dataset):\n",
        "    \"\"\"The class represents a sample set for training.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, data_pair, vocab):\n",
        "        self.src_sents = [x[0] for x in data_pair]\n",
        "        self.trg_sents = [x[1] for x in data_pair]\n",
        "        self.vocab = vocab\n",
        "        # Keep track of how many data points.\n",
        "        self._len = len(data_pair)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, oov = source2ids(self.src_sents[index], self.vocab)\n",
        "        return {\n",
        "            'x': [self.vocab.SOS] + x + [self.vocab.EOS],\n",
        "            'OOV': oov,\n",
        "            'len_OOV': len(oov),\n",
        "            'y': [self.vocab.SOS] +\n",
        "            abstract2ids(self.trg_sents[index],\n",
        "                         self.vocab, oov) + [self.vocab.EOS],\n",
        "            'x_len': len(self.src_sents[index]),\n",
        "            'y_len': len(self.trg_sents[index])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Split data set into batches and do padding for each batch.\n",
        "\n",
        "    Args:\n",
        "        x_padded (Tensor): Padded source sequences.\n",
        "        y_padded (Tensor): Padded reference sequences.\n",
        "        x_len (int): Sequence length of the sources.\n",
        "        y_len (int): Sequence length of the references.\n",
        "        OOV (dict): Out-of-vocabulary tokens.\n",
        "        len_OOV (int): Number of OOV tokens.\n",
        "    \"\"\"\n",
        "    def padding(indice, max_length, pad_idx=0):\n",
        "        pad_indice = [item + [pad_idx] * max(0, max_length - len(item))\n",
        "                      for item in indice]\n",
        "        return torch.tensor(pad_indice)\n",
        "\n",
        "    data_batch = sort_batch_by_len(batch)\n",
        "\n",
        "    x = data_batch[\"x\"]\n",
        "    x_max_length = max([len(t) for t in x])\n",
        "    y = data_batch[\"y\"]\n",
        "    y_max_length = max([len(t) for t in y])\n",
        "\n",
        "    OOV = data_batch[\"OOV\"]\n",
        "    len_OOV = torch.tensor(data_batch[\"len_OOV\"])\n",
        "\n",
        "    x_padded = padding(x, x_max_length)\n",
        "    y_padded = padding(y, y_max_length)\n",
        "\n",
        "    x_len = torch.tensor(data_batch[\"x_len\"])\n",
        "    y_len = torch.tensor(data_batch[\"y_len\"])\n",
        "    return x_padded, y_padded, x_len, y_len, OOV, len_OOV\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "mMQkkV__PyVx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "\n",
        "# from dataset import collate_fn\n",
        "# import config\n",
        "\n",
        "\n",
        "def evaluate(model, val_data, epoch):\n",
        "    \"\"\"Evaluate the loss for an epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        val_data (dataset.PairDataset): The evaluation data set.\n",
        "        epoch (int): The epoch number.\n",
        "\n",
        "    Returns:\n",
        "        numpy ndarray: The average loss of the dev set.\n",
        "    \"\"\"\n",
        "    print('validating')\n",
        "\n",
        "    val_loss = []\n",
        "    with torch.no_grad():\n",
        "        DEVICE = config.DEVICE\n",
        "        val_dataloader = DataLoader(dataset=val_data,\n",
        "                                    batch_size=config.batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    pin_memory=True, drop_last=True,\n",
        "                                    collate_fn=collate_fn)\n",
        "        for batch, data in enumerate(tqdm(val_dataloader)):\n",
        "            x, y, x_len, y_len, oov, len_oovs = data\n",
        "            if config.is_cuda:\n",
        "                x = x.to(DEVICE)\n",
        "                y = y.to(DEVICE)\n",
        "                x_len = x_len.to(DEVICE)\n",
        "                len_oovs = len_oovs.to(DEVICE)\n",
        "            loss = model(x,\n",
        "                         x_len,\n",
        "                         y,\n",
        "                         len_oovs,\n",
        "                         batch=batch,\n",
        "                         num_batches=len(val_dataloader),\n",
        "                         teacher_forcing=True)\n",
        "            val_loss.append(loss.item())\n",
        "    return np.mean(val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "BA3LXwHDP13i"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "# import config\n",
        "# from utils import timer, replace_oovs\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_size,\n",
        "                 hidden_size,\n",
        "                 rnn_drop: float = 0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(embed_size,\n",
        "                            hidden_size,\n",
        "                            bidirectional=True,\n",
        "                            dropout=rnn_drop,\n",
        "                            batch_first=True)\n",
        "\n",
        "#     @timer('encoder')\n",
        "    def forward(self, x, decoder_embedding):\n",
        "        \"\"\"Define forward propagation for the endoer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input samples as shape (batch_size, seq_len).\n",
        "            decoder_embedding (torch.nn.modules): The input embedding layer from decoder\n",
        "        Returns:\n",
        "            output (Tensor):\n",
        "                The output of lstm with shape\n",
        "                (batch_size, seq_len, 2 * hidden_units).\n",
        "            hidden (tuple):\n",
        "                The hidden states of lstm (h_n, c_n).\n",
        "                Each with shape (2, batch_size, hidden_units)\n",
        "        \"\"\"\n",
        "        if config.weight_tying:\n",
        "            embedded = decoder_embedding(x)\n",
        "        else:\n",
        "            embedded = self.embedding(x)\n",
        "        output, hidden = self.lstm(embedded)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(Attention, self).__init__()\n",
        "        # Define feed-forward layers.\n",
        "        self.Wh = nn.Linear(2*hidden_units, 2*hidden_units, bias=False)\n",
        "        self.Ws = nn.Linear(2*hidden_units, 2*hidden_units)\n",
        "        # wc for coverage feature\n",
        "        self.wc = nn.Linear(1, 2*hidden_units, bias=False)\n",
        "        self.v = nn.Linear(2*hidden_units, 1, bias=False)\n",
        "\n",
        "#     @timer('attention')\n",
        "    def forward(self,\n",
        "                decoder_states,\n",
        "                encoder_output,\n",
        "                x_padding_masks,\n",
        "                coverage_vector):\n",
        "        \"\"\"Define forward propagation for the attention network.\n",
        "\n",
        "        Args:\n",
        "            decoder_states (tuple):\n",
        "                The hidden states from lstm (h_n, c_n) in the decoder,\n",
        "                each with shape (1, batch_size, hidden_units)\n",
        "            encoder_output (Tensor):\n",
        "                The output from the lstm in the decoder with\n",
        "                shape (batch_size, seq_len, hidden_units).\n",
        "            x_padding_masks (Tensor):\n",
        "                The padding masks for the input sequences\n",
        "                with shape (batch_size, seq_len).\n",
        "            coverage_vector (Tensor):\n",
        "                The coverage vector from last time step.\n",
        "                with shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            context_vector (Tensor):\n",
        "                Dot products of attention weights and encoder hidden states.\n",
        "                The shape is (batch_size, 2*hidden_units).\n",
        "            attention_weights (Tensor): The shape is (batch_size, seq_length).\n",
        "            coverage_vector (Tensor): The shape is (batch_size, seq_length).\n",
        "        \"\"\"\n",
        "        # Concatenate h and c to get s_t and expand the dim of s_t.\n",
        "        h_dec, c_dec = decoder_states\n",
        "        # (1, batch_size, 2*hidden_units)\n",
        "        s_t = torch.cat([h_dec, c_dec], dim=2)\n",
        "        # (batch_size, 1, 2*hidden_units)\n",
        "        s_t = s_t.transpose(0, 1)\n",
        "        # (batch_size, seq_length, 2*hidden_units)\n",
        "        s_t = s_t.expand_as(encoder_output).contiguous()\n",
        "\n",
        "        # calculate attention scores\n",
        "        # Equation(11).\n",
        "        # Wh h_* (batch_size, seq_length, 2*hidden_units)\n",
        "        encoder_features = self.Wh(encoder_output.contiguous())\n",
        "        # Ws s_t (batch_size, seq_length, 2*hidden_units)\n",
        "        decoder_features = self.Ws(s_t)\n",
        "        # (batch_size, seq_length, 2*hidden_units)\n",
        "        att_inputs = encoder_features + decoder_features\n",
        "\n",
        "        # Add coverage feature.\n",
        "        if config.coverage:\n",
        "            coverage_features = self.wc(coverage_vector.unsqueeze(2))  # wc c\n",
        "            att_inputs = att_inputs + coverage_features\n",
        "\n",
        "        # (batch_size, seq_length, 1)\n",
        "        score = self.v(torch.tanh(att_inputs))\n",
        "        # (batch_size, seq_length)\n",
        "        attention_weights = F.softmax(score, dim=1).squeeze(2)\n",
        "        attention_weights = attention_weights * x_padding_masks\n",
        "        # Normalize attention weights after excluding padded positions.\n",
        "        normalization_factor = attention_weights.sum(1, keepdim=True)\n",
        "        attention_weights = attention_weights / normalization_factor\n",
        "        # (batch_size, 1, 2*hidden_units)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1),\n",
        "                                   encoder_output)\n",
        "        # (batch_size, 2*hidden_units)\n",
        "        context_vector = context_vector.squeeze(1)\n",
        "\n",
        "        # Update coverage vector.\n",
        "        if config.coverage:\n",
        "            coverage_vector = coverage_vector + attention_weights\n",
        "\n",
        "        return context_vector, attention_weights, coverage_vector\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_size,\n",
        "                 hidden_size,\n",
        "                 enc_hidden_size=None,\n",
        "                 is_cuda=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.DEVICE = torch.device('cuda') if is_cuda else torch.device('cpu')\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "        self.W1 = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.W2 = nn.Linear(self.hidden_size, vocab_size)\n",
        "        if config.pointer:\n",
        "            self.w_gen = nn.Linear(self.hidden_size * 4 + embed_size, 1)\n",
        "\n",
        "#     @timer('decoder')\n",
        "    def forward(self, x_t, decoder_states, context_vector):\n",
        "        \"\"\"Define forward propagation for the decoder.\n",
        "\n",
        "        Args:\n",
        "            x_t (Tensor):\n",
        "                The input of the decoder x_t of shape (batch_size, 1).\n",
        "            decoder_states (tuple):\n",
        "                The hidden states(h_n, c_n) of the decoder from last time step.\n",
        "                The shapes are (1, batch_size, hidden_units) for each.\n",
        "            context_vector (Tensor):\n",
        "                The context vector from the attention network\n",
        "                of shape (batch_size,2*hidden_units).\n",
        "\n",
        "        Returns:\n",
        "            p_vocab (Tensor):\n",
        "                The vocabulary distribution of shape (batch_size, vocab_size).\n",
        "            docoder_states (tuple):\n",
        "                The lstm states in the decoder.\n",
        "                The shapes are (1, batch_size, hidden_units) for each.\n",
        "            p_gen (Tensor):\n",
        "                The generation probabilities of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        decoder_emb = self.embedding(x_t)\n",
        "\n",
        "        decoder_output, decoder_states = self.lstm(decoder_emb, decoder_states)\n",
        "\n",
        "        # concatenate context vector and decoder state\n",
        "        # (batch_size, 3*hidden_units)\n",
        "        decoder_output = decoder_output.view(-1, config.hidden_size)\n",
        "        concat_vector = torch.cat(\n",
        "            [decoder_output,\n",
        "             context_vector],\n",
        "            dim=-1)\n",
        "\n",
        "        # calculate vocabulary distribution\n",
        "        # (batch_size, hidden_units)\n",
        "        FF1_out = self.W1(concat_vector)\n",
        "        # (batch_size, vocab_size)\n",
        "        if config.weight_tying:\n",
        "            FF2_out = torch.mm(FF1_out, torch.t(self.embedding.weight))\n",
        "        else:\n",
        "            FF2_out = self.W2(FF1_out)\n",
        "        # (batch_size, vocab_size)\n",
        "        p_vocab = F.softmax(FF2_out, dim=1)\n",
        "\n",
        "        # Concatenate h and c to get s_t and expand the dim of s_t.\n",
        "        h_dec, c_dec = decoder_states\n",
        "        # (1, batch_size, 2*hidden_units)\n",
        "        s_t = torch.cat([h_dec, c_dec], dim=2)\n",
        "\n",
        "        p_gen = None\n",
        "        if config.pointer:\n",
        "            # Calculate p_gen.\n",
        "            # Refer to equation (8).\n",
        "            x_gen = torch.cat([\n",
        "                context_vector,\n",
        "                s_t.squeeze(0),\n",
        "                decoder_emb.squeeze(1)\n",
        "            ],\n",
        "                              dim=-1)\n",
        "            p_gen = torch.sigmoid(self.w_gen(x_gen))\n",
        "\n",
        "        return p_vocab, decoder_states, p_gen\n",
        "\n",
        "\n",
        "class ReduceState(nn.Module):\n",
        "    \"\"\"\n",
        "    Since the encoder has a bidirectional LSTM layer while the decoder has a\n",
        "    unidirectional LSTM layer, we add this module to reduce the hidden states\n",
        "    output by the encoder (merge two directions) before input the hidden states\n",
        "    nto the decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ReduceState, self).__init__()\n",
        "\n",
        "    def forward(self, hidden):\n",
        "        \"\"\"The forward propagation of reduce state module.\n",
        "\n",
        "        Args:\n",
        "            hidden (tuple):\n",
        "                Hidden states of encoder,\n",
        "                each with shape (2, batch_size, hidden_units).\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                Reduced hidden states,\n",
        "                each with shape (1, batch_size, hidden_units).\n",
        "        \"\"\"\n",
        "        h, c = hidden\n",
        "        h_reduced = torch.sum(h, dim=0, keepdim=True)\n",
        "        c_reduced = torch.sum(c, dim=0, keepdim=True)\n",
        "        return (h_reduced, c_reduced)\n",
        "\n",
        "\n",
        "class PGN(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            v\n",
        "    ):\n",
        "        super(PGN, self).__init__()\n",
        "        self.v = v\n",
        "        self.DEVICE = config.DEVICE\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.encoder = Encoder(\n",
        "            len(v),\n",
        "            config.embed_size,\n",
        "            config.hidden_size,\n",
        "        )\n",
        "        self.decoder = Decoder(len(v),\n",
        "                               config.embed_size,\n",
        "                               config.hidden_size,\n",
        "                               )\n",
        "        self.reduce_state = ReduceState()\n",
        "\n",
        "    def load_model(self):\n",
        "\n",
        "        if (os.path.exists(config.encoder_save_name)):\n",
        "            print('Loading model: ', config.encoder_save_name)\n",
        "            self.encoder = torch.load(config.encoder_save_name)\n",
        "            self.decoder = torch.load(config.decoder_save_name)\n",
        "            self.attention = torch.load(config.attention_save_name)\n",
        "            self.reduce_state = torch.load(config.reduce_state_save_name)\n",
        "\n",
        "        elif config.fine_tune:\n",
        "            print('Loading model: ', '../saved_model/pgn/encoder.pt')\n",
        "            self.encoder = torch.load('../saved_model/pgn/encoder.pt')\n",
        "            self.decoder = torch.load('../saved_model/pgn/decoder.pt')\n",
        "            self.attention = torch.load('../saved_model/pgn/attention.pt')\n",
        "            self.reduce_state = torch.load('../saved_model/pgn/reduce_state.pt')\n",
        "\n",
        "#     @timer('final dist')\n",
        "    def get_final_distribution(self, x, p_gen, p_vocab, attention_weights,\n",
        "                               max_oov):\n",
        "        \"\"\"Calculate the final distribution for the model.\n",
        "\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            p_gen: (batch_size, 1)\n",
        "            p_vocab: (batch_size, vocab_size)\n",
        "            attention_weights: (batch_size, seq_len)\n",
        "            max_oov: (Tensor or int): The maximum sequence length in the batch.\n",
        "\n",
        "        Returns:\n",
        "            final_distribution (Tensor):\n",
        "            The final distribution over the extended vocabualary.\n",
        "            The shape is (batch_size, )\n",
        "        \"\"\"\n",
        "\n",
        "        if not config.pointer:\n",
        "            return p_vocab\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        # Clip the probabilities.\n",
        "        p_gen = torch.clamp(p_gen, 0.001, 0.999)\n",
        "        # Get the weighted probabilities.\n",
        "        # Refer to equation (9).\n",
        "        p_vocab_weighted = p_gen * p_vocab\n",
        "        # (batch_size, seq_len)\n",
        "        attention_weighted = (1 - p_gen) * attention_weights\n",
        "\n",
        "        # Get the extended-vocab probability distribution\n",
        "        # extended_size = len(self.v) + max_oovs\n",
        "        extension = torch.zeros((batch_size, max_oov)).float().to(self.DEVICE)\n",
        "        # (batch_size, extended_vocab_size)\n",
        "        p_vocab_extended = torch.cat([p_vocab_weighted, extension], dim=1)\n",
        "\n",
        "        # Add the attention weights to the corresponding vocab positions.\n",
        "        # Refer to equation (9).\n",
        "        final_distribution = \\\n",
        "            p_vocab_extended.scatter_add_(dim=1,\n",
        "                                          index=x,\n",
        "                                          src=attention_weighted)\n",
        "\n",
        "        return final_distribution\n",
        "\n",
        "#     @timer('model forward')\n",
        "    def forward(self, x, x_len, y, len_oovs, batch, num_batches, teacher_forcing):\n",
        "        \"\"\"Define the forward propagation for the seq2seq model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor):\n",
        "                Input sequences as source with shape (batch_size, seq_len)\n",
        "            x_len ([int): Sequence length of the current batch.\n",
        "            y (Tensor):\n",
        "                Input sequences as reference with shape (bacth_size, y_len)\n",
        "            len_oovs (Tensor):\n",
        "                The numbers of out-of-vocabulary words for samples in this batch.\n",
        "            batch (int): The number of the current batch.\n",
        "            num_batches(int): Number of batches in the epoch.\n",
        "            teacher_forcing(bool): teacher_forcing or not\n",
        "\n",
        "        Returns:\n",
        "            batch_loss (Tensor): The average loss of the current batch.\n",
        "        \"\"\"\n",
        "\n",
        "        x_copy = replace_oovs(x, self.v)\n",
        "        x_padding_masks = torch.ne(x, 0).byte().float()\n",
        "        # Call encoder  forward propagation\n",
        "        encoder_output, encoder_states = self.encoder(x_copy, self.decoder.embedding)\n",
        "        # Reduce encoder hidden states.\n",
        "        decoder_states = self.reduce_state(encoder_states)\n",
        "        # Initialize coverage vector.\n",
        "        coverage_vector = torch.zeros(x.size()).to(self.DEVICE)\n",
        "        # Calculate loss for every step.\n",
        "        step_losses = []\n",
        "        # use ground true to set x_t as first step data for decoder input\n",
        "        x_t = y[:, 0]\n",
        "        for t in range(y.shape[1]-1):\n",
        "\n",
        "            # use ground true to set x_t ,if teacher_forcing is True\n",
        "            if teacher_forcing:\n",
        "                x_t = y[:, t]\n",
        "\n",
        "            x_t = replace_oovs(x_t, self.v)\n",
        "\n",
        "            y_t = y[:, t+1]\n",
        "            # Get context vector from the attention network.\n",
        "            context_vector, attention_weights, coverage_vector = \\\n",
        "                self.attention(decoder_states,\n",
        "                               encoder_output,\n",
        "                               x_padding_masks,\n",
        "                               coverage_vector)\n",
        "            # Get vocab distribution and hidden states from the decoder.\n",
        "            p_vocab, decoder_states, p_gen = self.decoder(x_t.unsqueeze(1),\n",
        "                                                          decoder_states,\n",
        "                                                          context_vector)\n",
        "\n",
        "            final_dist = self.get_final_distribution(x,\n",
        "                                                     p_gen,\n",
        "                                                     p_vocab,\n",
        "                                                     attention_weights,\n",
        "                                                     torch.max(len_oovs))\n",
        "            # t step predict result as t+1 step input\n",
        "            x_t = torch.argmax(final_dist, dim=1).to(self.DEVICE)\n",
        "\n",
        "            # Get the probabilities predict by the model for target tokens.\n",
        "            if not config.pointer:\n",
        "                y_t = replace_oovs(y_t, self.v)\n",
        "            target_probs = torch.gather(final_dist, 1, y_t.unsqueeze(1))\n",
        "            target_probs = target_probs.squeeze(1)\n",
        "\n",
        "            # Apply a mask such that pad zeros do not affect the loss\n",
        "            mask = torch.ne(y_t, 0).byte()\n",
        "            # Do smoothing to prevent getting NaN loss because of log(0).\n",
        "            loss = -torch.log(target_probs + config.eps)\n",
        "\n",
        "            if config.coverage:\n",
        "                # Add coverage loss.\n",
        "                ct_min = torch.min(attention_weights, coverage_vector)\n",
        "                cov_loss = torch.sum(ct_min, dim=1)\n",
        "                loss = loss + config.LAMBDA * cov_loss\n",
        "\n",
        "            mask = mask.float()\n",
        "            loss = loss * mask\n",
        "\n",
        "            step_losses.append(loss)\n",
        "\n",
        "        sample_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
        "        # get the non-padded length of each sequence in the batch\n",
        "        seq_len_mask = torch.ne(y, 0).byte().float()\n",
        "        batch_seq_len = torch.sum(seq_len_mask, dim=1)\n",
        "\n",
        "        # get batch loss by dividing the loss of each batch\n",
        "        # by the target sequence length and mean\n",
        "        batch_loss = torch.mean(sample_losses / batch_seq_len)\n",
        "        return batch_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_QQK39-P8IK",
        "outputId": "98dfd1ac-b921-4527-d9f0-d8de64988615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset /content/train.txt... 14732 pairs.\n",
            "Reading dataset /content/val.txt... 818 pairs.\n",
            "loading data\n",
            "initializing optimizer\n",
            "initializing optimizer DONE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:   0%|          | 0/1 [00:01<?, ?it/s, Batch=0, Loss=10.3]\u001b[A\n",
            "Epoch 0: 100%|| 1/1 [00:01<00:00,  1.06s/it, Batch=0, Loss=10.3]\u001b[A\n",
            "Epoch 0: 100%|| 1/1 [01:58<00:00,  1.06s/it, Batch=0, Loss=10.3]\u001b[A\n",
            "Epoch 0: 100%|| 1/1 [01:58<00:00,  1.06s/it, Batch=100, Loss=6.3]\u001b[A\n",
            "Epoch 0: : 2it [02:14, 67.42s/it, Batch=100, Loss=6.3]\n",
            "Epoch 0:  12%|        | 1/8 [02:14<15:43, 134.86s/it, Loss=6.79]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:02,  2.06it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.08it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.45it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.51it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 0/1 [00:01<?, ?it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 0/1 [00:01<?, ?it/s, Batch=0, Loss=5.95]\u001b[A\n",
            "Epoch 1: 100%|| 1/1 [00:01<00:00,  1.16s/it, Batch=0, Loss=5.95]\u001b[A\n",
            "Epoch 1: 100%|| 1/1 [01:58<00:00,  1.16s/it, Batch=0, Loss=5.95]\u001b[A\n",
            "Epoch 1: 100%|| 1/1 [01:58<00:00,  1.16s/it, Batch=100, Loss=5.85]\u001b[A\n",
            "Epoch 1: : 2it [02:16, 68.18s/it, Batch=100, Loss=5.85]\n",
            "Epoch 1:  25%|       | 2/8 [04:34<13:46, 137.74s/it, Loss=5.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:03,  1.49it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:01<00:01,  2.07it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.35it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.27it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.25it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, Batch=0, Loss=5.4]\u001b[A\n",
            "Epoch 2: 100%|| 1/1 [00:00<00:00,  1.15it/s, Batch=0, Loss=5.4]\u001b[A\n",
            "Epoch 2: 100%|| 1/1 [01:58<00:00,  1.15it/s, Batch=0, Loss=5.4]\u001b[A\n",
            "Epoch 2: 100%|| 1/1 [01:58<00:00,  1.15it/s, Batch=100, Loss=5.45]\u001b[A\n",
            "Epoch 2: : 2it [02:15, 67.73s/it, Batch=100, Loss=5.45]\n",
            "Epoch 2:  38%|      | 3/8 [06:53<11:30, 138.10s/it, Loss=5.34]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:01,  2.97it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.83it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  1.99it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.02it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.26it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:   0%|          | 0/1 [00:01<?, ?it/s]\u001b[A\n",
            "Epoch 3:   0%|          | 0/1 [00:01<?, ?it/s, Batch=0, Loss=4.81]\u001b[A\n",
            "Epoch 3: 100%|| 1/1 [00:01<00:00,  1.24s/it, Batch=0, Loss=4.81]\u001b[A\n",
            "Epoch 3: 100%|| 1/1 [02:00<00:00,  1.24s/it, Batch=0, Loss=4.81]\u001b[A\n",
            "Epoch 3: 100%|| 1/1 [02:00<00:00,  1.24s/it, Batch=100, Loss=4.86]\u001b[A\n",
            "Epoch 3: : 2it [02:16, 68.36s/it, Batch=100, Loss=4.86]\n",
            "Epoch 3:  50%|     | 4/8 [09:12<09:15, 138.77s/it, Loss=4.81]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:01,  3.14it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.40it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  1.95it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:01,  1.98it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.22it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, Batch=0, Loss=4.11]\u001b[A\n",
            "Epoch 4: 100%|| 1/1 [00:00<00:00,  1.05it/s, Batch=0, Loss=4.11]\u001b[A\n",
            "Epoch 4: 100%|| 1/1 [01:56<00:00,  1.05it/s, Batch=0, Loss=4.11]\u001b[A\n",
            "Epoch 4: 100%|| 1/1 [01:56<00:00,  1.05it/s, Batch=100, Loss=4.25]\u001b[A\n",
            "Epoch 4: : 2it [02:16, 68.16s/it, Batch=100, Loss=4.25]\n",
            "Epoch 4:  62%|   | 5/8 [11:31<06:56, 138.86s/it, Loss=4.15]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:03,  1.50it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.20it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.15it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.41it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.34it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, Batch=0, Loss=3.4]\u001b[A\n",
            "Epoch 5: 100%|| 1/1 [00:00<00:00,  1.03it/s, Batch=0, Loss=3.4]\u001b[A\n",
            "Epoch 5: 100%|| 1/1 [01:59<00:00,  1.03it/s, Batch=0, Loss=3.4]\u001b[A\n",
            "Epoch 5: 100%|| 1/1 [01:59<00:00,  1.03it/s, Batch=100, Loss=3.5]\u001b[A\n",
            "Epoch 5: : 2it [02:16, 68.40s/it, Batch=100, Loss=3.5]\n",
            "Epoch 5:  75%|  | 6/8 [13:51<04:38, 139.05s/it, Loss=3.45]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:03,  1.42it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:01<00:01,  2.10it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.12it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.46it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.60it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, Batch=0, Loss=2.73]\u001b[A\n",
            "Epoch 6: 100%|| 1/1 [00:00<00:00,  1.14it/s, Batch=0, Loss=2.73]\u001b[A\n",
            "Epoch 6: 100%|| 1/1 [02:00<00:00,  1.14it/s, Batch=0, Loss=2.73]\u001b[A\n",
            "Epoch 6: 100%|| 1/1 [02:00<00:00,  1.14it/s, Batch=100, Loss=2.95]\u001b[A\n",
            "Epoch 6: : 2it [02:16, 68.04s/it, Batch=100, Loss=2.95]\n",
            "Epoch 6:  88%| | 7/8 [16:10<02:18, 138.92s/it, Loss=2.85]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:01,  2.91it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.45it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.27it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.32it/s]\u001b[A\n",
            " 83%| | 5/6 [00:02<00:00,  2.51it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name = cov_pgn, pointer = True, coverage = True, fine_tune = False, scheduled_sampling = False, weight_tying = False,source = train  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:   0%|          | 0/1 [00:01<?, ?it/s, Batch=0, Loss=2.37]\u001b[A\n",
            "Epoch 7: 100%|| 1/1 [00:01<00:00,  1.08s/it, Batch=0, Loss=2.37]\u001b[A\n",
            "Epoch 7: 100%|| 1/1 [02:00<00:00,  1.08s/it, Batch=0, Loss=2.37]\u001b[A\n",
            "Epoch 7: 100%|| 1/1 [02:00<00:00,  1.08s/it, Batch=100, Loss=2.61]\u001b[A\n",
            "Epoch 7: : 2it [02:17, 68.60s/it, Batch=100, Loss=2.61]\n",
            "Epoch 7: 100%|| 8/8 [18:29<00:00, 139.14s/it, Loss=2.38]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|        | 1/6 [00:00<00:01,  2.56it/s]\u001b[A\n",
            " 33%|      | 2/6 [00:00<00:01,  2.78it/s]\u001b[A\n",
            " 50%|     | 3/6 [00:01<00:01,  2.56it/s]\u001b[A\n",
            " 67%|   | 4/6 [00:01<00:00,  2.73it/s]\u001b[A\n",
            " 83%| | 5/6 [00:01<00:00,  2.42it/s]\u001b[A\n",
            "100%|| 6/6 [00:02<00:00,  2.21it/s]\n",
            "Epoch 7: 100%|| 8/8 [18:32<00:00, 139.05s/it, Loss=2.38]\n"
          ]
        }
      ],
      "source": [
        "from logging import disable\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "\n",
        "# from dataset import PairDataset\n",
        "# from model import PGN\n",
        "# import config\n",
        "# from evaluate import evaluate\n",
        "# from dataset import collate_fn, SampleDataset\n",
        "# from utils import ScheduledSampler, config_info\n",
        "\n",
        "\n",
        "\n",
        "def train(dataset, val_dataset, v, start_epoch=0):\n",
        "    \"\"\"Train the model, evaluate it and store it.\n",
        "\n",
        "    Args:\n",
        "        dataset (dataset.PairDataset): The training dataset.\n",
        "        val_dataset (dataset.PairDataset): The evaluation dataset.\n",
        "        v (vocab.Vocab): The vocabulary built from the training dataset.\n",
        "        start_epoch (int, optional): The starting epoch number. Defaults to 0.\n",
        "    \"\"\"\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if config.is_cuda else \"cpu\")\n",
        "\n",
        "    model = PGN(v)\n",
        "    model.load_model()\n",
        "    model.to(DEVICE)\n",
        "    if config.fine_tune:\n",
        "        # In fine-tuning mode, we fix the weights of all parameters except attention.wc.\n",
        "        print('Fine-tuning mode.')\n",
        "        for name, params in model.named_parameters():\n",
        "            if name != 'attention.wc.weight':\n",
        "                params.requires_grad=False\n",
        "    # forward\n",
        "    print(\"loading data\")\n",
        "    train_data = SampleDataset(dataset.pairs, v)\n",
        "    val_data = SampleDataset(val_dataset.pairs, v)\n",
        "\n",
        "    print(\"initializing optimizer\")\n",
        "\n",
        "    # Define the optimizer.\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=config.learning_rate)\n",
        "    print(\"initializing optimizer DONE\")\n",
        "    train_dataloader = DataLoader(dataset=train_data,\n",
        "                                  batch_size=config.batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  collate_fn=collate_fn)\n",
        "\n",
        "    val_losses = np.inf\n",
        "    if (os.path.exists(config.losses_path)):\n",
        "        with open(config.losses_path, 'rb') as f:\n",
        "            val_losses = pickle.load(f)\n",
        "\n",
        "#     torch.cuda.empty_cache()\n",
        "    # SummaryWriter: Log writer used for TensorboardX visualization.\n",
        "    writer = SummaryWriter(config.log_path)\n",
        "    # tqdm: A tool for drawing progress bars during training.\n",
        "    # scheduled_sampler : A tool for choosing teacher_forcing or not\n",
        "    num_epochs =  len(range(start_epoch, config.epochs))\n",
        "    scheduled_sampler = ScheduledSampler(num_epochs)\n",
        "    if config.scheduled_sampling:\n",
        "        print('scheduled_sampling mode.')\n",
        "    #  teacher_forcing = True\n",
        "\n",
        "    with tqdm(total=config.epochs) as epoch_progress:\n",
        "        for epoch in range(start_epoch, config.epochs):\n",
        "            print(config_info(config))\n",
        "            batch_losses = []  # Get loss of each batch.\n",
        "            num_batches = len(train_dataloader)\n",
        "            # set a teacher_forcing signal\n",
        "            if config.scheduled_sampling:\n",
        "                teacher_forcing = scheduled_sampler.teacher_forcing(epoch - start_epoch)\n",
        "            else:\n",
        "                teacher_forcing = True\n",
        "            # print('teacher_forcing = {}'.format(teacher_forcing))\n",
        "            with tqdm(total=num_batches//100) as batch_progress:\n",
        "                for batch, data in enumerate(tqdm(train_dataloader, disable=True)):\n",
        "                    x, y, x_len, y_len, oov, len_oovs = data\n",
        "                    assert not np.any(np.isnan(x.numpy()))\n",
        "                    if config.is_cuda:  # Training with GPUs.\n",
        "                        x = x.to(DEVICE)\n",
        "                        y = y.to(DEVICE)\n",
        "                        x_len = x_len.to(DEVICE)\n",
        "                        len_oovs = len_oovs.to(DEVICE)\n",
        "\n",
        "                    model.train()  # Sets the module in training mode.\n",
        "                    optimizer.zero_grad()  # Clear gradients.\n",
        "                    # Calculate loss.  Call model forward propagation\n",
        "                    loss = model(x, x_len, y, len_oovs, batch=batch, num_batches=num_batches, teacher_forcing=teacher_forcing)\n",
        "                    batch_losses.append(loss.item())\n",
        "                    loss.backward()  # Backpropagation.\n",
        "\n",
        "                    # Do gradient clipping to prevent gradient explosion.\n",
        "                    clip_grad_norm_(model.encoder.parameters(),\n",
        "                                    config.max_grad_norm)\n",
        "                    clip_grad_norm_(model.decoder.parameters(),\n",
        "                                    config.max_grad_norm)\n",
        "                    clip_grad_norm_(model.attention.parameters(),\n",
        "                                    config.max_grad_norm)\n",
        "                    optimizer.step()  # Update weights.\n",
        "\n",
        "                    # Output and record epoch loss every 100 batches.\n",
        "                    if (batch % 100) == 0:\n",
        "                        batch_progress.set_description(f'Epoch {epoch}')\n",
        "                        batch_progress.set_postfix(Batch=batch,\n",
        "                                                   Loss=loss.item())\n",
        "                        batch_progress.update()\n",
        "                        # Write loss for tensorboard.\n",
        "                        writer.add_scalar(f'Average loss for epoch {epoch}',\n",
        "                                          np.mean(batch_losses),\n",
        "                                          global_step=batch)\n",
        "            # Calculate average loss over all batches in an epoch.\n",
        "            epoch_loss = np.mean(batch_losses)\n",
        "\n",
        "            epoch_progress.set_description(f'Epoch {epoch}')\n",
        "            epoch_progress.set_postfix(Loss=epoch_loss)\n",
        "            epoch_progress.update()\n",
        "\n",
        "            avg_val_loss = evaluate(model, val_data, epoch)\n",
        "\n",
        "            # print('training loss:{}'.format(epoch_loss),\n",
        "            #       'validation loss:{}'.format(avg_val_loss))\n",
        "\n",
        "            # Update minimum evaluating loss.\n",
        "            if (avg_val_loss < val_losses):\n",
        "                torch.save(model.encoder, config.encoder_save_name)\n",
        "                torch.save(model.decoder, config.decoder_save_name)\n",
        "                torch.save(model.attention, config.attention_save_name)\n",
        "                torch.save(model.reduce_state, config.reduce_state_save_name)\n",
        "                val_losses = avg_val_loss\n",
        "            with open(config.losses_path, 'wb') as f:\n",
        "                pickle.dump(val_losses, f)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Prepare dataset for training.\n",
        "    DEVICE = torch.device('cuda') if config.is_cuda else torch.device('cpu')\n",
        "    dataset = PairDataset(config.data_path,\n",
        "                          max_src_len=config.max_src_len,\n",
        "                          max_tgt_len=config.max_tgt_len,\n",
        "                          truncate_src=config.truncate_src,\n",
        "                          truncate_tgt=config.truncate_tgt)\n",
        "    val_dataset = PairDataset(config.val_data_path,\n",
        "                              max_src_len=config.max_src_len,\n",
        "                              max_tgt_len=config.max_tgt_len,\n",
        "                              truncate_src=config.truncate_src,\n",
        "                              truncate_tgt=config.truncate_tgt)\n",
        "\n",
        "    vocab = dataset.build_vocab(embed_file=config.embed_file)\n",
        "\n",
        "    train(dataset, val_dataset, vocab, start_epoch=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60WAwIyfP_vc",
        "outputId": "3870f21e-bc9e-4bf1-ea85-ebefbd01f418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset /content/train.txt... 14732 pairs.\n",
            "Loading model:  /content/saved_model/cov_pgn/encoder.pt\n",
            "1.1661098003387451 secs used for  initalize predicter\n",
            "vocab_size:  20004\n",
            "/content/test.txt\n",
            "source:  Will: hey babe, what do you want for dinner tonight?\n",
            "Emma:  gah, don't even worry about it tonight\n",
            "Will: what do you mean? everything ok?\n",
            "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
            "Will: Well what time will you be home?\n",
            "Emma: soon, hopefully\n",
            "Will: you sure? Maybe you want me to pick you up?\n",
            "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
            "Will: Alright, love you. \n",
            "Emma: love you too.  \n",
            "\n",
            "0.7905666828155518 secs used for  doing prediction\n",
            "greedy:  <UNK> <UNK> and <UNK> are going to the cinema to see the <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
            "\n",
            "16.651530742645264 secs used for  doing prediction\n",
            "beam:  Jane is going to be home in 5 minutes to go to the cinema to go to the cinema to see the cinema. worry will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. up? will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. worry will be there in 5 minutes. about \n",
            "\n",
            "ref:  Emma will be home soon and she will let Will know. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import jieba\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "abs_path = Path(os.getcwd()).absolute()\n",
        "sys.path.append(abs_path)\n",
        "\n",
        "\n",
        "class Predict():\n",
        "    @timer(module='initalize predicter')\n",
        "    def __init__(self):\n",
        "        self.DEVICE = config.DEVICE\n",
        "\n",
        "        dataset = PairDataset(config.data_path,\n",
        "                              max_src_len=config.max_src_len,\n",
        "                              max_tgt_len=config.max_tgt_len,\n",
        "                              truncate_src=config.truncate_src,\n",
        "                              truncate_tgt=config.truncate_tgt)\n",
        "\n",
        "        self.vocab = dataset.build_vocab(embed_file=config.embed_file)\n",
        "\n",
        "        self.model = PGN(self.vocab)\n",
        "        self.model.load_model()\n",
        "        self.model.to(self.DEVICE)\n",
        "\n",
        "    def greedy_search(self,\n",
        "                      x,\n",
        "                      max_sum_len,\n",
        "                      len_oovs,\n",
        "                      x_padding_masks):\n",
        "        \"\"\"Function which returns a summary by always picking\n",
        "           the highest probability option conditioned on the previous word.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input sequence as the source.\n",
        "            max_sum_len (int): The maximum length a summary can have.\n",
        "            len_oovs (Tensor): Numbers of out-of-vocabulary tokens.\n",
        "            x_padding_masks (Tensor):\n",
        "                The padding masks for the input sequences\n",
        "                with shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            summary (list): The token list of the result summary.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get encoder output and states.Call encoder forward propagation\n",
        "        encoder_output, encoder_states = self.model.encoder(\n",
        "            replace_oovs(x, self.vocab), self.model.decoder.embedding)\n",
        "\n",
        "        # Initialize decoder's hidden states with encoder's hidden states.\n",
        "        decoder_states = self.model.reduce_state(encoder_states)\n",
        "\n",
        "        # Initialize decoder's input at time step 0 with the SOS token.\n",
        "        x_t = torch.ones(1) * self.vocab.SOS\n",
        "        x_t = x_t.to(self.DEVICE, dtype=torch.int64)\n",
        "        summary = [self.vocab.SOS]\n",
        "        coverage_vector = torch.zeros((1, x.shape[1])).to(self.DEVICE)\n",
        "        # Generate hypothesis with maximum decode step.\n",
        "        while int(x_t.item()) != (self.vocab.EOS) \\\n",
        "                and len(summary) < max_sum_len:\n",
        "\n",
        "            context_vector, attention_weights, coverage_vector = \\\n",
        "                self.model.attention(decoder_states,\n",
        "                                     encoder_output,\n",
        "                                     x_padding_masks,\n",
        "                                     coverage_vector)\n",
        "            p_vocab, decoder_states, p_gen = \\\n",
        "                self.model.decoder(x_t.unsqueeze(1),\n",
        "                                   decoder_states,\n",
        "                                   context_vector)\n",
        "            final_dist = self.model.get_final_distribution(x,\n",
        "                                                           p_gen,\n",
        "                                                           p_vocab,\n",
        "                                                           attention_weights,\n",
        "                                                           torch.max(len_oovs))\n",
        "            # Get next token with maximum probability.\n",
        "            x_t = torch.argmax(final_dist, dim=1).to(self.DEVICE)\n",
        "            decoder_word_idx = x_t.item()\n",
        "            summary.append(decoder_word_idx)\n",
        "            x_t = replace_oovs(x_t, self.vocab)\n",
        "\n",
        "        return summary\n",
        "\n",
        "#     @timer('best k')\n",
        "    def best_k(self, beam, k, encoder_output, x_padding_masks, x, len_oovs):\n",
        "        \"\"\"Get best k tokens to extend the current sequence at the current time step.\n",
        "\n",
        "        Args:\n",
        "            beam (untils.Beam): The candidate beam to be extended.\n",
        "            k (int): Beam size.\n",
        "            encoder_output (Tensor): The lstm output from the encoder.\n",
        "            x_padding_masks (Tensor):\n",
        "                The padding masks for the input sequences.\n",
        "            x (Tensor): Source token ids.\n",
        "            len_oovs (Tensor): Number of oov tokens in a batch.\n",
        "\n",
        "        Returns:\n",
        "            best_k (list(Beam)): The list of best k candidates.\n",
        "\n",
        "        \"\"\"\n",
        "        # use decoder to generate vocab distribution for the next token\n",
        "        x_t = torch.tensor(beam.tokens[-1]).reshape(1, 1)\n",
        "        x_t = x_t.to(self.DEVICE)\n",
        "\n",
        "        # Get context vector from attention network.\n",
        "        context_vector, attention_weights, coverage_vector = \\\n",
        "            self.model.attention(beam.decoder_states,\n",
        "                                 encoder_output,\n",
        "                                 x_padding_masks,\n",
        "                                 beam.coverage_vector)\n",
        "\n",
        "        # Replace the indexes of OOV words with the index of OOV token\n",
        "        # to prevent index-out-of-bound error in the decoder.\n",
        "\n",
        "        p_vocab, decoder_states, p_gen = \\\n",
        "            self.model.decoder(replace_oovs(x_t, self.vocab),\n",
        "                               beam.decoder_states,\n",
        "                               context_vector)\n",
        "\n",
        "        final_dist = self.model.get_final_distribution(x,\n",
        "                                                       p_gen,\n",
        "                                                       p_vocab,\n",
        "                                                       attention_weights,\n",
        "                                                       torch.max(len_oovs))\n",
        "        # Calculate log probabilities.\n",
        "        log_probs = torch.log(final_dist.squeeze())\n",
        "        # Filter forbidden tokens.\n",
        "        if len(beam.tokens) == 1:\n",
        "            forbidden_ids = [\n",
        "                self.vocab[u\"\"],\n",
        "                self.vocab[u\"\"],\n",
        "                self.vocab[u\"\"],\n",
        "                self.vocab[u\"\"],\n",
        "                self.vocab[u\"\"],\n",
        "            ]\n",
        "            log_probs[forbidden_ids] = -float('inf')\n",
        "        # EOS token penalty. Follow the definition in\n",
        "        # https://opennmt.net/OpenNMT/translation/beam_search/.\n",
        "        log_probs[self.vocab.EOS] *= \\\n",
        "            config.gamma * x.size()[1] / len(beam.tokens)\n",
        "\n",
        "        log_probs[self.vocab.UNK] = -float('inf')\n",
        "        # Get top k tokens and the corresponding logprob.\n",
        "        topk_probs, topk_idx = torch.topk(log_probs, k)\n",
        "\n",
        "        # Extend the current hypo with top k tokens, resulting k new hypos.\n",
        "        best_k = [beam.extend(x,\n",
        "                  log_probs[x],\n",
        "                  decoder_states,\n",
        "                  coverage_vector) for x in topk_idx.tolist()]\n",
        "\n",
        "        return best_k\n",
        "\n",
        "    def beam_search(self,\n",
        "                    x,\n",
        "                    max_sum_len,\n",
        "                    beam_width,\n",
        "                    len_oovs,\n",
        "                    x_padding_masks):\n",
        "        \"\"\"Using beam search to generate summary.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input sequence as the source.\n",
        "            max_sum_len (int): The maximum length a summary can have.\n",
        "            beam_width (int): Beam size.\n",
        "            max_oovs (int): Number of out-of-vocabulary tokens.\n",
        "            x_padding_masks (Tensor):\n",
        "                The padding masks for the input sequences.\n",
        "\n",
        "        Returns:\n",
        "            result (list(Beam)): The list of best k candidates.\n",
        "        \"\"\"\n",
        "        # run body_sequence input through encoder. Call encoder forward propagation\n",
        "        encoder_output, encoder_states = self.model.encoder(\n",
        "            replace_oovs(x, self.vocab), self.model.decoder.embedding)\n",
        "        coverage_vector = torch.zeros((1, x.shape[1])).to(self.DEVICE)\n",
        "        # initialize decoder states with encoder forward states\n",
        "        decoder_states = self.model.reduce_state(encoder_states)\n",
        "\n",
        "        # initialize the hypothesis with a class Beam instance.\n",
        "\n",
        "        init_beam = Beam([self.vocab.SOS],\n",
        "                         [0],\n",
        "                         decoder_states,\n",
        "                         coverage_vector)\n",
        "\n",
        "        # get the beam size and create a list for stroing current candidates\n",
        "        # and a list for completed hypothesis\n",
        "        k = beam_width\n",
        "        curr, completed = [init_beam], []\n",
        "\n",
        "        # use beam search for max_sum_len (maximum length) steps\n",
        "        for _ in range(max_sum_len):\n",
        "            # get k best hypothesis when adding a new token\n",
        "\n",
        "            topk = []\n",
        "            for beam in curr:\n",
        "                # When an EOS token is generated, add the hypo to the completed\n",
        "                # list and decrease beam size.\n",
        "                if beam.tokens[-1] == self.vocab.EOS:\n",
        "                    completed.append(beam)\n",
        "                    k -= 1\n",
        "                    continue\n",
        "                for can in self.best_k(beam,\n",
        "                                       k,\n",
        "                                       encoder_output,\n",
        "                                       x_padding_masks,\n",
        "                                       x,\n",
        "                                       torch.max(len_oovs)\n",
        "                                      ):\n",
        "                    # Using topk as a heap to keep track of top k candidates.\n",
        "                    # Using the sequence scores of the hypos to campare\n",
        "                    # and object ids to break ties.\n",
        "                    add2heap(topk, (can.seq_score(), id(can), can), k)\n",
        "\n",
        "            curr = [items[2] for items in topk]\n",
        "            # stop when there are enough completed hypothesis\n",
        "            if len(completed) == beam_width:\n",
        "                break\n",
        "        # When there are not engouh completed hypotheses,\n",
        "        # take whatever when have in current best k as the final candidates.\n",
        "        completed += curr\n",
        "        # sort the hypothesis by normalized probability and choose the best one\n",
        "        result = sorted(completed,\n",
        "                        key=lambda x: x.seq_score(),\n",
        "                        reverse=True)[0].tokens\n",
        "        return result\n",
        "\n",
        "    @timer(module='doing prediction')\n",
        "    def predict(self, text, tokenize=True, beam_search=True):\n",
        "        \"\"\"Generate summary.\n",
        "\n",
        "        Args:\n",
        "            text (str or list): Source.\n",
        "            tokenize (bool, optional):\n",
        "                Whether to do tokenize or not. Defaults to True.\n",
        "            beam_search (bool, optional):\n",
        "                Whether to use beam search or not.\n",
        "                Defaults to True (means using greedy search).\n",
        "\n",
        "        Returns:\n",
        "            str: The final summary.\n",
        "        \"\"\"\n",
        "        if isinstance(text, str) and tokenize:\n",
        "            text = list(jieba.cut(text))\n",
        "        x, oov = source2ids(text, self.vocab)\n",
        "        x = torch.tensor(x).to(self.DEVICE)\n",
        "        len_oovs = torch.tensor([len(oov)]).to(self.DEVICE)\n",
        "        x_padding_masks = torch.ne(x, 0).byte().float()\n",
        "        if beam_search:\n",
        "            summary = self.beam_search(x.unsqueeze(0),\n",
        "                                       max_sum_len=config.max_dec_steps,\n",
        "                                       beam_width=config.beam_size,\n",
        "                                       len_oovs=len_oovs,\n",
        "                                       x_padding_masks=x_padding_masks)\n",
        "        else:\n",
        "            summary = self.greedy_search(x.unsqueeze(0),\n",
        "                                         max_sum_len=config.max_dec_steps,\n",
        "                                         len_oovs=len_oovs,\n",
        "                                         x_padding_masks=x_padding_masks)\n",
        "        summary = outputids2words(summary,\n",
        "                                  oov,\n",
        "                                  self.vocab)\n",
        "        return summary.replace('<SOS>', '').replace('<EOS>', '').strip()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pred = Predict()\n",
        "    print('vocab_size: ', len(pred.vocab))\n",
        "    # Randomly pick a sample in test set to predict.\n",
        "    print(config.test_data_path)\n",
        "\n",
        "    input_file = '/content/Soul-AI/data/raw/summarization/samsum/samsum-test.csv'\n",
        "\n",
        "    dialogs = []\n",
        "    summarizations = []\n",
        "\n",
        "    # Open the CSV file\n",
        "    with open(input_file, 'r') as file:\n",
        "        # Create a CSV reader object\n",
        "        csv_reader = csv.reader(file)\n",
        "\n",
        "        next(csv_reader)\n",
        "\n",
        "        # Iterate over each row in the CSV file\n",
        "        for row in csv_reader:\n",
        "            # Each row is a list representing the columns in that row\n",
        "            dialogs.append(row[1])\n",
        "            summarizations.append(row[2])\n",
        "\n",
        "\n",
        "\n",
        "    print('source: ', dialogs[3], '\\n')\n",
        "    greedy_prediction = pred.predict(dialogs[3].split(),  beam_search=False)\n",
        "    print('greedy: ', greedy_prediction, '\\n')\n",
        "    beam_prediction = pred.predict(dialogs[3].split(),  beam_search=True)\n",
        "    print('beam: ', beam_prediction, '\\n')\n",
        "    print('ref: ', summarizations[3], '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CavKs5j9rCCt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
