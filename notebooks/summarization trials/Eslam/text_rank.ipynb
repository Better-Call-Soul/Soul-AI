{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49fjiObUyPWV",
        "outputId": "50a17852-41a5-46c4-e020-1ff04fc97d57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX3TvKtITyQg",
        "outputId": "76e13f48-e135-4cfe-8796-d5f00a24e230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)  # Set a fixed seed for reproducibility\n"
      ],
      "metadata": {
        "id": "C0iDfVJANb2W"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
        "\n",
        "sentences = [\"I am sad\"]\n",
        "\n",
        "model_outputs = classifier(sentences)\n",
        "print(model_outputs[0])\n",
        "print(mental_health_score(model_outputs[0]))"
      ],
      "metadata": {
        "id": "2lRxk2YK_tD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998ad81b-b3f7-4047-c4e3-0501551f6c78"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'sadness', 'score': 0.9104166626930237}, {'label': 'disappointment', 'score': 0.06846530735492706}, {'label': 'grief', 'score': 0.03181026130914688}, {'label': 'neutral', 'score': 0.026156943291425705}, {'label': 'remorse', 'score': 0.02074185200035572}, {'label': 'annoyance', 'score': 0.01124617736786604}, {'label': 'realization', 'score': 0.01070040836930275}, {'label': 'caring', 'score': 0.00934799574315548}, {'label': 'approval', 'score': 0.008505051955580711}, {'label': 'love', 'score': 0.007716427091509104}, {'label': 'admiration', 'score': 0.007372341118752956}, {'label': 'disapproval', 'score': 0.00708762789145112}, {'label': 'anger', 'score': 0.006395332049578428}, {'label': 'joy', 'score': 0.005612883251160383}, {'label': 'amusement', 'score': 0.0055735232308506966}, {'label': 'nervousness', 'score': 0.004651421681046486}, {'label': 'disgust', 'score': 0.004631553310900927}, {'label': 'curiosity', 'score': 0.004111076705157757}, {'label': 'desire', 'score': 0.0038776907604187727}, {'label': 'fear', 'score': 0.0036533386446535587}, {'label': 'gratitude', 'score': 0.0034208600409328938}, {'label': 'confusion', 'score': 0.0026301962789148092}, {'label': 'surprise', 'score': 0.0025329492054879665}, {'label': 'optimism', 'score': 0.0024612105917185545}, {'label': 'embarrassment', 'score': 0.0020719042513519526}, {'label': 'relief', 'score': 0.0018372114282101393}, {'label': 'excitement', 'score': 0.0018270740984007716}, {'label': 'pride', 'score': 0.00043052714318037033}]\n",
            "0.9136514937385187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mental_health_score(emotions, threshold=0.5):\n",
        "    # Define a set of labels associated with mental health concerns\n",
        "    mental_health_labels = {\n",
        "        'sadness', 'disappointment', 'annoyance', 'disapproval',\n",
        "        'nervousness', 'remorse', 'embarrassment', 'anger',\n",
        "        'disgust', 'grief', 'confusion', 'fear'\n",
        "    }\n",
        "\n",
        "    # Initialize variables to calculate the score\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "\n",
        "    # Iterate over the emotions\n",
        "    for emotion in emotions:\n",
        "        label = emotion['label']\n",
        "        score = emotion['score']\n",
        "        if label in mental_health_labels:\n",
        "            total_score += score\n",
        "        max_score += score\n",
        "\n",
        "    # Normalize the total score by the maximum possible score\n",
        "    normalized_score = total_score / max_score if max_score > 0 else 0\n",
        "\n",
        "    return normalized_score\n"
      ],
      "metadata": {
        "id": "J_Rnl8zUwhbw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# POS_TAGGER_FUNCTION : TYPE 1\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "def lemmatize(input):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  sentence = \" \".join(input)\n",
        "\n",
        "  # tokenize the sentence and find the POS tag for each token\n",
        "  pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "\n",
        "  #>[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'),\n",
        "  # ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'),\n",
        "  # ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
        "\n",
        "  # As you may have noticed, the above pos tags are a little confusing.\n",
        "\n",
        "  # we use our own pos_tagger function to make things simpler to understand.\n",
        "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "  #>[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None),\n",
        "  # ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'),\n",
        "  # ('mat', 'n'), ('under', None), ('many', 'a'), ('flying', 'v'), ('geese', 'a')]\n",
        "\n",
        "  lemmatized_sentence = []\n",
        "  for word, tag in wordnet_tagged:\n",
        "      if tag is None:\n",
        "          # if there is no available tag, append the token as is\n",
        "          lemmatized_sentence.append(word)\n",
        "      else:\n",
        "          # else use the tag to lemmatize the token\n",
        "          lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "  return lemmatized_sentence\n"
      ],
      "metadata": {
        "id": "1qNceuK3-YuF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vertex:\n",
        "    def __init__(self, name, arr_words):\n",
        "        self.name = name\n",
        "        self.edge = {}  # {name: edge weight}\n",
        "        self.weight = 1  # 1-10\n",
        "        self.words = {}  # {word: count}\n",
        "        for word in arr_words:\n",
        "            if word in self.words:\n",
        "                self.words[word] += 1\n",
        "            else:\n",
        "                self.words[word] = 1\n",
        "        self.num_words = sum(self.words.values())\n",
        "\n",
        "    def connection(self, connect_name, weight):\n",
        "        self.edge[connect_name] = weight\n"
      ],
      "metadata": {
        "id": "uKbJoYk7T5TR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummaryBot:\n",
        "    def __init__(self):\n",
        "        self.vertices = []\n",
        "        self.dampening_factor = 0.85\n",
        "        self.num_vertices = 0\n",
        "        self.original_sentences = []\n",
        "        self.filtered_sentences = []\n",
        "        self.error_threshold = 0.01\n",
        "        self.unconnected_vertices = set()\n",
        "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "    def run(self, text, num_return_sentences=None, test_statistics=False):\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError('Ensure that you pass valid values into SummaryBot.run')\n",
        "\n",
        "        self._process_string(text)\n",
        "        self._initialize()\n",
        "        last_v0 = None\n",
        "        cur_v0 = None\n",
        "        error_level = 10\n",
        "        while error_level > self.error_threshold:\n",
        "            self._update_all_vertex_weights()\n",
        "            if cur_v0 is None:\n",
        "                cur_v0 = self.vertices[0].weight\n",
        "            else:\n",
        "                last_v0 = cur_v0\n",
        "                cur_v0 = self.vertices[0].weight\n",
        "                error_level = abs(cur_v0 - last_v0)\n",
        "\n",
        "        if not num_return_sentences:\n",
        "            num_return_sentences = self._find_best_num_sentences()\n",
        "\n",
        "        output = self.get_top_sentences(num_return_sentences)\n",
        "\n",
        "\n",
        "\n",
        "        if test_statistics:\n",
        "            output += self.summary()\n",
        "        return output\n",
        "\n",
        "    def _find_best_num_sentences(self):\n",
        "        if len(self.vertices) <= 3:\n",
        "            return len(self.vertices)\n",
        "        return round(1.3 * math.log(len(self.original_sentences)))\n",
        "\n",
        "    def get_top_sentences(self, num_sentences):\n",
        "        self._sort_vertices_by_weight()\n",
        "\n",
        "        top_sentences_arr = []\n",
        "        if num_sentences > self.num_vertices:\n",
        "            return '. '.join(self.original_sentences)\n",
        "        else:\n",
        "            for sentence_out in range(num_sentences):\n",
        "                top_sentences_arr.append(self.vertices[sentence_out])\n",
        "\n",
        "            top_sentences_arr.sort(key=lambda vert: vert.name)\n",
        "            out = [self.original_sentences[vert.name] for vert in top_sentences_arr]\n",
        "\n",
        "        if any(vert.name == len(self.original_sentences) - 1 for vert in top_sentences_arr):\n",
        "            return '. '.join(out)\n",
        "        return '. '.join(out) + '.'\n",
        "\n",
        "    def _process_string(self, text):\n",
        "        self.original_sentences = re.split(r'[.?!]', text)\n",
        "        self.filtered_sentences = [sentence.replace(r'[^a-zA-Z0-9 ]', ' ') for sentence in self.original_sentences]\n",
        "        self.filtered_sentences = [sentence for sentence in self.filtered_sentences if sentence]\n",
        "\n",
        "        for sentence in self.filtered_sentences:\n",
        "            word_list = [word.lower() for word in sentence.split() if word and len(word) != 1 and word not in stop_words]\n",
        "            # print(word_list)\n",
        "            word_list=lemmatize(word_list)\n",
        "            # print(word_list)\n",
        "\n",
        "            self._add_vertex(word_list)\n",
        "\n",
        "    def _add_vertex(self, word_arr):\n",
        "        self.vertices.append(Vertex(self.num_vertices, word_arr))\n",
        "        self.num_vertices += 1\n",
        "\n",
        "    def _initialize(self):\n",
        "        for vertex_num in range(len(self.vertices) - 1):\n",
        "            for inner_vertex in range(vertex_num + 1, len(self.vertices)):\n",
        "                similarity = self._find_similarity(self.vertices[vertex_num], self.vertices[inner_vertex])\n",
        "                self.vertices[vertex_num].connection(inner_vertex, similarity)\n",
        "                self.vertices[inner_vertex].connection(vertex_num, similarity)\n",
        "\n",
        "    def _update_all_vertex_weights(self):\n",
        "        total_edge_weights = self._get_edge_totals()\n",
        "        for vertex_num in range(self.num_vertices):\n",
        "            temp_sum = 0\n",
        "            for other_vertex in range(self.num_vertices):\n",
        "                if other_vertex != vertex_num and other_vertex not in self.unconnected_vertices:\n",
        "                    temp_sum += (self.vertices[vertex_num].edge[other_vertex] / total_edge_weights[other_vertex] * self.vertices[other_vertex].weight)\n",
        "            self.vertices[vertex_num].weight = ((1 - self.dampening_factor) + self.dampening_factor * temp_sum)+0.2*mental_health_score(classifier(self.original_sentences[self.vertices[vertex_num].name])[0])\n",
        "\n",
        "    def _get_edge_totals(self):\n",
        "        out = []\n",
        "        for vertex_num in range(self.num_vertices):\n",
        "            temp_sum = sum(self.vertices[vertex_num].edge.values())\n",
        "            if not temp_sum:\n",
        "                self.vertices[vertex_num].weight = 0\n",
        "                self.unconnected_vertices.add(vertex_num)\n",
        "            out.append(temp_sum)\n",
        "        return out\n",
        "\n",
        "    # def _find_similarity(self, vert1, vert2):\n",
        "    #     overlap = 0\n",
        "    #     for word in vert1.words:\n",
        "    #         if word in vert2.words:\n",
        "    #             overlap += math.pow(vert1.words[word] * vert2.words[word], 0.7)\n",
        "\n",
        "    #     return overlap / (math.log(vert1.num_words) + math.log(vert2.num_words)+1)\n",
        "\n",
        "    # def _find_similarity(self, vert1, vert2):\n",
        "    #     \"\"\"\n",
        "    #     Computes the cosine similarity between two vertices based on their word frequencies.\n",
        "\n",
        "    #     Args:\n",
        "    #         vert1 (Vertex): The first vertex.\n",
        "    #         vert2 (Vertex): The second vertex.\n",
        "\n",
        "    #     Returns:\n",
        "    #         float: The cosine similarity between the two vertices.\n",
        "    #     \"\"\"\n",
        "    #     # Convert the word dictionaries to vectors\n",
        "    #     words1 = vert1.words\n",
        "    #     words2 = vert2.words\n",
        "\n",
        "    #     # Create a set of all words in both vertices\n",
        "    #     all_words = list(set(words1.keys()).union(set(words2.keys())))\n",
        "\n",
        "    #     # Create vectors for each vertex\n",
        "    #     vector1 = [words1.get(word, 0) for word in all_words]\n",
        "    #     vector2 = [words2.get(word, 0) for word in all_words]\n",
        "    #     # Compute the cosine similarity\n",
        "    #     vec1 = np.array(vector1).reshape(1, -1)\n",
        "    #     vec2 = np.array(vector2).reshape(1, -1)\n",
        "    #     similarity = cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "    #     return similarity\n",
        "\n",
        "    # def _find_similarity(self, vert1, vert2):\n",
        "    #   # Prepare the data for TF-IDF\n",
        "    #   sentences = [self.original_sentences[vert1.name], self.original_sentences[vert2.name]]\n",
        "    #   vectorizer = TfidfVectorizer()\n",
        "    #   tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    #   # Calculate cosine similarity\n",
        "    #   similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "    #   return similarity\n",
        "\n",
        "    def _find_similarity(self, vert1, vert2):\n",
        "        # Get sentence embeddings using a pre-trained SentenceTransformer model\n",
        "        sentence1 = self.original_sentences[vert1.name]\n",
        "        sentence2 = self.original_sentences[vert2.name]\n",
        "        embedding1 = self.model.encode(sentence1, convert_to_tensor=True)\n",
        "        embedding2 = self.model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
        "        return similarity\n",
        "\n",
        "    # def _find_similarity(self, vert1, vert2):\n",
        "    #     # Combine TF-IDF cosine similarity and Sentence Embedding cosine similarity\n",
        "    #     tfidf_similarity = self._word_frequency_cosine_similarity(vert1, vert2)\n",
        "    #     embedding_similarity = self._sentence_embedding_cosine_similarity(vert1, vert2)\n",
        "    #     # Weighted average or any other combination method can be applied\n",
        "    #     combined_similarity = 0.0*tfidf_similarity + 1.0*embedding_similarity\n",
        "    #     return combined_similarity\n",
        "    def _sort_vertices_by_weight(self):\n",
        "        self.vertices.sort(key=lambda vert: vert.weight, reverse=True)\n"
      ],
      "metadata": {
        "id": "H7g4CbglT-wr"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"The solar system is a fascinating and complex entity, consisting of the Sun and all the objects that orbit around it, including the eight planets and their moons, as well as dwarf planets, comets, and asteroids. The Sun, a star at the center, provides the necessary heat and light to sustain life on Earth. Each planet has unique characteristics, such as Mars with its red surface, Jupiter with its Great Red Spot, and Saturn with its rings. Understanding the solar system helps scientists learn more about the origins of the universe and the potential for life beyond Earth.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pz2KBg2UAQc",
        "outputId": "a8bea299-a509-4c22-84fb-55d5ea852079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The solar system is a fascinating and complex entity, consisting of the Sun and all the objects that orbit around it, including the eight planets and their moons, as well as dwarf planets, comets, and asteroids. The Sun, a star at the center, provides the necessary heat and light to sustain life on Earth. Each planet has unique characteristics, such as Mars with its red surface, Jupiter with its Great Red Spot, and Saturn with its rings. Understanding the solar system helps scientists learn more about the origins of the universe and the potential for life beyond Earth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "The solar system is a fascinating and complex entity, consisting of the Sun and all the objects that orbit around it, including the eight planets and their moons, as well as dwarf planets, comets, and asteroids. The Sun, a star at the center, provides the necessary heat and light to sustain life on Earth. Each planet has unique characteristics, such as Mars with its red surface, Jupiter with its Great Red Spot, and Saturn with its rings. Understanding the solar system helps scientists learn more about the origins of the universe and the potential for life beyond Earth.\n",
        "The solar system is a fascinating and complex entity, consisting of the Sun and all the objects that orbit around it, including the eight planets and their moons, as well as dwarf planets, comets, and asteroids. The Sun, a star at the center, provides the necessary heat and light to sustain life on Earth. Each planet has unique characteristics, such as Mars with its red surface, Jupiter with its Great Red Spot, and Saturn with its rings. Understanding the solar system helps scientists learn more about the origins of the universe and the potential for life beyond Earth.\n"
      ],
      "metadata": {
        "id": "xe3FRH5CVBCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## consine"
      ],
      "metadata": {
        "id": "Jv-oFtt_MJtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Hello, I am depressed. yes, but no one loves me I should kill my self. How to find the right person. may be I will. I hate everyone I hate every body, no one cares about me. I just think I should die. yes, but I think family or friends wont help me. may be I will try to talk with my family. I will try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Dkjg3GiqAw",
        "outputId": "13ace627-2549-454c-e9b9-be97a6123c8c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " yes, but no one loves me I should kill my self.  yes, but I think family or friends wont help me.  may be I will try to talk with my family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Hello, I am depressed. yes, but no one loves me I should kill my self. How to find the right person. may be I will. I hate everyone I hate every body, no one cares about me. I just think I should die. yes, but I think family or friends wont help me. may be I will try to talk with my family. I will try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-MM4WRL9P4",
        "outputId": "b3131dae-c134-4cae-f289-c21b1083e9e9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " yes, but no one loves me I should kill my self.  yes, but I think family or friends wont help me.  may be I will try to talk with my family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Hello, I am depressed. yes, but no one loves me I should kill my self. How to find the right person. may be I will. I hate everyone I hate every body, no one cares about me. I just think I should die. yes, but I think family or friends wont help me. may be I will try to talk with my family. I will try.\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_6X6fDOW5RK",
        "outputId": "66175b30-f366-4cbf-8e1c-ab64469d3043"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " yes, but no one loves me I should kill my self.  I just think I should die.  may be I will try to talk with my family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Hello, I am depressed. yes, but no one loves me I should kill my self. How to find the right person. may be I will. I hate everyone I hate every body, no one cares about me. I just think I should die. yes, but I think family or friends wont help me. may be I will try to talk with my family. I will try.\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gec89JVV0cjK",
        "outputId": "124fe6ec-8807-4359-fc67-d5767ec30e89"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " yes, but no one loves me I should kill my self.  may be I will.  may be I will try to talk with my family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"I guess. Just been feeling a bit overwhelmed lately. Well, work has been really stressful, and I've been having trouble sleeping. Not really. I've tried a few things, like deep breathing and taking walks, but nothing seems to help much. No do you think seeing a therapist will be help? Yeah, I think that could be really helpful. I'm willing to give it a try. Thanks, doc. I appreciate it. It feels good to know I have someone to talk to about this stuff. Do you like video games? you may should try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnE7CRER0lJF",
        "outputId": "c2a356ae-075d-482d-eb1c-e80d5259646a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Well, work has been really stressful, and I've been having trouble sleeping.  I've tried a few things, like deep breathing and taking walks, but nothing seems to help much.  Yeah, I think that could be really helpful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"I guess. Just been feeling a bit overwhelmed lately. Well, work has been really stressful, and I've been having trouble sleeping. Not really. I've tried a few things, like deep breathing and taking walks, but nothing seems to help much. No do you think seeing a therapist will be help? Yeah, I think that could be really helpful. I'm willing to give it a try. Thanks, doc. I appreciate it. It feels good to know I have someone to talk to about this stuff. Do you like video games? you may should try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0_BJKytLzrJ",
        "outputId": "0eba57a3-1ac7-4512-d728-a52d16d8484b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Well, work has been really stressful, and I've been having trouble sleeping.  I've tried a few things, like deep breathing and taking walks, but nothing seems to help much.  Yeah, I think that could be really helpful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"I guess. Just been feeling a bit overwhelmed lately. Well, work has been really stressful, and I've been having trouble sleeping. Not really. I've tried a few things, like deep breathing and taking walks, but nothing seems to help much. No do you think seeing a therapist will be help? Yeah, I think that could be really helpful. I'm willing to give it a try. Thanks, doc. I appreciate it. It feels good to know I have someone to talk to about this stuff. Do you like video games? you may should try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swQIdqXTAok4",
        "outputId": "c9e0d379-cf3d-409d-a20f-3d8370c95716"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Well, work has been really stressful, and I've been having trouble sleeping.  No do you think seeing a therapist will be help.  I'm willing to give it a try.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"I guess. Just been feeling a bit overwhelmed lately. Well, work has been really stressful, and I've been having trouble sleeping. Not really. I've tried a few things, like deep breathing and taking walks, but nothing seems to help much. No do you think seeing a therapist will be help? Yeah, I think that could be really helpful. I'm willing to give it a try. Thanks, doc. I appreciate it. It feels good to know I have someone to talk to about this stuff. Do you like video games? you may should try.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "EWuRXueDA-Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c5225d-9ebe-4187-a76a-18b609cdaf23"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yeah, I think that could be really helpful.  I'm willing to give it a try.  It feels good to know I have someone to talk to about this stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Not great, to be honest. User've been feeling very low and hopeless lately. Yes, but User don't see any point in living anymore. User just feel no one loves User. what should User do? User don't believe Assistant. Nothing ever changes for User. User've tried everything, and nothing works. That is maybe good. Thank Assistant. User don't know. User guess User can try. User have nothing to lose. Haha right. Mmmm User like to go clubbing and swim. Yes, swimming changes User's mood. Thank Assistant, goodbye.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qf6q7uHYzOl",
        "outputId": "02fd06fe-dac4-45a4-8e12-cb06e0f528fb"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " User just feel no one loves User.  what should User do.  User don't believe Assistant.  User guess User can try.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Not great, to be honest. User've been feeling very low and hopeless lately. Yes, but User don't see any point in living anymore. User just feel no one loves User. what should User do? User don't believe Assistant. Nothing ever changes for User. User've tried everything, and nothing works. That is maybe good. Thank Assistant. User don't know. User guess User can try. User have nothing to lose. Haha right. Mmmm User like to go clubbing and swim. Yes, swimming changes User's mood. Thank Assistant, goodbye.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DegotSMcZhiE",
        "outputId": "2747a781-164e-4568-a65a-59084dddc57e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " User just feel no one loves User.  what should User do.  User don't believe Assistant.  User guess User can try.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Not great, to be honest. User've been feeling very low and hopeless lately. Yes, but User don't see any point in living anymore. User just feel no one loves User. what should User do? User don't believe Assistant. Nothing ever changes for User. User've tried everything, and nothing works. That is maybe good. Thank Assistant. User don't know. User guess User can try. User have nothing to lose. Haha right. Mmmm User like to go clubbing and swim. Yes, swimming changes User's mood. Thank Assistant, goodbye.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfeJ4_1kZq5m",
        "outputId": "e04d0541-d873-492a-c234-17329c1e058c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yes, but User don't see any point in living anymore.  User just feel no one loves User.  what should User do.  User have nothing to lose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = SummaryBot()\n",
        "summary = bot.run(\"Not great, to be honest. User've been feeling very low and hopeless lately. Yes, but User don't see any point in living anymore. User just feel no one loves User. what should User do? User don't believe Assistant. Nothing ever changes for User. User've tried everything, and nothing works. That is maybe good. Thank Assistant. User don't know. User guess User can try. User have nothing to lose. Haha right. Mmmm User like to go clubbing and swim. Yes, swimming changes User's mood. Thank Assistant, goodbye.\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-tMKLD7ZpqS",
        "outputId": "60998545-ece7-47c6-93f8-3060224dca03"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yes, but User don't see any point in living anymore.  what should User do.  User don't know.  User have nothing to lose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "all_words= [\"i\",\"love\",\"better\"]\n",
        "words_lemmatized = [lemmatizer.lemmatize(word.lower(), pos=\"a\") for word in all_words]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqOQRLcc5qNS",
        "outputId": "e9e05e46-60c4-42ac-a9c5-8c973e6df514"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_lemmatized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3HYNOaJ-fsu",
        "outputId": "d398c886-037f-41c4-f028-fd21b398bbaf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'good']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [\"I do not like sadness\"]\n",
        "\n",
        "model_outputs = classifier(sentences)\n",
        "print(model_outputs[0])\n",
        "print(mental_health_score(model_outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKZtoAd__Nqr",
        "outputId": "e299e32e-fdbb-430b-b475-cad67f7530ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'disapproval', 'score': 0.5441403388977051}, {'label': 'sadness', 'score': 0.3584756851196289}, {'label': 'disappointment', 'score': 0.13139913976192474}, {'label': 'annoyance', 'score': 0.04687810689210892}, {'label': 'neutral', 'score': 0.03726961463689804}, {'label': 'anger', 'score': 0.02318556047976017}, {'label': 'realization', 'score': 0.011338314972817898}, {'label': 'approval', 'score': 0.01024869829416275}, {'label': 'disgust', 'score': 0.009279333986341953}, {'label': 'caring', 'score': 0.008014443330466747}, {'label': 'remorse', 'score': 0.006530173122882843}, {'label': 'love', 'score': 0.006400663871318102}, {'label': 'joy', 'score': 0.004949627444148064}, {'label': 'confusion', 'score': 0.0046013290993869305}, {'label': 'grief', 'score': 0.004066526889801025}, {'label': 'embarrassment', 'score': 0.003994239494204521}, {'label': 'fear', 'score': 0.003558812430128455}, {'label': 'nervousness', 'score': 0.0032670439686626196}, {'label': 'desire', 'score': 0.0029782289639115334}, {'label': 'amusement', 'score': 0.002443612553179264}, {'label': 'curiosity', 'score': 0.0021923065651208162}, {'label': 'optimism', 'score': 0.0021619629114866257}, {'label': 'admiration', 'score': 0.0020681098103523254}, {'label': 'surprise', 'score': 0.0019914782606065273}, {'label': 'gratitude', 'score': 0.0019118540221825242}, {'label': 'relief', 'score': 0.001653774525038898}, {'label': 'excitement', 'score': 0.0016481102211400867}, {'label': 'pride', 'score': 0.0003926441422663629}]\n",
            "0.921050681083145\n"
          ]
        }
      ]
    }
  ]
}