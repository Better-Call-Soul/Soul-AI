{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import neattext.functions as nfx\n",
    "# # Download NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MELD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my companys tr...</td>\n",
       "      <td>also I was the point person on my companys tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You mustve had your hands full.</td>\n",
       "      <td>You mustve had your hands full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>That I did That I did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So lets talk a little bit about your duties.</td>\n",
       "      <td>So lets talk a little bit about your duties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>My duties  All right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>You or me?</td>\n",
       "      <td>You or me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>I got it Uh Joey women dont have Adams apples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>You guys are messing with me right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>Yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>That was a good one For a second there I was l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Utterance  \\\n",
       "0     also I was the point person on my companys tr...   \n",
       "1                      You mustve had your hands full.   \n",
       "2                               That I did. That I did.   \n",
       "3         So lets talk a little bit about your duties.   \n",
       "4                                My duties?  All right.   \n",
       "...                                                 ...   \n",
       "9984                                         You or me?   \n",
       "9985  I got it. Uh, Joey, women don't have Adam's ap...   \n",
       "9986               You guys are messing with me, right?   \n",
       "9987                                              Yeah.   \n",
       "9988  That was a good one. For a second there, I was...   \n",
       "\n",
       "                                             Clean_Text  \n",
       "0     also I was the point person on my companys tra...  \n",
       "1                        You mustve had your hands full  \n",
       "2                                 That I did That I did  \n",
       "3           So lets talk a little bit about your duties  \n",
       "4                                  My duties  All right  \n",
       "...                                                 ...  \n",
       "9984                                          You or me  \n",
       "9985      I got it Uh Joey women dont have Adams apples  \n",
       "9986                 You guys are messing with me right  \n",
       "9987                                               Yeah  \n",
       "9988  That was a good one For a second there I was l...  \n",
       "\n",
       "[9989 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = r'D:\\College\\Fourth Year\\GP\\Meld\\train_sent_emo.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df.isnull().sum()\n",
    "# df['Emotion'].value_counts().plot(kind='bar')\n",
    "df['Clean_Text'] = df['Utterance'].apply(nfx.remove_multiple_spaces)\n",
    "df['Clean_Text'] = df['Utterance'].apply(nfx.remove_bad_quotes)\n",
    "df['Clean_Text'] = df['Utterance'].apply(nfx.remove_special_characters)\n",
    "df[['Utterance','Clean_Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = df['Clean_Text']\n",
    "y = df['Emotion']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_class_weight(y_train):\n",
    "    # Define the class labels\n",
    "    class_labels = ['neutral', 'joy', 'surprise', 'anger', 'sadness', 'disgust', 'fear']\n",
    "\n",
    "    # Map class labels to their corresponding indices\n",
    "    class_indices = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "    # Convert emotions to class indices\n",
    "    class_indices_array = np.array([class_indices[emotion] for emotion in y_train])\n",
    "\n",
    "    # Calculate class weights based on the inverse of class frequencies\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(class_indices_array), y=class_indices_array)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_labels = label_encoder.fit_transform(y_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    return class_weight_dict,class_indices_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model, param_grid, X_train, y_train, X_test, y_test):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Best model accuracy:\", accuracy)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3958958958958959\n"
     ]
    }
   ],
   "source": [
    "class_labels = ['neutral', 'joy', 'surprise', 'anger', 'sadness', 'disgust', 'fear']\n",
    "class_weight,y_train_indices = update_class_weight(y_train)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear',class_weight=class_weight)\n",
    "\n",
    "# Fit the SVM classifier on the TF-IDF transformed training data\n",
    "svm_classifier.fit(X_train_tfidf, y_train_indices)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "# Convert emotions to class indices\n",
    "class_indices_array = np.array([class_indices[emotion] for emotion in y_test])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(class_indices_array, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters found:\n",
      "{'C': 10.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best model accuracy: 0.5145145145145145\n",
      "Accuracy: 0.3958958958958959\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "best_model = tune_hyperparameters(svm_classifier, param_grid, X_train_tfidf, y_train_indices, X_test_tfidf, class_indices_array)\n",
    "best_model.fit(X_train_tfidf, y_train_indices)\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "# Convert emotions to class indices\n",
    "class_indices_array = np.array([class_indices[emotion] for emotion in y_test])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(class_indices_array, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36436436436436437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(class_weight=class_weight)\n",
    "logistic_regression.fit(X_train_tfidf, y_train_indices)\n",
    "y_pred2 = logistic_regression.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(class_indices_array, y_pred2)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters found:\n",
      "{'C': 10.0, 'penalty': 'l2'}\n",
      "Best model accuracy: 0.3953953953953954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_model = tune_hyperparameters(logistic_regression, param_grid, X_train_tfidf, y_train_indices, X_test_tfidf, class_indices_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3813813813813814\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(class_weight=class_weight)\n",
    "decision_tree.fit(X_train_tfidf, y_train_indices)\n",
    "y_pred3 = decision_tree.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(class_indices_array, y_pred3)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.534034034034034"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])\n",
    "pipe_lr.fit(X_train,y_train)\n",
    "pipe_lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DAILY DIALOGUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spellchecker\n",
      "  Using cached spellchecker-0.4-py3-none-any.whl\n",
      "Collecting setuptools (from spellchecker)\n",
      "  Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting inexactsearch (from spellchecker)\n",
      "  Using cached inexactsearch-1.0.2-py3-none-any.whl\n",
      "Collecting soundex>=1.0 (from inexactsearch->spellchecker)\n",
      "  Using cached soundex-1.1.3-py3-none-any.whl\n",
      "Collecting silpa-common>=0.3 (from inexactsearch->spellchecker)\n",
      "  Using cached silpa_common-0.3-py3-none-any.whl\n",
      "Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "Installing collected packages: silpa-common, setuptools, soundex, inexactsearch, spellchecker\n",
      "  Attempting uninstall: silpa-common\n",
      "    Found existing installation: silpa_common 0.3\n",
      "    Uninstalling silpa_common-0.3:\n",
      "      Successfully uninstalled silpa_common-0.3\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.5.1\n",
      "    Uninstalling setuptools-69.5.1:\n",
      "      Successfully uninstalled setuptools-69.5.1\n",
      "  Attempting uninstall: soundex\n",
      "    Found existing installation: soundex 1.1.3\n",
      "    Uninstalling soundex-1.1.3:\n",
      "      Successfully uninstalled soundex-1.1.3\n",
      "  Attempting uninstall: inexactsearch\n",
      "    Found existing installation: inexactsearch 1.0.2\n",
      "    Uninstalling inexactsearch-1.0.2:\n",
      "      Successfully uninstalled inexactsearch-1.0.2\n",
      "  Attempting uninstall: spellchecker\n",
      "    Found existing installation: spellchecker 0.4\n",
      "    Uninstalling spellchecker-0.4:\n",
      "      Successfully uninstalled spellchecker-0.4\n",
      "Successfully installed inexactsearch-1.0.2 setuptools-69.5.1 silpa-common-0.3 soundex-1.1.3 spellchecker-0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Callable, Dict\n",
    "import argparse\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, classification_report, confusion_matrix)\n",
    "import numpy as np\n",
    "import contractions\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"D:/College/Fourth Year/GP/dailydialog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = f\"{dataset_path}/train/dialogues_train.txt\"\n",
    "train_label_path = f\"{dataset_path}/train/dialogues_emotion_train.txt\"\n",
    "test_data_path = f\"{dataset_path}/test/dialogues_test.txt\"\n",
    "test_label_path = f\"{dataset_path}/test/dialogues_emotion_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_sentence(sentence: str) -> str:\n",
    "    '''\n",
    "    Lowercase the sentence.\n",
    "    :param data: The sentence to lowercase.\n",
    "    :return: The lowercased sentence\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove emails from the sentence.\n",
    "    :param sentence: The sentence to remove emails from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without emails.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(r\"\\S*@\\S*\\s?\", \"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii_diacritic(sentence: str) -> str:\n",
    "    '''\n",
    "\n",
    "    Remove diacritics from the sentence.\n",
    "\n",
    "    :param sentence: The sentence to remove diacritics from.\n",
    "\n",
    "    :type sentence: str\n",
    "\n",
    "    :return: The sentence without diacritics.\n",
    "\n",
    "    :rtype: str\n",
    "    '''\n",
    "\n",
    "    return unicodedata.normalize(\"NFKD\", sentence).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove HTML tags from the sentence.\n",
    "    :param sentence: The sentence to remove HTML tags from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without HTML tags.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return BeautifulSoup(sentence, \"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeated_chars(sentence: str) -> str:\n",
    "    '''\n",
    "    Replace repeated characters in the sentence.\n",
    "    :param sentence: The sentence to replace repeated characters in.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with replaced repeated characters.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Replace consecutive occurrences of ',', '!', '.', and '?' with a single occurrence\n",
    "    return re.sub(r'([,!?.])\\1+', r'\\1', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emojis_to_text(sentence: str) -> str:\n",
    "    '''\n",
    "    Translate emojis in the sentence to text.\n",
    "    :param sentence: The sentence to translate emojis to text.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with translated emojis to text.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Translate emojis to text codes\n",
    "    translated_text = emoji.demojize(sentence)\n",
    "    # Remove colons from the translated text\n",
    "    translated_text = re.sub(r':', '', translated_text)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_sentence(sentence: str) -> str:\n",
    "    '''\n",
    "    Expand the contractions in the sentence.\n",
    "    :param sentence: The sentence to expand contractions in.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with expanded contractions.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return contractions.fix(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove URLs from the sentence.\n",
    "    :param sentence: The sentence to remove URLs from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without URLs.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\", '', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_possessives(sentence: str) -> str:\n",
    "    '''\n",
    "    Strip possessives from the sentence.\n",
    "    :param sentence: The sentence to strip possessives from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without possessives.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Stripping the possessives\n",
    "    sentence = sentence.replace(\"'s\", '')\n",
    "    sentence = sentence.replace('’s', '')\n",
    "    sentence = sentence.replace('s’', 's')\n",
    "    sentence = sentence.replace(\"s'\", 's')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_space(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove extra spaces from the sentence.\n",
    "    :param sentence: The sentence to remove extra spaces from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without extra spaces.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(r'\\s+', ' ', sentence).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence: str) -> list[str]:\n",
    "    '''\n",
    "    Tokenize the sentence.\n",
    "    :param sentence: The sentence to tokenize.\n",
    "    :type sentence: str\n",
    "    :return: The tokenized sentence.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Remove stop words from the sentence.\n",
    "    :param sentence: The sentence to remove stop words from.\n",
    "    :type sentence: list[str]\n",
    "    :return: The sentence without stop words.\n",
    "    :rtype: list[str]\n",
    "    '''\n",
    "    return [word for word in sentence if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_sentence(sentence: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Lemmatize the sentence.\n",
    "    :param sentence: The sentence to lemmatize.\n",
    "    :type sentence: list[str]\n",
    "    :return: The lemmatized sentence.\n",
    "    :rtype: list[str]\n",
    "    '''\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(sentence)\n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        # Map Penn Treebank POS tags to WordNet POS tags\n",
    "        if pos.startswith('N'):  # Nouns\n",
    "            pos = 'n'\n",
    "        elif pos.startswith('V'):  # Verbs\n",
    "            pos = 'v'\n",
    "        elif pos.startswith('J'):  # Adjectives\n",
    "            pos = 'a'\n",
    "        elif pos.startswith('R'):  # Adverbs\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            pos = 'n'  # Default to noun if POS tag not found\n",
    "\n",
    "        # Lemmatize the word using the appropriate POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemmatized_words.append(lemma)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train(line: str) -> list[str]:\n",
    "    '''\n",
    "    Clean the line and return it as a list of tokens\n",
    "    :param line: the line to clean\n",
    "    :type line: str\n",
    "    :return: the cleaned line as a list of tokens\n",
    "    :rtype: list\n",
    "    '''\n",
    "    # translate emojis\n",
    "    line = translate_emojis_to_text(line)\n",
    "    # lower the line\n",
    "    line = lower_sentence(line)\n",
    "    # remove non ascii\n",
    "    line = remove_nonascii_diacritic(line)\n",
    "    # remove emails\n",
    "    line = remove_emails(line)\n",
    "    # remove html\n",
    "    line = clean_html(line)\n",
    "    # remove urls\n",
    "    line = remove_url(line)\n",
    "    # replace repeated chars\n",
    "    line = replace_repeated_chars(line)\n",
    "    # expand\n",
    "    line = expand_sentence(line)\n",
    "    # remove possessives\n",
    "    line = remove_possessives(line)\n",
    "    # remove extra spaces\n",
    "    line = remove_extra_space(line)\n",
    "    # tekonize\n",
    "    line = tokenize_sentence(line)\n",
    "    # remove stopwords\n",
    "    line = remove_stop_words(line)\n",
    "    # lemmetization\n",
    "    line = lemm_sentence(line)\n",
    "    if len(line) == 0:\n",
    "        return ['Normal']\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dialogue_data(file_path: str) -> List[List[str]]:\n",
    "    '''\n",
    "    Read the dialogue data from the file path.\n",
    "    :param file_path: The path of the file.\n",
    "    :type file_path: str\n",
    "    :return: A list of dialogues, where each dialogue is a list of sentences,\n",
    "             and each sentence is a string.\n",
    "    :rtype: list\n",
    "    '''\n",
    "    # define dialogues list\n",
    "    dialogues = []\n",
    "    # read data file\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as file_data:\n",
    "        dialogues = [\n",
    "            [\n",
    "                sentence.replace(\".\", \" . \").replace(\"?\", \" ? \").replace(\"!\", \" ! \").replace(\n",
    "                    \";\", \" ; \").replace(\":\", \" : \").replace(\",\", \" , \").strip()\n",
    "                for sentence in line.split(\"__eou__\") if sentence.strip()\n",
    "            ]\n",
    "            for line in file_data\n",
    "        ]\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dataset_dailyDialog(data_path: str, label_path: str) -> Tuple[List[List[List[str]]], List[List[int]]]:\n",
    "    '''\n",
    "    Take the data path and the label path and read them.\n",
    "    It then splits the conversations and extracts each conversation, sentence, and words of each sentence.\n",
    "    It reads the labels of each sentence in the conversation\n",
    "\n",
    "    :param data_path: The path of the conversations.\n",
    "    :type data_path: str\n",
    "    :param label_path: The path of the labels for the conversations.\n",
    "    :type label_path: str\n",
    "    :return: A tuple containing inputs and targets.\n",
    "             inputs: List of conversations, where each conversation is a list of sentences,\n",
    "                     and each sentence is a list of words.\n",
    "             targets: List of labels for each conversation.\n",
    "    :rtype: tuple\n",
    "    '''\n",
    "    # define targets list\n",
    "    targets = []\n",
    "    # read labels file\n",
    "    with open(label_path, \"r\", encoding=\"utf8\") as file_data:\n",
    "        targets = [[int(label) for label in line.strip(\n",
    "            \"\\n\").strip(\" \").split(\" \")] for line in file_data]\n",
    "        # for loop version\n",
    "        # for line in file_data:\n",
    "        #     labels = [int(label) for label in line.strip(\"\\n\").strip(\" \").split(\" \")]\n",
    "        #     targets.append(labels)\n",
    "\n",
    "    # read data file\n",
    "    dialogues = read_dialogue_data(data_path)\n",
    "    # define inputs list\n",
    "    inputs = [\n",
    "        [\n",
    "            clean_train(sentence) for sentence in dialogue\n",
    "        ]\n",
    "        for dialogue in dialogues\n",
    "    ]\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_1888\\964458276.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11118\n",
      "11118\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = read_dataset_dailyDialog(train_data_path, train_label_path)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_1888\\964458276.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = read_dataset_dailyDialog(test_data_path, test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.842764857881137\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train and X_test are lists of lists\n",
    "# Flatten X_train into a list of strings\n",
    "flattened_X_train = [' '.join(sentence) for dialog in X_train for sentence in dialog]\n",
    "# Flatten X_train into a list of strings\n",
    "flattened_X_test = [' '.join(sentence) for dialog in X_test for sentence in dialog]\n",
    "\n",
    "# Flatten y_train into a single list\n",
    "y_train_flat = [label for dialog_labels in y_train for label in dialog_labels]\n",
    "y_test_flat = [label for dialog_labels in y_test for label in dialog_labels]\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Fit the SVM classifier on the TF-IDF transformed training data\n",
    "svm_classifier.fit(X_train_tfidf, y_train_flat)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_path):\n",
    "    \"\"\"\n",
    "    Save a trained model to a file using pickle.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model object\n",
    "    - save_path: File path to save the model\n",
    "    \"\"\"\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved to:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML\n"
     ]
    }
   ],
   "source": [
    "save_model(svm_classifier,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no emotion (0), anger (1), disgust (2), fear (3), happiness (4), sadness (5) and surprise (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Example of a new sentence\n",
    "new_sentence = \"I don't know . My life is a big mess .\"\n",
    "\n",
    "preprocessed_text = clean_train(new_sentence)\n",
    "\n",
    "# Transform the preprocessed sentence into TF-IDF representation\n",
    "tfidf_vectorized_sentence = tfidf_vectorizer.transform(preprocessed_text)\n",
    "\n",
    "# Predict the label for the TF-IDF transformed sentence\n",
    "predicted_label = svm_classifier.predict(tfidf_vectorized_sentence)\n",
    "\n",
    "print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6600775193798449\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weight_dict = dict(zip(np.unique(y_train_flat), class_weights))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "\n",
    "# Fit the SVM classifier on the TF-IDF transformed training data\n",
    "svm_classifier.fit(X_train_tfidf, y_train_flat)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML_weighted\n"
     ]
    }
   ],
   "source": [
    "save_model(svm_classifier,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad']\n",
      "Predicted label: [5]\n"
     ]
    }
   ],
   "source": [
    "# Example of a new sentence\n",
    "new_sentence = \"sad\"\n",
    "\n",
    "preprocessed_text = clean_train(new_sentence)\n",
    "print(preprocessed_text)\n",
    "# Transform the preprocessed sentence into TF-IDF representation\n",
    "tfidf_vectorized_sentence = tfidf_vectorizer.transform(preprocessed_text)\n",
    "\n",
    "# Predict the label for the TF-IDF transformed sentence\n",
    "predicted_label = svm_classifier.predict(tfidf_vectorized_sentence)\n",
    "\n",
    "print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6076227390180878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.61      0.73      6321\n",
      "           1       0.11      0.50      0.18       118\n",
      "           2       0.08      0.32      0.13        47\n",
      "           3       0.08      0.59      0.14        17\n",
      "           4       0.40      0.64      0.49      1019\n",
      "           5       0.12      0.58      0.20       102\n",
      "           6       0.10      0.55      0.18       116\n",
      "\n",
      "    accuracy                           0.61      7740\n",
      "   macro avg       0.26      0.54      0.29      7740\n",
      "weighted avg       0.81      0.61      0.67      7740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Flatten X_train into a list of strings\n",
    "flattened_X_train = [' '.join(sentence) for dialog in X_train for sentence in dialog]\n",
    "# Flatten X_train into a list of strings\n",
    "flattened_X_test = [' '.join(sentence) for dialog in X_test for sentence in dialog]\n",
    "\n",
    "# Flatten y_train into a single list\n",
    "y_train_flat = [label for dialog_labels in y_train for label in dialog_labels]\n",
    "y_test_flat = [label for dialog_labels in y_test for label in dialog_labels]\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weight_dict = dict(zip(np.unique(y_train_flat), class_weights))\n",
    "\n",
    "logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
    "\n",
    "logistic_regression.fit(X_train_tfidf, y_train_flat)\n",
    "y_pred2 = logistic_regression.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test_flat, y_pred2)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test_flat, y_pred2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    0: 'neutral',\n",
    "    1: 'anger',\n",
    "    2: 'disgust',\n",
    "    3: 'fear',\n",
    "    4: 'happiness',\n",
    "    5: 'sadness',\n",
    "    6: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay', ',', 'give', 'try', '.', 'still', 'scar', 'doubtful', '.']\n",
      "Probability of neutral: 0.06441659914938573\n",
      "Probability of anger: 0.007847991876326342\n",
      "Probability of disgust: 0.005675938055684583\n",
      "Probability of fear: 0.8110976504209492\n",
      "Probability of happiness: 0.06154204997979236\n",
      "Probability of sadness: 0.020639569339246187\n",
      "Probability of surprise: 0.028780201178615725\n",
      "Predicted emotion: fear\n"
     ]
    }
   ],
   "source": [
    "# Example of a new sentence\n",
    "new_sentence = \"Okay, I'll give it a try. But I'm still scared and doubtful.\"\n",
    "\n",
    "preprocessed_text = clean_train(new_sentence)\n",
    "print(preprocessed_text)\n",
    "# Transform the preprocessed sentence into TF-IDF representation\n",
    "tfidf_vectorized_sentence = tfidf_vectorizer.transform([\" \".join(preprocessed_text)])\n",
    "\n",
    "# Predict the label for the TF-IDF transformed sentence\n",
    "predicted_emotion = logistic_regression.predict(tfidf_vectorized_sentence)[0]\n",
    "\n",
    "class_probabilities = logistic_regression.predict_proba(tfidf_vectorized_sentence)[0]\n",
    "\n",
    "\n",
    "emotion_probabilities = {emotion_map[i]: prob for i, prob in enumerate(class_probabilities)}\n",
    "\n",
    "# Output the probabilities for each class\n",
    "for emotion, probability in emotion_probabilities.items():\n",
    "    print(f\"Probability of {emotion}: {probability}\")\n",
    "\n",
    "# Get the predicted emotion (the one with the highest probability)\n",
    "# predicted_emotion = max(emotion_probabilities, key=emotion_probabilities.get)\n",
    "print(\"Predicted emotion:\", emotion_map[predicted_emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression\n"
     ]
    }
   ],
   "source": [
    "save_model(logistic_regression,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/College/Fourth Year/GP/glove_vectors.pkl', 'rb') as f:\n",
    "    glove_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with GloVe embeddings: 0.4904392764857881\n",
      "Classification Report with GloVe embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.47      0.63      6321\n",
      "           1       0.08      0.39      0.13       118\n",
      "           2       0.04      0.43      0.07        47\n",
      "           3       0.02      0.41      0.04        17\n",
      "           4       0.38      0.59      0.46      1019\n",
      "           5       0.08      0.52      0.14       102\n",
      "           6       0.09      0.60      0.16       116\n",
      "\n",
      "    accuracy                           0.49      7740\n",
      "   macro avg       0.23      0.49      0.23      7740\n",
      "weighted avg       0.82      0.49      0.58      7740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your preprocessing function using GloVe embeddings\n",
    "def preprocess_with_glove(sentence, glove_vectors):\n",
    "    vectorized_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word in glove_vectors:\n",
    "            vectorized_sentence.append(glove_vectors[word])\n",
    "    return np.mean(vectorized_sentence, axis=0) if vectorized_sentence else np.zeros_like(list(glove_vectors.values())[0])\n",
    "\n",
    "# Transform the training data using GloVe embeddings\n",
    "X_train_glove = [preprocess_with_glove(sentence, glove_vectors) for sentence in flattened_X_train]\n",
    "X_train_glove = np.vstack(X_train_glove)\n",
    "\n",
    "# Transform the testing data using GloVe embeddings\n",
    "X_test_glove = [preprocess_with_glove(sentence, glove_vectors) for sentence in flattened_X_test]\n",
    "X_test_glove = np.vstack(X_test_glove)\n",
    "\n",
    "# Define logistic regression model with class weights\n",
    "logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
    "\n",
    "# Fit logistic regression model\n",
    "logistic_regression.fit(X_train_glove, y_train_flat)\n",
    "\n",
    "# Predict on testing data\n",
    "y_pred_glove = logistic_regression.predict(X_test_glove)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_glove = accuracy_score(y_test_flat, y_pred_glove)\n",
    "print(\"Accuracy with GloVe embeddings:\", accuracy_glove)\n",
    "\n",
    "# Calculate classification report\n",
    "report_glove = classification_report(y_test_flat, y_pred_glove)\n",
    "print(\"Classification Report with GloVe embeddings:\")\n",
    "print(report_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: sadness\n",
      "Probability of neutral: 0.18763722940773414\n",
      "Probability of anger: 0.030659054586403434\n",
      "Probability of disgust: 0.11225866478299383\n",
      "Probability of fear: 0.11656221603375616\n",
      "Probability of happiness: 0.15791877457306758\n",
      "Probability of sadness: 0.35241141926781683\n",
      "Probability of surprise: 0.042552641348227956\n"
     ]
    }
   ],
   "source": [
    "# Example of a new sentence\n",
    "new_sentence = \" I don't know. I guess I can try. I don't have anything to lose.\"\n",
    "\n",
    "# Preprocess the new sentence using GloVe embeddings\n",
    "preprocessed_sentence = preprocess_with_glove(new_sentence, glove_vectors)\n",
    "\n",
    "# Reshape the preprocessed sentence to match the expected input shape\n",
    "preprocessed_sentence = preprocessed_sentence.reshape(1, -1)\n",
    "\n",
    "# Predict the label for the preprocessed sentence\n",
    "predicted_label = logistic_regression.predict(preprocessed_sentence)[0]\n",
    "\n",
    "# Predict the probabilities for each class for the preprocessed sentence\n",
    "class_probabilities = logistic_regression.predict_proba(preprocessed_sentence)[0]\n",
    "\n",
    "# Create a dictionary mapping emotions to probabilities\n",
    "emotion_probabilities = {emotion_map[i]: prob for i, prob in enumerate(class_probabilities)}\n",
    "\n",
    "# Output the predicted label\n",
    "print(\"Predicted label:\", emotion_map[predicted_label])\n",
    "\n",
    "# Output the probabilities for each class\n",
    "for emotion, probability in emotion_probabilities.items():\n",
    "    print(f\"Probability of {emotion}: {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression_glv\n"
     ]
    }
   ],
   "source": [
    "save_model(logistic_regression,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression_glv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
