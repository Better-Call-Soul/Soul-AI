{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZRLKbimTb7Y",
        "outputId": "8e340783-0a30-4516-bfde-c707fe2dce0f"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Callable, Dict\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import torch.autograd\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
        "                             recall_score, classification_report, confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "import contractions\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "import emoji\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9OlOvht5UEha"
      },
      "outputs": [],
      "source": [
        "dataset_path = '../data/raw/suicidal_detection/Suicide_Detection.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TtGBgeE3UFyO"
      },
      "outputs": [],
      "source": [
        "def lower_sentence(sentence: str) -> str:\n",
        "    '''\n",
        "    Lowercase the sentence.\n",
        "    :param data: The sentence to lowercase.\n",
        "    :return: The lowercased sentence\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return sentence.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "an4hh-RZUGzM"
      },
      "outputs": [],
      "source": [
        "def remove_emails(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove emails from the sentence.\n",
        "    :param sentence: The sentence to remove emails from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without emails.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(r\"\\S*@\\S*\\s?\", \"\", sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dkf1qsFKUHz5"
      },
      "outputs": [],
      "source": [
        "def remove_nonascii_diacritic(sentence: str) -> str:\n",
        "    '''\n",
        "\n",
        "    Remove diacritics from the sentence.\n",
        "\n",
        "    :param sentence: The sentence to remove diacritics from.\n",
        "\n",
        "    :type sentence: str\n",
        "\n",
        "    :return: The sentence without diacritics.\n",
        "\n",
        "    :rtype: str\n",
        "    '''\n",
        "\n",
        "    return unicodedata.normalize(\"NFKD\", sentence).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vZc2W843UI_Z"
      },
      "outputs": [],
      "source": [
        "def clean_html(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove HTML tags from the sentence.\n",
        "    :param sentence: The sentence to remove HTML tags from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without HTML tags.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return BeautifulSoup(sentence, \"html.parser\").get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nlP0cghAUKOR"
      },
      "outputs": [],
      "source": [
        "def replace_repeated_chars(sentence: str) -> str:\n",
        "    '''\n",
        "    Replace repeated characters in the sentence.\n",
        "    :param sentence: The sentence to replace repeated characters in.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with replaced repeated characters.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Replace consecutive occurrences of ',', '!', '.', and '?' with a single occurrence\n",
        "    return re.sub(r'([,!?.])\\1+', r'\\1', sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3DdnIds0UL9_"
      },
      "outputs": [],
      "source": [
        "def translate_emojis_to_text(sentence: str) -> str:\n",
        "    '''\n",
        "    Translate emojis in the sentence to text.\n",
        "    :param sentence: The sentence to translate emojis to text.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with translated emojis to text.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Translate emojis to text codes\n",
        "    translated_text = emoji.demojize(sentence)\n",
        "    # Remove colons from the translated text\n",
        "    translated_text = re.sub(r':', '', translated_text)\n",
        "    return translated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dT4WWVqlUMcE"
      },
      "outputs": [],
      "source": [
        "def expand_sentence(sentence: str) -> str:\n",
        "    '''\n",
        "    Expand the contractions in the sentence.\n",
        "    :param sentence: The sentence to expand contractions in.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with expanded contractions.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return contractions.fix(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XBCz9nCJUNks"
      },
      "outputs": [],
      "source": [
        "def remove_url(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove URLs from the sentence.\n",
        "    :param sentence: The sentence to remove URLs from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without URLs.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\", '', sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cBh1BqrlUO0_"
      },
      "outputs": [],
      "source": [
        "def remove_possessives(sentence: str) -> str:\n",
        "    '''\n",
        "    Strip possessives from the sentence.\n",
        "    :param sentence: The sentence to strip possessives from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without possessives.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Stripping the possessives\n",
        "    sentence = sentence.replace(\"'s\", '')\n",
        "    sentence = sentence.replace('’s', '')\n",
        "    sentence = sentence.replace('s’', 's')\n",
        "    sentence = sentence.replace(\"s'\", 's')\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mwMEFZ8FUQ4M"
      },
      "outputs": [],
      "source": [
        "def remove_extra_space(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove extra spaces from the sentence.\n",
        "    :param sentence: The sentence to remove extra spaces from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without extra spaces.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(r'\\s+', ' ', sentence).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qmDKtHcCUR-k"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence: str) -> list[str]:\n",
        "    '''\n",
        "    Tokenize the sentence.\n",
        "    :param sentence: The sentence to tokenize.\n",
        "    :type sentence: str\n",
        "    :return: The tokenized sentence.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOlbL9OUUC7",
        "outputId": "5ed5be8e-e93a-4505-9d5b-da35be13d77b"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stop_words(sentence: list[str]) -> list[str]:\n",
        "    '''\n",
        "    Remove stop words from the sentence.\n",
        "    :param sentence: The sentence to remove stop words from.\n",
        "    :type sentence: list[str]\n",
        "    :return: The sentence without stop words.\n",
        "    :rtype: list[str]\n",
        "    '''\n",
        "    return [word for word in sentence if word not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IFm0wABKUVP5"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemm_sentence(sentence: list[str]) -> list[str]:\n",
        "    '''\n",
        "    Lemmatize the sentence.\n",
        "    :param sentence: The sentence to lemmatize.\n",
        "    :type sentence: list[str]\n",
        "    :return: The lemmatized sentence.\n",
        "    :rtype: list[str]\n",
        "    '''\n",
        "    # Perform POS tagging\n",
        "    pos_tags = pos_tag(sentence)\n",
        "    # Lemmatize each word based on its POS tag\n",
        "    lemmatized_words = []\n",
        "    for word, pos in pos_tags:\n",
        "        # Map Penn Treebank POS tags to WordNet POS tags\n",
        "        if pos.startswith('N'):  # Nouns\n",
        "            pos = 'n'\n",
        "        elif pos.startswith('V'):  # Verbs\n",
        "            pos = 'v'\n",
        "        elif pos.startswith('J'):  # Adjectives\n",
        "            pos = 'a'\n",
        "        elif pos.startswith('R'):  # Adverbs\n",
        "            pos = 'r'\n",
        "        else:\n",
        "            pos = 'n'  # Default to noun if POS tag not found\n",
        "\n",
        "        # Lemmatize the word using the appropriate POS tag\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "        lemmatized_words.append(lemma)\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tEDyO6eOUXEY"
      },
      "outputs": [],
      "source": [
        "def clean_train(line: str) -> list[str]:\n",
        "    '''\n",
        "    Clean the line and return it as a list of tokens\n",
        "    :param line: the line to clean\n",
        "    :type line: str\n",
        "    :return: the cleaned line as a list of tokens\n",
        "    :rtype: list\n",
        "    '''\n",
        "    # translate emojis\n",
        "    line = translate_emojis_to_text(line)\n",
        "    # lower the line\n",
        "    line = lower_sentence(line)\n",
        "    # remove non ascii\n",
        "    line = remove_nonascii_diacritic(line)\n",
        "    # remove emails\n",
        "    line = remove_emails(line)\n",
        "    # remove html\n",
        "    line = clean_html(line)\n",
        "    # remove urls\n",
        "    line = remove_url(line)\n",
        "    # replace repeated chars\n",
        "    line = replace_repeated_chars(line)\n",
        "    # expand\n",
        "    line = expand_sentence(line)\n",
        "    # remove possessives\n",
        "    line = remove_possessives(line)\n",
        "    # remove extra spaces\n",
        "    line = remove_extra_space(line)\n",
        "    # tekonize\n",
        "    line = tokenize_sentence(line)\n",
        "    # remove stopwords\n",
        "    line = remove_stop_words(line)\n",
        "    # lemmetization\n",
        "    line = lemm_sentence(line)\n",
        "    if len(line) == 0:\n",
        "        return ['Normal']\n",
        "    return line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IH7ScDyOUYV5"
      },
      "outputs": [],
      "source": [
        "# define the mapping from 's' to 1 and 'u' to 0\n",
        "forward_label_mapping = {'suicide': 1, 'non-suicide': 0}\n",
        "\n",
        "# define the reverse mapping from 0 to 'u' and 1 to 's'\n",
        "reverse_label_mapping = {0: 'non-suicide', 1: 'suicide'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUP-1lRdUZJ_",
        "outputId": "2fbfe479-4822-4573-a64e-bc7f4aa50140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Unique Values:\n",
            "Unnamed: 0    232074\n",
            "text          232074\n",
            "class              2\n",
            "dtype: int64\n",
            "unique target classes ['suicide' 'non-suicide']\n",
            "Number of Dialogues:  232074\n",
            "First 5 elements: \n",
            "   Unnamed: 0                                               text        class\n",
            "0           2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
            "1           3  Am I weird I don't get affected by compliments...  non-suicide\n",
            "2           4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
            "3           8          i need helpjust help me im crying so hard      suicide\n",
            "4           9  I’m so lostHello, my name is Adam (16) and I’v...      suicide\n"
          ]
        }
      ],
      "source": [
        "# Read CSV file into DataFrame\n",
        "df = pd.read_csv(dataset_path)\n",
        "# Count unique values in each column\n",
        "unique_counts = df.nunique()\n",
        "# Display the count of unique values\n",
        "print('Number of Unique Values:')\n",
        "print(unique_counts)\n",
        "# print the unique values of emotion\n",
        "print('unique target classes', df['class'].unique())\n",
        "# get the number of dialogue\n",
        "num_Dialogues = len(df['class'])\n",
        "# print the number of dialogues\n",
        "print('Number of Dialogues: ', num_Dialogues)\n",
        "print('First 5 elements: ')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NTiAvgwnUaSY"
      },
      "outputs": [],
      "source": [
        "def read_dataset_suicidal_detection(data_path: str, split_seed: int) -> Tuple[Tuple[List[str], List[int]], Tuple[List[str], List[int]]]:\n",
        "    '''\n",
        "    Read the dataset for the suicidal detection task.\n",
        "    :param data_path: The path to the dataset.\n",
        "    :type data_path: str\n",
        "    :param split_seed: The seed to use for splitting the dataset.\n",
        "    :type split_seed: int\n",
        "    :return: The training, and test sets.\n",
        "    :rtype: Tuple[Tuple[List[str], List[int]], Tuple[List[str], List[int]]\n",
        "    '''\n",
        "    data = pd.read_csv(data_path)\n",
        "    # extract the dialogues and their corresponding labels\n",
        "    dialogues = data['text'].tolist()\n",
        "    labels = data['class'].apply(lambda x: forward_label_mapping[x]).tolist()\n",
        "\n",
        "    # split the data into training and temporary sets (70% training, 30% temporary)\n",
        "    dialogues_train, dialogues_test, labels_train, labels_test = train_test_split(\n",
        "        dialogues, labels, test_size=0.3, random_state=split_seed)\n",
        "\n",
        "\n",
        "    return (dialogues_train, labels_train),  (dialogues_test, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQhKxo6RUbL3",
        "outputId": "6a6cb1c6-c60f-4142-dcf8-3563112a9964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of dialogues in training set: 162451 (0.70%)\n",
            "Number of dialogues in test set: 69623 (0.30%)\n"
          ]
        }
      ],
      "source": [
        "split_seed = 10\n",
        "# read the data set\n",
        "(dialogues_train, labels_train),  (dialogues_test,labels_test) = read_dataset_suicidal_detection(dataset_path, split_seed)\n",
        "# print the number of dialogues in each set and percentage of total data\n",
        "print(\n",
        "    f\"Number of dialogues in training set: {len(dialogues_train)} ({len(dialogues_train) / num_Dialogues:.2f}%)\")\n",
        "print(\n",
        "    f\"Number of dialogues in test set: {len(dialogues_test)} ({len(dialogues_test) / num_Dialogues:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "reverse_label_mapping = {0: 'non-suicide', 1: 'suicide'}\n",
        "\n",
        "def save_data_to_csv(data: Tuple[List[str], List[int]], file_path: str):\n",
        "        '''\n",
        "        Save the data to a CSV file.\n",
        "        :param data: The data to save.\n",
        "        :type data: Tuple[List[str], List[int]]\n",
        "        :param file_path: The path to the CSV file.\n",
        "        :type file_path: str\n",
        "        '''\n",
        "        df = pd.DataFrame({'text': data[0], 'class': data[1]})\n",
        "        df['class'] = df['class'].apply(lambda x: reverse_label_mapping[x])\n",
        "        df.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XXWsLe2AUchx"
      },
      "outputs": [],
      "source": [
        "def clean_data(data: List[str]) -> List[List[str]]:\n",
        "    '''\n",
        "    Clean the data.\n",
        "    :param data: The data to clean.\n",
        "    :type data: List[str]\n",
        "    :return: The cleaned data.\n",
        "    :rtype: List[List[str]]\n",
        "    '''\n",
        "    cleaned_data = []\n",
        "    for line in tqdm(data):\n",
        "        cleaned_line = clean_train(line)\n",
        "        cleaned_data.append(cleaned_line)\n",
        "    return cleaned_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNJ-kWu9UdjS",
        "outputId": "b7e7991d-112f-4876-b555-f2ed5e81f0a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/162451 [00:01<56:16:18,  1.25s/it]D:\\temp\\ipykernel_12824\\964458276.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n",
            "100%|██████████| 162451/162451 [13:08<00:00, 205.94it/s] \n",
            "100%|██████████| 69623/69623 [06:26<00:00, 180.34it/s] \n"
          ]
        }
      ],
      "source": [
        "dialogues_train_proccessed = clean_data(dialogues_train)\n",
        "dialogues_test_proccessed = clean_data(dialogues_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7p3VAzPUeji",
        "outputId": "d6f737ef-8cf4-44cf-8ea8-35eb10c53267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['stupid', 'fuck', 'dumb', 'dude', '.', 'literally', 'thought', 'la', 'vega', 'new', 'vega', '7', 'year', '.', 'go', 'die']\n",
            "['blame', 'familymy', 'family', 'lose', 'sister', 'law', 'suicide', '.', 'brother', 'best', 'friend', 'since', 'young', '(', 'know', '10+', 'year', ')', 'grow', 'old', 'become', 'couple', 'recently', 'child', 'turn', '1.', 'inseparable', '.', 'live', 'together', 'come', 'live', 'awhile', '.', 'become', 'close', 'used', 'see', 'together', 'everyday', '.', 'someone', 'always', 'smile', ',', 'love', 'son', 'brother', 'much', '.', 'recently', 'move', 'guess', 'something', 'happen', 'longer', 'together', '.', 'brother', 'expose', 'someone', 'covid-19', 'quarantine', '2', 'week', '.', 'go', 'house', 'want', 'get', 'son', 'sick', '.', 'make', 'post', 'know', 'father', 'choose', 'see', 'kid', '.', 'go', 'downhill', '.', 'know', 'together', 'post', 'would', 'share', 'fb', '.', 'sure', 'reason', ',', 'still', '.', 'however', ',', 'within', 'last', '2-3', 'day', 'suicide', 'act', 'weird', ',', 'come', '12am', 'hold', 'son', 'could', 'barely', 'stand', '.', 'next', 'day', 'mom', 'take', 'son', 'guessing', 'notice', 'something', 'right', '.', 'come', 'back', 'house', 'crash', 'car', 'say', 'due', 'someone', 'chasing', '.', 'say', 'fine', 'bumper', '.', 'leave', 'come', 'back', 'someone', '(', 'sure', 'yet', ')', '?', 'bring', 'steal', 'brother', 'car', 'night', 'crash', 'front', 'light', 'broken', '.', 'say', 'steal', 'car', 'ask', 'show', 'car', '.', 'say', 'remember', '.', 'mom', 'sit', 'ask', 'wrong', ',', 'could', 'tell', 'u', 'anything', '.', 'tell', 'u', 'fine', '.', 'make', 'stay', 'night', 'get', 'sleep', 'morning', 'put', 'sugar', 'car', 'tank', 'part', 'car', 'house', ',', 'beat', 'car', 'stick', ',', 'try', 'climb', 'window', 'brother', '.', 'start', 'hit', ',', 'break', 'necklace', ',', 'grab', 'thing', 'start', 'destroy', 'argue', 'get', 'physical', '.', 'broke', '.', 'go', 'grabbed', 'mom', 'come', 'make', 'sure', 'everything', 'ok', 'say', 'continue', 'happen', '.', 'brother', 'call', 'sister', 'tell', 'si', 'law', ',', 'say', 'fault', 'hung', '.', 'call', 'dad', 'tell', 'go', 'look', 'find', 'side', 'street', 'leave', 'car', 'park', 'windows', 'key', 'street', '.', 'know', '.', 'bring', 'mom', 'think', 'would', 'safe', ',', 'grabbed', 'son', 'fell', '.', 'parent', 'sibling', 'leave', '.', 'find', 'go', 'walk', 'commit', 'suicide', 'jumping', 'bridge', '.', 'brother', 'sister', 'follow', 'car', 'jump', 'bridge', '.', 'blame', 'brother', 'say', 'go', 'funeral', 'son', 'back', 'never', 'want', 'see', 'u', '.', 'also', 'tell', 'people', 'brother', 'fault', '.', 'know', ',', 'say', '.', 'miss', '.', 'heartbroken', '.', 'love', '.', 'life', 'never', '.', 'play', 'last', 'moment', 'head', 'think', 'do', '.', 'never', 'expect', 'ever', 'happen', '.', 'look', 'advise', 'opinion', '.', 'literally', 'feel', 'pain', 'chest', 'crying_face']\n"
          ]
        }
      ],
      "source": [
        "print(dialogues_train_proccessed[0])\n",
        "print(dialogues_train_proccessed[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xjyifhCWUfkN"
      },
      "outputs": [],
      "source": [
        "flattened_X_train = [' '.join(tokens) for tokens in dialogues_train_proccessed]\n",
        "flattened_X_test = [' '.join(tokens) for tokens in dialogues_test_proccessed]\n",
        "\n",
        "save_data_to_csv((flattened_X_train, labels_train), '../data/processed/suicidal_detection_train.csv')\n",
        "save_data_to_csv((flattened_X_test, labels_test), '../data/processed/suicidal_detection_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMLDymw3UgLh",
        "outputId": "0e574c0a-442f-48c6-85a0-cec79b025d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stupid fuck dumb dude . literally thought la vega new vega 7 year . go die\n",
            "blame familymy family lose sister law suicide . brother best friend since young ( know 10+ year ) grow old become couple recently child turn 1. inseparable . live together come live awhile . become close used see together everyday . someone always smile , love son brother much . recently move guess something happen longer together . brother expose someone covid-19 quarantine 2 week . go house want get son sick . make post know father choose see kid . go downhill . know together post would share fb . sure reason , still . however , within last 2-3 day suicide act weird , come 12am hold son could barely stand . next day mom take son guessing notice something right . come back house crash car say due someone chasing . say fine bumper . leave come back someone ( sure yet ) ? bring steal brother car night crash front light broken . say steal car ask show car . say remember . mom sit ask wrong , could tell u anything . tell u fine . make stay night get sleep morning put sugar car tank part car house , beat car stick , try climb window brother . start hit , break necklace , grab thing start destroy argue get physical . broke . go grabbed mom come make sure everything ok say continue happen . brother call sister tell si law , say fault hung . call dad tell go look find side street leave car park windows key street . know . bring mom think would safe , grabbed son fell . parent sibling leave . find go walk commit suicide jumping bridge . brother sister follow car jump bridge . blame brother say go funeral son back never want see u . also tell people brother fault . know , say . miss . heartbroken . love . life never . play last moment head think do . never expect ever happen . look advise opinion . literally feel pain chest crying_face\n"
          ]
        }
      ],
      "source": [
        "print(flattened_X_train[0])\n",
        "print(flattened_X_train[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvMX5RoUjy3",
        "outputId": "804ad3ee-9344-4fea-8ca1-965f372a8a35"
      },
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# # Compute class weights\n",
        "\n",
        "# pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])\n",
        "# pipe_lr.fit(flattened_X_train,labels_train)\n",
        "\n",
        "# y_pred = pipe_lr.predict(flattened_X_test)\n",
        "# report = classification_report(labels_test, y_pred)\n",
        "# print(report)\n",
        "X_train = flattened_X_train\n",
        "X_test = flattened_X_test\n",
        "\n",
        "y_train = labels_train\n",
        "y_test = labels_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rChOekRUl6V",
        "outputId": "4426e7fe-4dd8-4b00-cc42-8851ffb8e7c5"
      },
      "outputs": [],
      "source": [
        "# # Example of a new sentence\n",
        "# new_sentence = \"suicide\"\n",
        "\n",
        "# preprocessed_tokens = clean_train(new_sentence)\n",
        "# preprocessed_text = ' '.join(preprocessed_tokens)\n",
        "\n",
        "# predicted_label = pipe_lr.predict([preprocessed_text])\n",
        "\n",
        "# print(\"Predicted label:\", reverse_label_mapping[predicted_label[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: Count Vectorizer, Classifier: Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.82      0.89     34954\n",
            "           1       0.84      0.97      0.90     34669\n",
            "\n",
            "    accuracy                           0.89     69623\n",
            "   macro avg       0.90      0.90      0.89     69623\n",
            "weighted avg       0.90      0.89      0.89     69623\n",
            "\n",
            "\n",
            "Feature: Count Vectorizer, Classifier: SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89     34954\n",
            "           1       0.92      0.84      0.88     34669\n",
            "\n",
            "    accuracy                           0.88     69623\n",
            "   macro avg       0.89      0.88      0.88     69623\n",
            "weighted avg       0.89      0.88      0.88     69623\n",
            "\n",
            "\n",
            "Feature: Count Vectorizer, Classifier: KNN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83     34954\n",
            "           1       0.90      0.72      0.80     34669\n",
            "\n",
            "    accuracy                           0.82     69623\n",
            "   macro avg       0.83      0.82      0.82     69623\n",
            "weighted avg       0.83      0.82      0.82     69623\n",
            "\n",
            "\n",
            "Feature: Count Vectorizer, Classifier: Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89     34954\n",
            "           1       0.88      0.92      0.90     34669\n",
            "\n",
            "    accuracy                           0.90     69623\n",
            "   macro avg       0.90      0.90      0.90     69623\n",
            "weighted avg       0.90      0.90      0.90     69623\n",
            "\n",
            "\n",
            "Feature: Count Vectorizer, Classifier: XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91     34954\n",
            "           1       0.93      0.88      0.91     34669\n",
            "\n",
            "    accuracy                           0.91     69623\n",
            "   macro avg       0.91      0.91      0.91     69623\n",
            "weighted avg       0.91      0.91      0.91     69623\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: Count Vectorizer, Classifier: Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93     34954\n",
            "           1       0.95      0.91      0.93     34669\n",
            "\n",
            "    accuracy                           0.93     69623\n",
            "   macro avg       0.93      0.93      0.93     69623\n",
            "weighted avg       0.93      0.93      0.93     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.79      0.87     34954\n",
            "           1       0.82      0.98      0.89     34669\n",
            "\n",
            "    accuracy                           0.88     69623\n",
            "   macro avg       0.89      0.88      0.88     69623\n",
            "weighted avg       0.89      0.88      0.88     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92     34954\n",
            "           1       0.93      0.91      0.92     34669\n",
            "\n",
            "    accuracy                           0.92     69623\n",
            "   macro avg       0.92      0.92      0.92     69623\n",
            "weighted avg       0.92      0.92      0.92     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: KNN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.94      0.84     34954\n",
            "           1       0.92      0.71      0.80     34669\n",
            "\n",
            "    accuracy                           0.82     69623\n",
            "   macro avg       0.84      0.82      0.82     69623\n",
            "weighted avg       0.84      0.82      0.82     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.91     34954\n",
            "           1       0.90      0.91      0.91     34669\n",
            "\n",
            "    accuracy                           0.91     69623\n",
            "   macro avg       0.91      0.91      0.91     69623\n",
            "weighted avg       0.91      0.91      0.91     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92     34954\n",
            "           1       0.93      0.89      0.91     34669\n",
            "\n",
            "    accuracy                           0.91     69623\n",
            "   macro avg       0.91      0.91      0.91     69623\n",
            "weighted avg       0.91      0.91      0.91     69623\n",
            "\n",
            "\n",
            "Feature: tf-idf, Classifier: Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93     34954\n",
            "           1       0.94      0.93      0.93     34669\n",
            "\n",
            "    accuracy                           0.93     69623\n",
            "   macro avg       0.93      0.93      0.93     69623\n",
            "weighted avg       0.93      0.93      0.93     69623\n",
            "\n",
            "\n",
            "Best combination: Feature: tf-idf, Classifier: Logistic Regression, Precision: 0.9341357011434501, Recall: 0.9340304209815722\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': xgb.XGBClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}\n",
        "\n",
        "# Define vectorizers\n",
        "vectorizers = {\n",
        "    # 'Bag of Words': CountVectorizer(),\n",
        "    # 'Unigram': CountVectorizer(ngram_range=(1, 1)),\n",
        "    # 'N-Gram': CountVectorizer(ngram_range=(1, 2)),\n",
        "    # 'CharLevel': CountVectorizer(analyzer='char'),\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'tf-idf': TfidfVectorizer()\n",
        "}\n",
        "\n",
        "# Prepare results storage\n",
        "results = []\n",
        "\n",
        "# Evaluate all combinations\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        steps = [('vectorizer', vectorizer)]\n",
        "        \n",
        "        if clf_name in ['SVM', 'KNN']:\n",
        "            steps.append(('svd', TruncatedSVD(n_components=100)))\n",
        "        \n",
        "        steps.append(('classifier', clf))\n",
        "        \n",
        "        # Create a pipeline\n",
        "        pipe = Pipeline(steps)\n",
        "        \n",
        "        # Fit the model\n",
        "        pipe.fit(X_train, y_train_enc)\n",
        "\n",
        "        # Predict\n",
        "        y_pred = pipe.predict(X_test)\n",
        "\n",
        "        # Evaluate\n",
        "        precision, recall, _, _ = precision_recall_fscore_support(y_test_enc, y_pred, average='weighted')\n",
        "        \n",
        "        # Store results\n",
        "        results.append((vec_name, clf_name, precision, recall))\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"Feature: {vec_name}, Classifier: {clf_name}\")\n",
        "        print(classification_report(y_test_enc, y_pred))\n",
        "        print()\n",
        "\n",
        "# Find the best combination\n",
        "best_combination = max(results, key=lambda x: (x[2], x[3]))  # Based on precision and recall\n",
        "print(f\"Best combination: Feature: {best_combination[0]}, Classifier: {best_combination[1]}, Precision: {best_combination[2]}, Recall: {best_combination[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: Bag of Words, Classifier: Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.82      0.89     34954\n",
            "           1       0.84      0.97      0.90     34669\n",
            "\n",
            "    accuracy                           0.89     69623\n",
            "   macro avg       0.90      0.90      0.89     69623\n",
            "weighted avg       0.90      0.89      0.89     69623\n",
            "\n",
            "\n",
            "Feature: Bag of Words, Classifier: SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89     34954\n",
            "           1       0.92      0.84      0.88     34669\n",
            "\n",
            "    accuracy                           0.88     69623\n",
            "   macro avg       0.89      0.88      0.88     69623\n",
            "weighted avg       0.89      0.88      0.88     69623\n",
            "\n",
            "\n",
            "Feature: Bag of Words, Classifier: KNN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83     34954\n",
            "           1       0.90      0.71      0.79     34669\n",
            "\n",
            "    accuracy                           0.82     69623\n",
            "   macro avg       0.83      0.82      0.81     69623\n",
            "weighted avg       0.83      0.82      0.81     69623\n",
            "\n",
            "\n",
            "Feature: Bag of Words, Classifier: Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89     34954\n",
            "           1       0.88      0.92      0.90     34669\n",
            "\n",
            "    accuracy                           0.90     69623\n",
            "   macro avg       0.90      0.90      0.90     69623\n",
            "weighted avg       0.90      0.90      0.90     69623\n",
            "\n",
            "\n",
            "Feature: Bag of Words, Classifier: XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91     34954\n",
            "           1       0.93      0.88      0.91     34669\n",
            "\n",
            "    accuracy                           0.91     69623\n",
            "   macro avg       0.91      0.91      0.91     69623\n",
            "weighted avg       0.91      0.91      0.91     69623\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: Bag of Words, Classifier: Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93     34954\n",
            "           1       0.95      0.91      0.93     34669\n",
            "\n",
            "    accuracy                           0.93     69623\n",
            "   macro avg       0.93      0.93      0.93     69623\n",
            "weighted avg       0.93      0.93      0.93     69623\n",
            "\n",
            "\n",
            "Feature: Unigram, Classifier: Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.82      0.89     34954\n",
            "           1       0.84      0.97      0.90     34669\n",
            "\n",
            "    accuracy                           0.89     69623\n",
            "   macro avg       0.90      0.90      0.89     69623\n",
            "weighted avg       0.90      0.89      0.89     69623\n",
            "\n",
            "\n",
            "Feature: Unigram, Classifier: SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89     34954\n",
            "           1       0.92      0.84      0.88     34669\n",
            "\n",
            "    accuracy                           0.88     69623\n",
            "   macro avg       0.89      0.88      0.88     69623\n",
            "weighted avg       0.89      0.88      0.88     69623\n",
            "\n",
            "\n",
            "Feature: Unigram, Classifier: KNN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83     34954\n",
            "           1       0.90      0.71      0.79     34669\n",
            "\n",
            "    accuracy                           0.82     69623\n",
            "   macro avg       0.83      0.82      0.81     69623\n",
            "weighted avg       0.83      0.82      0.81     69623\n",
            "\n",
            "\n",
            "Feature: Unigram, Classifier: Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.90     34954\n",
            "           1       0.88      0.91      0.90     34669\n",
            "\n",
            "    accuracy                           0.90     69623\n",
            "   macro avg       0.90      0.90      0.90     69623\n",
            "weighted avg       0.90      0.90      0.90     69623\n",
            "\n",
            "\n",
            "Feature: Unigram, Classifier: XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91     34954\n",
            "           1       0.93      0.88      0.91     34669\n",
            "\n",
            "    accuracy                           0.91     69623\n",
            "   macro avg       0.91      0.91      0.91     69623\n",
            "weighted avg       0.91      0.91      0.91     69623\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: Unigram, Classifier: Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93     34954\n",
            "           1       0.95      0.91      0.93     34669\n",
            "\n",
            "    accuracy                           0.93     69623\n",
            "   macro avg       0.93      0.93      0.93     69623\n",
            "weighted avg       0.93      0.93      0.93     69623\n",
            "\n",
            "\n",
            "Feature: CharLevel, Classifier: Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.38      0.52     34954\n",
            "           1       0.60      0.93      0.73     34669\n",
            "\n",
            "    accuracy                           0.65     69623\n",
            "   macro avg       0.72      0.66      0.63     69623\n",
            "weighted avg       0.72      0.65      0.63     69623\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_components(100) must be <= n_features(89).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:242\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m     U, Sigma, VT \u001b[38;5;241m=\u001b[39m randomized_svd(\n\u001b[0;32m    247\u001b[0m         X,\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m VT\n",
            "\u001b[1;31mValueError\u001b[0m: n_components(100) must be <= n_features(89)."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': xgb.XGBClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}\n",
        "\n",
        "# Define vectorizers\n",
        "vectorizers = {\n",
        "    'Bag of Words': CountVectorizer(),\n",
        "    'Unigram': CountVectorizer(ngram_range=(1, 1)),\n",
        "    # 'N-Gram': CountVectorizer(ngram_range=(1, 2)),\n",
        "    'CharLevel': CountVectorizer(analyzer='char'),\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'tf-idf': TfidfVectorizer()\n",
        "}\n",
        "\n",
        "# Prepare results storage\n",
        "results = []\n",
        "\n",
        "# Evaluate all combinations\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        steps = [('vectorizer', vectorizer)]\n",
        "        \n",
        "        if clf_name in ['SVM', 'KNN']:\n",
        "            steps.append(('svd', TruncatedSVD(n_components=100)))\n",
        "        \n",
        "        steps.append(('classifier', clf))\n",
        "        \n",
        "        # Create a pipeline\n",
        "        pipe = Pipeline(steps)\n",
        "        \n",
        "        # Fit the model\n",
        "        pipe.fit(X_train, y_train_enc)\n",
        "\n",
        "        # Predict\n",
        "        y_pred = pipe.predict(X_test)\n",
        "\n",
        "        # Evaluate\n",
        "        precision, recall, _, _ = precision_recall_fscore_support(y_test_enc, y_pred, average='weighted')\n",
        "        \n",
        "        # Store results\n",
        "        results.append((vec_name, clf_name, precision, recall))\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"Feature: {vec_name}, Classifier: {clf_name}\")\n",
        "        print(classification_report(y_test_enc, y_pred))\n",
        "        print()\n",
        "\n",
        "# Find the best combination\n",
        "best_combination = max(results, key=lambda x: (x[2], x[3]))  # Based on precision and recall\n",
        "print(f\"Best combination: Feature: {best_combination[0]}, Classifier: {best_combination[1]}, Precision: {best_combination[2]}, Recall: {best_combination[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoFLFbkYrOv4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
