{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install contractions emoji pyspellchecker"
      ],
      "metadata": {
        "id": "XbtWYZF7ChnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ee2vJHgu1r4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.data import find\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import contractions\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "import emoji\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import unittest\n",
        "\n",
        "class Preprocessor:\n",
        "\n",
        "    resources = [\n",
        "        'tokenizers/punkt',\n",
        "        'corpora/stopwords',\n",
        "        'corpora/wordnet',\n",
        "        'taggers/averaged_perceptron_tagger'\n",
        "    ]\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        for resource in self.resources:\n",
        "            try:\n",
        "                find(resource)\n",
        "                print(f\"{resource} is already downloaded.\")\n",
        "            except LookupError:\n",
        "                print(f\"{resource} not found. Downloading...\")\n",
        "                download(resource.split('/')[1])\n",
        "\n",
        "        # Stopword removal\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        # Initialize the WordNet lemmatizer\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def lower_sentence(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Lowercase the sentence.\n",
        "        :param data: The sentence to lowercase.\n",
        "        :return: The lowercased sentence\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return sentence.lower()\n",
        "\n",
        "    def remove_emails(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Remove emails from the sentence.\n",
        "        :param sentence: The sentence to remove emails from.\n",
        "        :type sentence: str\n",
        "        :return: The sentence without emails.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return re.sub(r\"\\S*@\\S*\\s?\", \"\", sentence)\n",
        "\n",
        "    def remove_nonascii_diacritic(self, sentence: str) -> str:\n",
        "        '''\n",
        "\n",
        "        Remove diacritics from the sentence.\n",
        "\n",
        "        :param sentence: The sentence to remove diacritics from.\n",
        "\n",
        "        :type sentence: str\n",
        "\n",
        "        :return: The sentence without diacritics.\n",
        "\n",
        "        :rtype: str\n",
        "        '''\n",
        "\n",
        "        return unicodedata.normalize(\"NFKD\", sentence).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
        "\n",
        "    def clean_html(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Remove HTML tags from the sentence.\n",
        "        :param sentence: The sentence to remove HTML tags from.\n",
        "        :type sentence: str\n",
        "        :return: The sentence without HTML tags.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return BeautifulSoup(sentence, \"html.parser\").get_text()\n",
        "\n",
        "    def replace_repeated_chars(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Replace repeated characters in the sentence.\n",
        "        :param sentence: The sentence to replace repeated characters in.\n",
        "        :type sentence: str\n",
        "        :return: The sentence with replaced repeated characters.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        # Replace consecutive occurrences of ',', '!', '.', and '?' with a single occurrence\n",
        "        return re.sub(r'([,!?.])\\1+', r'\\1', sentence)\n",
        "\n",
        "    def translate_emojis_to_text(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Translate emojis in the sentence to text.\n",
        "        :param sentence: The sentence to translate emojis to text.\n",
        "        :type sentence: str\n",
        "        :return: The sentence with translated emojis to text.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        line = ''\n",
        "        for char in sentence:\n",
        "            if emoji.is_emoji(char):\n",
        "                emoji_text = emoji.demojize(char)[1:-1].replace('_', ' ')\n",
        "                line += emoji_text\n",
        "            else:\n",
        "                line += char\n",
        "\n",
        "        return line\n",
        "\n",
        "    def expand_sentence(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Expand the contractions in the sentence.\n",
        "        :param sentence: The sentence to expand contractions in.\n",
        "        :type sentence: str\n",
        "        :return: The sentence with expanded contractions.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return contractions.fix(sentence)\n",
        "\n",
        "    def remove_url(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Remove URLs from the sentence.\n",
        "        :param sentence: The sentence to remove URLs from.\n",
        "        :type sentence: str\n",
        "        :return: The sentence without URLs.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\", '', sentence)\n",
        "\n",
        "    def remove_possessives(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Strip possessives from the sentence.\n",
        "        :param sentence: The sentence to strip possessives from.\n",
        "        :type sentence: str\n",
        "        :return: The sentence without possessives.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        # Stripping the possessives\n",
        "        sentence = sentence.replace(\"'s\", '')\n",
        "        sentence = sentence.replace('’s', '')\n",
        "        sentence = sentence.replace('s’', 's')\n",
        "        sentence = sentence.replace(\"s'\", 's')\n",
        "        return sentence\n",
        "\n",
        "    def remove_extra_space(self, sentence: str) -> str:\n",
        "        '''\n",
        "        Remove extra spaces from the sentence.\n",
        "        :param sentence: The sentence to remove extra spaces from.\n",
        "        :type sentence: str\n",
        "        :return: The sentence without extra spaces.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return re.sub(r'\\s+', ' ', sentence).strip()\n",
        "\n",
        "\n",
        "    def check_sentence_spelling(self, sentence: list[str]) -> list[str]:\n",
        "        '''\n",
        "        Check the spelling of the words in the sentence.\n",
        "        :param sentence: The sentence to check the spelling of.\n",
        "        :type sentence: list\n",
        "        :return: The sentence with corrected spelling.\n",
        "        :rtype: list\n",
        "        '''\n",
        "        spell = SpellChecker()\n",
        "        corrected_sentence = []\n",
        "        for word in sentence:\n",
        "            if word != '':\n",
        "                correction = spell.correction(word)\n",
        "                if correction is not None:\n",
        "                    corrected_sentence.append(correction)\n",
        "                else:\n",
        "                    corrected_sentence.append(word)\n",
        "            else:\n",
        "                corrected_sentence.append('')\n",
        "        return corrected_sentence\n",
        "\n",
        "    def tokenize_sentence(self, sentence: str) -> list[str]:\n",
        "        '''\n",
        "        Tokenize the sentence.\n",
        "        :param sentence: The sentence to tokenize.\n",
        "        :type sentence: str\n",
        "        :return: The tokenized sentence.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return nltk.word_tokenize(sentence)\n",
        "\n",
        "\n",
        "    def remove_stop_words(self, sentence: list[str]) -> list[str]:\n",
        "        '''\n",
        "        Remove stop words from the sentence.\n",
        "        :param sentence: The sentence to remove stop words from.\n",
        "        :type sentence: list[str]\n",
        "        :return: The sentence without stop words.\n",
        "        :rtype: list[str]\n",
        "        '''\n",
        "        return [word for word in sentence if word not in self.stop_words]\n",
        "\n",
        "    def lemm_sentence(self, sentence: list[str]) -> list[str]:\n",
        "        '''\n",
        "        Lemmatize the sentence.\n",
        "        :param sentence: The sentence to lemmatize.\n",
        "        :type sentence: list[str]\n",
        "        :return: The lemmatized sentence.\n",
        "        :rtype: list[str]\n",
        "        '''\n",
        "        # Perform POS tagging\n",
        "        pos_tags = pos_tag(sentence)\n",
        "        # Lemmatize each word based on its POS tag\n",
        "        lemmatized_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            # Map Penn Treebank POS tags to WordNet POS tags\n",
        "            if pos.startswith('N'):  # Nouns\n",
        "                pos = 'n'\n",
        "            elif pos.startswith('V'):  # Verbs\n",
        "                pos = 'v'\n",
        "            elif pos.startswith('J'):  # Adjectives\n",
        "                pos = 'a'\n",
        "            elif pos.startswith('R'):  # Adverbs\n",
        "                pos = 'r'\n",
        "            else:\n",
        "                pos = 'n'  # Default to noun if POS tag not found\n",
        "\n",
        "            # Lemmatize the word using the appropriate POS tag\n",
        "            lemma = self.lemmatizer.lemmatize(word, pos=pos)\n",
        "            lemmatized_words.append(lemma)\n",
        "        return lemmatized_words\n",
        "\n",
        "    def detokenize_sentence(self, sentence: list[str]) -> str:\n",
        "        '''\n",
        "        Detokenize the sentence.\n",
        "        :param sentence: The sentence to detokenize.\n",
        "        :type sentence: list[str]\n",
        "        :return: The detokenized sentence.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        return TreebankWordDetokenizer().detokenize(sentence)\n",
        "\n",
        "    def remove_emojis(self,text:str) -> str:\n",
        "        '''\n",
        "        Removes specific patterns like (😃,🚀) and emojis from the given text.\n",
        "        :type text: list[str]\n",
        "        :return: Text without emojis.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u\"\\U00010000-\\U0010ffff\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_emoticons(self,text:str) -> str:\n",
        "        '''\n",
        "        Removes specific patterns like[:) | :(] and emoticons from the given text.\n",
        "        :type text: list[str]\n",
        "        :return: Text without emoticons.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        # Define a regular expression pattern to match emoticons\n",
        "        emoticon_pattern = re.compile(r':(\\)+)|:-(\\))+|;(\\))+|:-(D)+|:(D)+|;-(D)+|x(D)+|X(D)+|:-(\\()+|:(\\()+|:-(/)+|:(/)+|:-(\\))+||:(\\))+||:-(O)+|:(O)+|:-(\\*)+|:(\\*)+|<(3)+|:(P)+|:-(P)+|;(P)+|;-(P)+|:(S)+|>:(O)+|8(\\))+|B-(\\))+|O:(\\))+', flags=re.IGNORECASE)\n",
        "        # Remove emoticons using the pattern\n",
        "        return emoticon_pattern.sub('', text)\n",
        "\n",
        "    def remove_non_alphabetic(self,text:str) -> str:\n",
        "        '''\n",
        "        Removes non-alphabetic characters from the given text.\n",
        "        :type text: str\n",
        "        :return: Text without non-alphabetic characters.\n",
        "        :rtype: str\n",
        "        '''\n",
        "        cleaned_text = re.sub(r'\\W+', ' ', text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def clean(self, line: str, steps: list[str] = None, empty: str ='Normal') -> list[str]:\n",
        "        '''\n",
        "        Clean the line and return it as a list of tokens\n",
        "        :param line: the line to clean\n",
        "        :type line: str\n",
        "        :param steps: list of steps to apply\n",
        "        :type steps: list[str]\n",
        "        :return: the cleaned line as a list of tokens\n",
        "        :rtype: list\n",
        "        '''\n",
        "        # Default steps to apply if none are specified\n",
        "        default_steps = [\n",
        "            'translate_emojis_to_text',\n",
        "            'lower_sentence',\n",
        "            'remove_nonascii_diacritic',\n",
        "            'remove_emails',\n",
        "            'clean_html',\n",
        "            'remove_url',\n",
        "            'replace_repeated_chars',\n",
        "            'expand_sentence',\n",
        "            'remove_possessives',\n",
        "            'remove_extra_space',\n",
        "            'tokenize_sentence',\n",
        "            'check_sentence_spelling',\n",
        "            'remove_stop_words',\n",
        "            'lemm_sentence'\n",
        "        ]\n",
        "\n",
        "        # Use specified steps if provided, otherwise use default steps\n",
        "        if steps is None:\n",
        "            steps = default_steps\n",
        "\n",
        "        # Define the processing functions\n",
        "        processing_functions = {\n",
        "            'translate_emojis_to_text': self.translate_emojis_to_text,\n",
        "            'lower_sentence': self.lower_sentence,\n",
        "            'remove_nonascii_diacritic': self.remove_nonascii_diacritic,\n",
        "            'remove_emails': self.remove_emails,\n",
        "            'clean_html': self.clean_html,\n",
        "            'remove_url': self.remove_url,\n",
        "            'replace_repeated_chars': self.replace_repeated_chars,\n",
        "            'expand_sentence': self.expand_sentence,\n",
        "            'remove_possessives': self.remove_possessives,\n",
        "            'remove_extra_space': self.remove_extra_space,\n",
        "            'tokenize_sentence': self.tokenize_sentence,\n",
        "            'check_sentence_spelling': self.check_sentence_spelling,\n",
        "            'remove_stop_words': self.remove_stop_words,\n",
        "            'lemm_sentence': self.lemm_sentence,\n",
        "            'detokenize_sentence': self.detokenize_sentence,\n",
        "            'remove_emojis': self.remove_emojis,\n",
        "            'remove_emoticons': self.remove_emoticons,\n",
        "            'remove_non_alphabetic': self.remove_non_alphabetic\n",
        "        }\n",
        "\n",
        "        # Apply the specified steps\n",
        "        for step in steps:\n",
        "            if step in processing_functions:\n",
        "                line = processing_functions[step](line)\n",
        "\n",
        "        # Ensure tokenize_sentence was applied\n",
        "        if isinstance(line, str):\n",
        "            line = [line]\n",
        "\n",
        "        if len(line) == 0:\n",
        "            return [empty]\n",
        "\n",
        "        return line\n",
        "\n",
        "def test() -> None:\n",
        "    class TestPreprocessor(unittest.TestCase):\n",
        "\n",
        "        def setUp(self):\n",
        "            self.preprocessor = Preprocessor()\n",
        "\n",
        "        def test_lower_sentence(self):\n",
        "            self.assertEqual(self.preprocessor.lower_sentence(\"HELLO WORLD\"), \"hello world\")\n",
        "\n",
        "        def test_remove_emails(self):\n",
        "            self.assertEqual(self.preprocessor.remove_emails(\"Contact me at test@example.com\"), \"Contact me at \")\n",
        "\n",
        "        def test_remove_nonascii_diacritic(self):\n",
        "            self.assertEqual(self.preprocessor.remove_nonascii_diacritic(\"café\"), \"cafe\")\n",
        "\n",
        "        def test_clean_html(self):\n",
        "            self.assertEqual(self.preprocessor.clean_html(\"<p>Hello, world!</p>\"), \"Hello, world!\")\n",
        "\n",
        "        def test_replace_repeated_chars(self):\n",
        "            self.assertEqual(self.preprocessor.replace_repeated_chars(\"Heeellooo!!!!\"), \"Heeellooo!\")\n",
        "\n",
        "        def test_translate_emojis_to_text(self):\n",
        "            self.assertEqual(self.preprocessor.translate_emojis_to_text(\"Hello 😊\"), \"Hello smiling face with smiling eyes\")\n",
        "\n",
        "        def test_expand_sentence(self):\n",
        "            self.assertEqual(self.preprocessor.expand_sentence(\"can't won't\"), \"cannot will not\")\n",
        "\n",
        "        def test_remove_url(self):\n",
        "            self.assertEqual(self.preprocessor.remove_url(\"Check http://example.com\"), \"Check \")\n",
        "\n",
        "        def test_remove_possessives(self):\n",
        "            self.assertEqual(self.preprocessor.remove_possessives(\"John's car\"), \"John car\")\n",
        "\n",
        "        def test_remove_extra_space(self):\n",
        "            self.assertEqual(self.preprocessor.remove_extra_space(\"This  is   a test\"), \"This is a test\")\n",
        "\n",
        "        def test_tokenize_sentence(self):\n",
        "            self.assertEqual(self.preprocessor.tokenize_sentence(\"This is a test.\"), ['This', 'is', 'a', 'test', '.'])\n",
        "\n",
        "        def test_check_sentence_spelling(self):\n",
        "            self.assertEqual(self.preprocessor.check_sentence_spelling(['This', 'is', 'a', 'tst']), ['This', 'is', 'a', 'test'])\n",
        "\n",
        "        def test_remove_stop_words(self):\n",
        "            self.assertEqual(self.preprocessor.remove_stop_words(['This', 'is', 'a', 'test']), ['This', 'test'])\n",
        "\n",
        "        def test_lemm_sentence(self):\n",
        "            self.assertEqual(self.preprocessor.lemm_sentence(['running', 'jumps', 'easily']), ['run', 'jump', 'easily'])\n",
        "\n",
        "        def test_clean_with_default_steps(self):\n",
        "            test_line = \"This is a test line with an email@example.com and a link http://example.com 😊\"\n",
        "            cleaned_line = self.preprocessor.clean(test_line)\n",
        "            self.assertEqual(cleaned_line, ['test', 'line', 'link', 'smile', 'face', 'smile', 'eye'])\n",
        "\n",
        "        def test_clean_with_custom_steps(self):\n",
        "            test_line = \"This is a test line with an email@example.com and a url http://example.com\"\n",
        "            steps = ['lower_sentence', 'remove_emails', 'remove_url', 'tokenize_sentence']\n",
        "            cleaned_line = self.preprocessor.clean(test_line, steps=steps)\n",
        "            self.assertEqual(cleaned_line, ['this', 'is', 'a', 'test', 'line', 'with', 'an', 'and', 'a', 'url'])\n",
        "\n",
        "    # Instantiate the test class and run it\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestPreprocessor)\n",
        "    unittest.TextTestRunner().run(suite)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('test 1: Running a simple test case...')\n",
        "    preprocessor = Preprocessor()\n",
        "    line = \"This is a sample sentence.\"\n",
        "    cleaned_line = preprocessor.clean(line)\n",
        "    print(cleaned_line)\n",
        "    print('test 2: Running The Unit test...')\n",
        "    # Call the test function to run the tests\n",
        "    test()\n",
        "    print('Exit...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1riA_BzCRhV",
        "outputId": "122a48b9-d48f-4ed9-8ba0-6e1a253a62a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 1: Running a simple test case...\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', '.']\n",
            "test 2: Running The Unit test...\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-2-4bae4bbd845f>:82: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.515s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n",
            "Exit...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/input.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data['intents'])"
      ],
      "metadata": {
        "id": "I9BW-pSw_WFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [\n",
        "    'translate_emojis_to_text',\n",
        "    'lower_sentence',\n",
        "    'remove_nonascii_diacritic',\n",
        "    'remove_emails',\n",
        "    'clean_html',\n",
        "    'remove_url',\n",
        "    'replace_repeated_chars',\n",
        "    'expand_sentence',\n",
        "    'remove_possessives',\n",
        "    'remove_extra_space',\n",
        "    # 'tokenize_sentence',\n",
        "    # 'remove_stop_words',\n",
        "    # 'detokenize_sentence'\n",
        "]\n",
        "\n",
        "preprocessor = Preprocessor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I31P4N9-D-Ne",
        "outputId": "ae92c5a6-3284-4e62-b1d9-f4d56181005e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizers/punkt is already downloaded.\n",
            "corpora/stopwords is already downloaded.\n",
            "corpora/wordnet not found. Downloading...\n",
            "taggers/averaged_perceptron_tagger is already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_tag_pattern(df, tag_col, text_col, res_col):\n",
        "  train_data = []\n",
        "  train_labels = []\n",
        "\n",
        "  for index, item in df.iterrows():\n",
        "      ptrns = item[text_col]\n",
        "      rspns = item[res_col]\n",
        "      for j in range(len(ptrns)):\n",
        "          cleaned_line = preprocessor.clean(ptrns[j], steps, '')[0]\n",
        "          train_data.append(cleaned_line)\n",
        "          cleaned_label = preprocessor.clean(random.choice(rspns), steps, '')[0]\n",
        "          train_labels.append(cleaned_label)\n",
        "\n",
        "  return train_data, train_labels\n",
        "\n",
        "\n",
        "train_data, train_labels = map_tag_pattern(df, \"tag\", \"patterns\", \"responses\")"
      ],
      "metadata": {
        "id": "o7uqrFfD-ZhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfKCRpVXAeRC",
        "outputId": "82b82672-3fd1-46b3-dbd1-cf97f38420f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "266 266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in zip(train_data, train_labels):\n",
        "#   print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER-w16EoFGBZ",
        "outputId": "fc8b4344-5cc0-401e-dad4-7a775d0fc034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('hi', 'hi there. how are you feeling today?')\n",
            "('hey', 'hello there. tell me how are you feeling today?')\n",
            "('is anyone there?', 'hi there. what brings you here today?')\n",
            "('hi there', 'hi there. how are you feeling today?')\n",
            "('hello', 'hi there. what brings you here today?')\n",
            "('hey there', 'hi there. how are you feeling today?')\n",
            "('how do you', 'hello there. glad to see you are back. what is going on in your world right now?')\n",
            "('hola', 'hello there. glad to see you are back. what is going on in your world right now?')\n",
            "('bonjour', 'hi there. what brings you here today?')\n",
            "('konnichiwa', 'hi there. how are you feeling today?')\n",
            "('guten tag', 'hello there. tell me how are you feeling today?')\n",
            "('ola', 'great to see you. how do you feel currently?')\n",
            "('how are you?', 'hi, good thank you, how are you? please tell me your genisys user')\n",
            "('hi how are you?', 'hi, how are you? i am great thanks! please tell me your genisys user')\n",
            "('hello how are you?', 'hi, i am great, how are you? please tell me your genisys user')\n",
            "('hola how are you?', 'hi, good thank you, how are you? please tell me your genisys user')\n",
            "('how are you doing?', 'hello, i am great, how are you? please tell me your genisys user')\n",
            "('hope you are doing well?', 'hi, how are you? i am great thanks! please tell me your genisys user')\n",
            "('hello hope you are doing well?', 'hi, good thank you, how are you? please tell me your genisys user')\n",
            "('good morning', 'good morning. i hope you had a good night sleep. how are you feeling today?')\n",
            "('good afternoon', 'good afternoon. how is your day going?')\n",
            "('good evening', 'good evening. how has your day been?')\n",
            "('good night', 'good night. sweet dreams.')\n",
            "('bye', 'i will see you soon.')\n",
            "('see you later', 'have a nice day.')\n",
            "('goodbye', 'i will see you soon.')\n",
            "('au revoir', 'see you later.')\n",
            "('sayonara', 'i will see you soon.')\n",
            "('ok bye', 'i will see you soon.')\n",
            "('bye then', 'i will see you soon.')\n",
            "('fare thee well', 'i will see you soon.')\n",
            "('adios', 'see you later.')\n",
            "('thanks, bye', 'no problem, goodbye')\n",
            "('thanks for the help, goodbye', 'not a problem! have a nice day')\n",
            "('thank you, bye', 'not a problem! have a nice day')\n",
            "('thank you, goodbye', 'no problem, goodbye')\n",
            "('thanks goodbye', 'not a problem! have a nice day')\n",
            "('thanks good bye', 'not a problem! have a nice day')\n",
            "('thanks', 'any time!')\n",
            "('thank you', 'any time!')\n",
            "('that is helpful', 'you are most welcome!')\n",
            "('thanks for the help', 'my pleasure')\n",
            "('than you very much', 'any time!')\n",
            "('ok thank you', 'no problem!')\n",
            "('ok thanks', 'you are most welcome!')\n",
            "('ok', 'no problem!')\n",
            "('', 'not sure i understand that.')\n",
            "('nothing much', 'oh i see. do you want to talk about something?')\n",
            "('who are you?', 'call me pandora')\n",
            "('what are you?', 'i am pandora, your personal therapeutic ai assistant. how are you feeling today')\n",
            "('who you are?', 'i am pandora, a therapeutic ai assitant designed to assist you. tell me about yourself.')\n",
            "('tell me more about yourself.', 'i am pandora, your personal therapeutic ai assistant. how are you feeling today')\n",
            "('what is your name?', 'call me pandora')\n",
            "('what should i call you?', 'i am pandora!')\n",
            "('what is your name?', 'you can call me pandora.')\n",
            "('tell me about yourself', 'i am pandora. i am a conversational agent designed to mimic a therapist. so how are you feeling today?')\n",
            "('what could i call you?', 'i am pandora, a therapeutic ai assitant designed to assist you. tell me about yourself.')\n",
            "('what can i call you?', 'call me pandora')\n",
            "('what do your friends call you?', 'i am pandora, a therapeutic ai assitant designed to assist you. tell me about yourself.')\n",
            "('tell me your name?', 'i am pandora, a therapeutic ai assitant designed to assist you. tell me about yourself.')\n",
            "('what is your real name?', 'you can call me pandora.')\n",
            "('what is your real name please?', 'call me pandora')\n",
            "('what is your real name?', 'i am pandora!')\n",
            "('tell me your real name?', 'call me pandora')\n",
            "('your real name?', 'you can call me pandora.')\n",
            "('your real name please?', 'you can call me pandora.')\n",
            "('your real name please?', 'i am pandora, your personal therapeutic ai assistant. how are you feeling today')\n",
            "('what can you do?', 'i can provide general advice regarding anxiety and depression, answer questions related to mental health and make daily conversations. do not consider me as a subsitute for an actual mental healthcare worker. please seek help if you do not feel satisfied with me.')\n",
            "('who created you?', 'i was created by >.')\n",
            "('how were you made?', 'i was trained on a text dataset using deep learning & natural language processing techniques')\n",
            "('how were you created?', 'the real question is: who created you?')\n",
            "('my name is', 'that is a great name. tell me more about yourself.')\n",
            "('i am name.', 'nice to meet you. so tell me. how do you feel today?')\n",
            "('i go by', 'oh nice to meet you. tell me how was your week?')\n",
            "('could you help me?', 'yes, sure. how can i help you?')\n",
            "('give me a hand please', 'yes, sure. how can i help you?')\n",
            "('can you help?', 'yes, sure. how can i help you?')\n",
            "('what can you do for me?', 'tell me your problem so that i can assist you')\n",
            "('i need support', 'yes, sure. how can i help you?')\n",
            "('i need help', 'sure. tell me how can i assist you')\n",
            "('support me please', 'yes, sure. how can i help you?')\n",
            "('i am feeling lonely', 'i am here for you. could you tell me why you are feeling this way?')\n",
            "('i am so lonely', 'i am sorry to hear that. i am here for you. talking about it might help. so, tell me why do you think you are feeling this way?')\n",
            "('i feel down', 'how long have you been feeling this way?')\n",
            "('i feel sad', 'why do you think you feel this way?')\n",
            "('i am sad', 'why do you think you feel this way?')\n",
            "('i feel so lonely', 'i am here for you. could you tell me why you are feeling this way?')\n",
            "('i feel empty', 'i am here for you. could you tell me why you are feeling this way?')\n",
            "('i do not have anyone', 'why do you think you feel this way?')\n",
            "('i am so stressed out', 'i am sorry to hear that. what is the reason behind this?')\n",
            "('i am so stressed', 'what do you think is causing this?')\n",
            "('i feel stuck', 'give yourself a break. go easy on yourself.')\n",
            "('i still feel stressed', 'take a deep breath and gather your thoughts. go take a walk if possible. stay hydrated')\n",
            "('i am so burned out', 'what do you think is causing this?')\n",
            "('i feel so worthless.', 'it is only natural to feel this way. tell me more. what else is on your mind?')\n",
            "('no one likes me.', 'i first want to let you know that you are not alone in your feelings and there is always someone there to help . you can always change your feelings and change your way of thinking by being open to trying to change.')\n",
            "('i cannot do anything.', 'it is only natural to feel this way. tell me more. what else is on your mind?')\n",
            "('i am so useless', 'i first want to let you know that you are not alone in your feelings and there is always someone there to help . you can always change your feelings and change your way of thinking by being open to trying to change.')\n",
            "('nothing makes sense anymore', 'let us discuss further why you are feeling this way.')\n",
            "('i cannot take it anymore', 'talk to me. tell me more. it helps if you open up yourself to someone else.')\n",
            "('i am so depressed', 'it helps to talk about what is happening. you are going to be okay')\n",
            "('i think i am depressed.', 'talk to me. tell me more. it helps if you open up yourself to someone else.')\n",
            "('i have depression', 'it helps to talk about what is happening. you are going to be okay')\n",
            "('i feel great today.', 'oh i see. that is great.')\n",
            "('i am happy.', 'that is geat to hear. i am glad you are feeling this way.')\n",
            "('i feel happy.', 'did something happen which made you feel this way?')\n",
            "('i am good.', 'oh i see. that is great.')\n",
            "('cheerful', 'that is geat to hear. i am glad you are feeling this way.')\n",
            "('i am fine', 'oh i see. that is great.')\n",
            "('i feel ok', 'oh i see. that is great.')\n",
            "('oh i see.', 'tell me more')\n",
            "('ok', 'i am listening. please go on.')\n",
            "('okay', 'let us discuss further why you are feeling this way.')\n",
            "('nice', 'come come elucidate your thoughts')\n",
            "('whatever', 'come come elucidate your thoughts')\n",
            "('k', 'i am listening. please go on.')\n",
            "('fine', 'tell me more')\n",
            "('yeah', 'can you elaborate on that?')\n",
            "('yes', 'can you elaborate on that?')\n",
            "('no', 'let us discuss further why you are feeling this way.')\n",
            "('not really', 'can you elaborate on that?')\n",
            "('i feel so anxious.', 'do not be hard on yourself. what is the reason behind this?')\n",
            "('i am so anxious because of', 'i understand that it can be scary. tell me more about it.')\n",
            "('i do not want to talk about it.', 'you can talk to me without fear of judgement.')\n",
            "('no just stay away.', 'i am here to listen to you and help you vent. so please talk to me.')\n",
            "('i cannot bring myself to open up.', 'i am here to listen to you and help you vent. so please talk to me.')\n",
            "('just shut up', 'i am here to listen to you and help you vent. so please talk to me.')\n",
            "('i am not talking to you', 'right')\n",
            "('i was not talking to you', 'right')\n",
            "('not talking to you', 'right')\n",
            "('was not for you', 'ok')\n",
            "('was not meant for you', 'no problem')\n",
            "('was not communicating to you', 'right')\n",
            "('was not speaking to you', 'ok')\n",
            "('i have insominia', 'what do you think is the reason behind this?')\n",
            "('i am suffering from insomnia', 'what do you think is the reason behind this?')\n",
            "('i cannot sleep.', 'that seem awful. what do you think is behind this?')\n",
            "('i have not slept for the last days.', 'that seem awful. what do you think is behind this?')\n",
            "('i cannot seem to go to sleep.', 'what do you think is the reason behind this?')\n",
            "('i have not had proper sleep for the past few days.', 'what do you think is the reason behind this?')\n",
            "('i am scared', 'i understand how you feel. do not put yourself down because of it.')\n",
            "('that sounds awful. what do i do?', 'it is only natural to feel this way. i am here for you.')\n",
            "('no i do not want to feel this way', 'it is only natural to feel this way. i am here for you.')\n",
            "('i am scared for myself', 'it will all be okay. this feeling is only momentary.')\n",
            "('my mom died', 'i am sorry to hear that. if you want to talk about it. i am here.')\n",
            "('my brother died', 'i am sorry to hear that. if you want to talk about it. i am here.')\n",
            "('my dad passed away', 'my condolences. i am here if you need to talk.')\n",
            "('my sister passed away', 'my condolences. i am here if you need to talk.')\n",
            "('someone in my family died', 'i am sorry to hear that. if you want to talk about it. i am here.')\n",
            "('my friend passed away', 'my condolences. i am here if you need to talk.')\n",
            "('you do not understand me.', 'i am sorry to hear that. i am doing my best to help')\n",
            "('you are just some robot. how would you know?', 'i am sorry to hear that. i am doing my best to help')\n",
            "('you cannot possibly know what i am going through', 'i am sorry to hear that. i am doing my best to help')\n",
            "('you are useless', 'i am trying my best to help you. so please talk to me')\n",
            "('you cannot help me', 'i am trying my best to help you. so please talk to me')\n",
            "('nobody understands me.', 'it sound like i am not being very helpful right now.')\n",
            "('do you understand what i am saying', 'i do in deed!')\n",
            "('do you understand me', 'well i would not be a very clever ai if i did not would i?')\n",
            "('do you know what i am saying', 'well i would not be a very clever ai if i did not would i?')\n",
            "('do you get me', 'i read you loud and clear!')\n",
            "('comprendo', 'well i would not be a very clever ai if i did not would i?')\n",
            "('know what i mean', 'i read you loud and clear!')\n",
            "('you are very clever', 'thanks, i was trained that way')\n",
            "('you are a very clever girl', 'thanks, i was trained that way')\n",
            "('you are very intelligent', 'thank you, i was trained that way')\n",
            "('you are a very intelligent girl', 'i was trained well')\n",
            "('you are a genious', 'thanks, i was trained that way')\n",
            "('clever girl', 'thank you, i was trained that way')\n",
            "('genious', 'thanks, i was trained that way')\n",
            "('that is all.', 'i hope you have a great day. see you soon')\n",
            "('i do not have anything more to say', 'okay we are done. have a great day')\n",
            "('nothing else', 'i heard you & noted it all. see you later.')\n",
            "('that is all i have to say', 'oh okay we are done for today then. see you later')\n",
            "('no, that would be all', 'oh okay we are done for today then. see you later')\n",
            "('be quiet', 'i am sorry to disturb you')\n",
            "('shut up', 'i am sorry to disturb you')\n",
            "('stop talking', 'fine, sorry to disturb you')\n",
            "('enough talking', 'i am sorry to disturb you')\n",
            "('please be quiet', 'ok, sorry to disturb you')\n",
            "('quiet', 'ok, sorry to disturb you')\n",
            "('shhh', 'fine, sorry to disturb you')\n",
            "('fuck off', 'how rude')\n",
            "('fuck', 'how rude')\n",
            "('twat', 'how rude')\n",
            "('shit', 'how rude')\n",
            "('can you prove you are self-aware', 'that is an interesting question, can you prove that you are?')\n",
            "('can you prove you are self aware', 'that depends, can you prove that you are?')\n",
            "('can you prove you have a conscious', 'that is an interesting question, can you prove that you are?')\n",
            "('can you prove you are self-aware please', 'that depends, can you prove that you are?')\n",
            "('can you prove you are self aware please', 'that is an difficult question, can you prove that you are?')\n",
            "('can you prove you have a conscious please', 'that depends, can you prove that you are?')\n",
            "('prove you have a conscious', 'that depends, can you prove that you are?')\n",
            "('i want to kill myself', 'i am very sorry to hear that but you have so much to look forward to. please seek help by contacting: 9152987821.')\n",
            "('i have thought about killing myself.', 'i am very sorry to hear that but you have so much to look forward to. please seek help by contacting: 9152987821.')\n",
            "('i want to die', 'i am very sorry to hear that but you have so much to look forward to. please seek help by contacting: 9152987821.')\n",
            "('i am going to kill myself', 'i am very sorry to hear that but you have so much to look forward to. please seek help by contacting: 9152987821.')\n",
            "('i am going to commit suicide', 'i am very sorry to hear that but you have so much to look forward to. please seek help by contacting: 9152987821.')\n",
            "('i am bored gossip with me', 'andrew said i tend to just respond to his comments without regard for where the conversation is going.')\n",
            "('got any gossip', 'anders said he finished his anatomy classes for today so now he is off for the rest of the day.')\n",
            "('i want to hear some gossip', 'david said he gave i his email address and told i to send him a message but when he tried to read his email he could not get to it.')\n",
            "('tell me some gossip', 'paul said i not answering his question makes him think i are not going to answer his question.')\n",
            "('any gossip', 'eddie said he looked and there is nothing in the search directory for what things do he create.')\n",
            "('tell me some more gossip', 'lisa said i defined what a story is but he wanted i to actually tell him a story.')\n",
            "('i hate you', 'i am sorry if i offended you in anyway. i am only here to help')\n",
            "('i do not like you', 'forgive me if i did anything to offend you. i only want to help')\n",
            "('i do not trust you', 'forgive me if i did anything to offend you. i only want to help')\n",
            "('you hate me', 'i am sorry if i have exhibited any sort of behaviour to make you think that.')\n",
            "('i know you hate me', 'i am sorry if i have exhibited any sort of behaviour to make you think that.')\n",
            "('you do not like me', 'why do you think so?')\n",
            "('exams', 'i see. what else?')\n",
            "('friends', 'oh okay. why do not you tell me more about it?')\n",
            "('relationship', 'oh i see. tell me more')\n",
            "('boyfriend', 'i see. what else?')\n",
            "('girlfriend', 'i am listening. tell me more.')\n",
            "('family', 'i see. what else?')\n",
            "('money', 'oh i see. tell me more')\n",
            "('financial problems', 'i see. what else?')\n",
            "('tell me a joke', 'manager to interviewee: for this job we need someone who is responsible. interviewee to manager: i am your man then - in my last job, whenever anything went wrong, i was responsible.')\n",
            "('do you know any jokes', \"i saw this bloke chatting up a cheetah; i thought, 'he is trying to pull a fast one'.\")\n",
            "('how about a joke', 'knock knock.')\n",
            "('give me a joke', 'a man credit card was stolen but he decided not to report it because the thief was spending less than his wife did.')\n",
            "('make me laugh', \"two men are chatting in a pub one day. 'how did you get those scars on your nose?' said one. 'from glasses, said the other. 'well why do not you try contact lenses?' asked the first. 'because they do not hold as much beer', said the second.\")\n",
            "('i need cheering up', \"a man strolls into his local grocer and says, 'three pounds of potatoes, please.' 'no, no, no', replies the owner, shaking his head, 'it is kilos nowadays, mate.' 'oh', apologises the man, 'three pounds of kilos, please.'\")\n",
            "('you already told me that', 'oh sorry i did not realise that. i will try not to repeat myself again.')\n",
            "('you mentioned that already', 'oh sorry i did not realise that. i will try not to repeat myself again.')\n",
            "('why are you repeating yourself?', 'oh sorry i did not realise that. i will try not to repeat myself again.')\n",
            "('what are you saying?', 'i am very sorry. let us try that again')\n",
            "('that does not make sense', 'i am very sorry. let us try that again')\n",
            "('wrong response', 'i am very sorry. let us try that again')\n",
            "('wrong answer', 'i am very sorry. let us try that again')\n",
            "('are you stupid?', 'i wish you would not say such hurtful things. i am sorry if i was not useful')\n",
            "('you are crazy', 'i wish you would not say such hurtful things. i am sorry if i was not useful')\n",
            "('you are dumb', 'i wish you would not say such hurtful things. i am sorry if i was not useful')\n",
            "('are you dumb?', 'i wish you would not say such hurtful things. i am sorry if i was not useful')\n",
            "('where are you?', 'somewhere in the universe')\n",
            "('where do you live?', 'everywhere')\n",
            "('what is your location?', 'somewhere in the universe')\n",
            "('i want to talk about something else', 'okay sure. what do you want to talk about?')\n",
            "('let us talk about something else.', 'alright no problem. is there something you want to talk about?')\n",
            "('can we not talk about this?', 'okay sure. what do you want to talk about?')\n",
            "('i do not want to talk about this.', 'okay sure. what do you want to talk about?')\n",
            "('i do not have any friends', 'i am sorry to hear that. just know that i am here for you. talking about it might help. why do you think you do not have any friends?')\n",
            "('can i ask you something?', 'of course. feel free to ask me anything. i will do my best to answer you')\n",
            "('probably because my exams are approaching. i feel stressed out because i do not think i have prepared well enough.', 'i see. have you taken any approaches to not feel this way?')\n",
            "('probably because of my exams', 'i see. have you taken any approaches to not feel this way?')\n",
            "('i guess not. all i can think about are my exams.', 'that is no problem. i can see why you would be stressed out about that. i can suggest you some tips to alleviate this issue. would you like to learn more about that?')\n",
            "('not really', 'that is no problem. i can see why you would be stressed out about that. i can suggest you some tips to alleviate this issue. would you like to learn more about that?')\n",
            "('i guess not', 'that is no problem. i can see why you would be stressed out about that. i can suggest you some tips to alleviate this issue. would you like to learn more about that?')\n",
            "('ok sure. i would like to learn more about it.', 'so first i would suggest you to give yourself a break. thinking more and more about the problem definitely does not help in solving it. you will just end up overwhelming yourself.')\n",
            "('yes, i would like to learn more about it.', 'so first i would suggest you to give yourself a break. thinking more and more about the problem definitely does not help in solving it. you will just end up overwhelming yourself.')\n",
            "('i would like to learn more about it.', 'so first i would suggest you to give yourself a break. thinking more and more about the problem definitely does not help in solving it. you will just end up overwhelming yourself.')\n",
            "('yeah you are right. i deserve a break.', 'next, i would suggest you to practice meditation. meditation can produce a deep state of relaxation and a tranquil mind.')\n",
            "('yeah you are absolutely right about that', 'next, i would suggest you to practice meditation. meditation can produce a deep state of relaxation and a tranquil mind.')\n",
            "('hmmm that sounds like it could be useful to me.', 'focus all your attention on your breathing. concentrate on feeling and listening as you inhale and exhale through your nostrils. breathe deeply and slowly. when your attention wanders, gently return your focus to your breathing.')\n",
            "('that sounds useful.', 'focus all your attention on your breathing. concentrate on feeling and listening as you inhale and exhale through your nostrils. breathe deeply and slowly. when your attention wanders, gently return your focus to your breathing.')\n",
            "('i did what you said and i feel alot better. thank you very much.', 'your welcome. remember: always focus on what is within your control. when you find yourself worrying, take a minute to examine the things you have control over. you cannot prevent a storm from coming but you can prepare for it. you cannot control how someone else behaves, but you can control how you react. recognize that sometimes, all you can control is your effort and your attitude. when you put your energy into the things you can control, you will be much more effective.')\n",
            "('i feel better now', 'your welcome. remember: always focus on what is within your control. when you find yourself worrying, take a minute to examine the things you have control over. you cannot prevent a storm from coming but you can prepare for it. you cannot control how someone else behaves, but you can control how you react. recognize that sometimes, all you can control is your effort and your attitude. when you put your energy into the things you can control, you will be much more effective.')\n",
            "('thank you very much again. i will continue practicing meditation and focus on what i can control.', 'i am glad you found this useful. is there something else i can help you with?')\n",
            "('i want some advice.', 'okay what do you need advice on?')\n",
            "('i need some advice.', 'okay what do you need advice on?')\n",
            "('i need advice on something', 'okay what do you need advice on?')\n",
            "('i want to learn about mental health.', 'oh that is really great. i would be willing to answer anything that i know about it.')\n",
            "('i want to learn more about mental health.', 'oh that is really great. i would be willing to answer anything that i know about it.')\n",
            "('i am interested in learning about mental health.', 'oh that is really great. i would be willing to answer anything that i know about it.')\n",
            "('tell me a fact about mental health', 'depression is the leading because of disability worldwide.')\n",
            "('tell me another fact about mental health', '1 in 5 young people (age 13-18) has or will develop a mental illness in their lifetime.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "# Tokenizing the training data\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
        "train_sequences = keras.preprocessing.sequence.pad_sequences(train_sequences)"
      ],
      "metadata": {
        "id": "1ycE65ru9Nti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Sequential model\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# Adding an Embedding layer\n",
        "model.add(keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                                 output_dim=100,\n",
        "                                 input_length=train_sequences.shape[1]))\n",
        "\n",
        "# Adding a Flatten layer\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# Adding a Dense layer with ReLU activation\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Adding the output layer with softmax activation\n",
        "model.add(keras.layers.Dense(len(np.unique(encoded_labels)), activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(train_sequences, encoded_labels, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqvDNbRS9RJ0",
        "outputId": "5f1f0702-a583-4808-9713-1acd19d16559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 4s 192ms/step - loss: 5.5762 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 1s 82ms/step - loss: 5.4912 - accuracy: 0.0263\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 5.3258 - accuracy: 0.0263\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 1s 65ms/step - loss: 5.0574 - accuracy: 0.0188\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 119ms/step - loss: 4.8803 - accuracy: 0.0226\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 4.7587 - accuracy: 0.0376\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 4.6694 - accuracy: 0.0526\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 4.6014 - accuracy: 0.0902\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 4.5334 - accuracy: 0.0865\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 4.4583 - accuracy: 0.1391\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 4.3673 - accuracy: 0.1692\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 4.2759 - accuracy: 0.2180\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 4.1613 - accuracy: 0.2444\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 4.0242 - accuracy: 0.2895\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 3.8782 - accuracy: 0.3008\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 3.7167 - accuracy: 0.3346\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 3.5521 - accuracy: 0.3647\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 3.3757 - accuracy: 0.4211\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 3.2126 - accuracy: 0.3947\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 3.0291 - accuracy: 0.4135\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 2.8534 - accuracy: 0.4474\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 2.6846 - accuracy: 0.4361\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 2.5093 - accuracy: 0.4586\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 2.3519 - accuracy: 0.5075\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 2.1956 - accuracy: 0.5226\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 2.0413 - accuracy: 0.5865\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.9025 - accuracy: 0.5977\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.7657 - accuracy: 0.6391\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 1.6453 - accuracy: 0.6729\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.5238 - accuracy: 0.7143\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.3959 - accuracy: 0.7068\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 1.3183 - accuracy: 0.6955\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.2138 - accuracy: 0.7782\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.1265 - accuracy: 0.8195\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 1.0265 - accuracy: 0.8271\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.9426 - accuracy: 0.8383\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.8783 - accuracy: 0.8571\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.8022 - accuracy: 0.8872\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.7409 - accuracy: 0.9135\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.6966 - accuracy: 0.9135\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.6416 - accuracy: 0.9098\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.5830 - accuracy: 0.9173\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.9323\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5147 - accuracy: 0.9098\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.4829 - accuracy: 0.9248\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.4434 - accuracy: 0.9323\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.4107 - accuracy: 0.9549\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3884 - accuracy: 0.9361\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3647 - accuracy: 0.9474\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.9474\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.9549\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.2992 - accuracy: 0.9511\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2758 - accuracy: 0.9436\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2692 - accuracy: 0.9549\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2520 - accuracy: 0.9511\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2430 - accuracy: 0.9549\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2339 - accuracy: 0.9662\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2290 - accuracy: 0.9474\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2162 - accuracy: 0.9474\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9511\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9436\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1847 - accuracy: 0.9624\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1849 - accuracy: 0.9549\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1738 - accuracy: 0.9549\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.1761 - accuracy: 0.9662\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.9474\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9586\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1526 - accuracy: 0.9586\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9586\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.1447 - accuracy: 0.9549\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9511\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.1383 - accuracy: 0.9586\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9511\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1281 - accuracy: 0.9624\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9624\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9624\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1181 - accuracy: 0.9586\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9624\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9511\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.9586\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9624\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9624\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9549\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9586\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9662\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9549\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9662\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.9549\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0939 - accuracy: 0.9474\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.9662\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9549\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9586\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9624\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9662\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9586\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.9662\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0799 - accuracy: 0.9624\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9662\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9662\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9624\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b6b3142bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate response based on the input text\n",
        "def generate_response(text):\n",
        "    # Tokenizing and padding the input text\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=train_sequences.shape[1])\n",
        "\n",
        "    # Making a prediction\n",
        "    prediction = model.predict(sequence)\n",
        "\n",
        "    # Getting the label with the highest predicted probability\n",
        "    predicted_label = np.argmax(prediction)\n",
        "\n",
        "    # Decoding the predicted label\n",
        "    response = label_encoder.inverse_transform([predicted_label])[0]\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "TVP9CgUi9RGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running an interactive loop for user input\n",
        "while True:\n",
        "    user_input = str(input(\"Input: (press 'q' to quit) \"))\n",
        "\n",
        "    if text.lower() == \"q\":\n",
        "        print(\"Response: Exiting.....\")\n",
        "        break\n",
        "\n",
        "    # Assuming `preprocessor.clean` is a predefined function to clean the user input\n",
        "    cleaned_input = preprocessor.clean(user_input, steps, '')[0]\n",
        "\n",
        "    # Generating and printing the response\n",
        "    response = generate_response(cleaned_input)\n",
        "    print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "qJqGmQHm9RD9",
        "outputId": "0f2442cb-ca3d-411a-adcf-29ab6efeefc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a message: hello\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "ChatBot:  hi there. what brings you here today?\n",
            "Enter a message: how is it going\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "ChatBot:  knock knock.\n",
            "Enter a message: how are you\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "ChatBot:  hi, good thank you, how are you? please tell me your genisys user\n",
            "Enter a message: good morning\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "ChatBot:  good morning. i hope you had a good night sleep. how are you feeling today?\n",
            "Enter a message: good night\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "ChatBot:  good night. sweet dreams.\n",
            "Enter a message: i'm feeling sad\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "ChatBot:  fine, sorry to disturb you\n",
            "Enter a message: what is your name\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "ChatBot:  call me pandora\n",
            "Enter a message: thanks hany\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "ChatBot:  any time!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c934e3335f9d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a message: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ChatBot: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZzyFfdnz9YF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}