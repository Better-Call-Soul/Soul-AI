{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0CSpZqZoi5S"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "# import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import string\n",
        "# import re\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.feature_selection import SelectKBest, chi2\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.svm import LinearSVC, SVC\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import numpy as np\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# import neattext.functions as nfx\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# # # Download NLTK resources\n",
        "# # nltk.download('punkt')\n",
        "# # nltk.download('stopwords')\n",
        "# # nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wpt7xAwoi5U"
      },
      "source": [
        "**MELD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W8Wvg9Doi5V",
        "outputId": "b5038399-7449-4425-bbb0-ea11dc5ad86b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Utterance</th>\n",
              "      <th>Clean_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>also I was the point person on my companys tr...</td>\n",
              "      <td>also I was the point person on my companys tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You mustve had your hands full.</td>\n",
              "      <td>You mustve had your hands full</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That I did. That I did.</td>\n",
              "      <td>That I did That I did</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>So lets talk a little bit about your duties.</td>\n",
              "      <td>So lets talk a little bit about your duties</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My duties?  All right.</td>\n",
              "      <td>My duties  All right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9984</th>\n",
              "      <td>You or me?</td>\n",
              "      <td>You or me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9985</th>\n",
              "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
              "      <td>I got it Uh Joey women dont have Adams apples</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9986</th>\n",
              "      <td>You guys are messing with me, right?</td>\n",
              "      <td>You guys are messing with me right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9987</th>\n",
              "      <td>Yeah.</td>\n",
              "      <td>Yeah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988</th>\n",
              "      <td>That was a good one. For a second there, I was...</td>\n",
              "      <td>That was a good one For a second there I was l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Utterance  \\\n",
              "0     also I was the point person on my companys tr...   \n",
              "1                      You mustve had your hands full.   \n",
              "2                               That I did. That I did.   \n",
              "3         So lets talk a little bit about your duties.   \n",
              "4                                My duties?  All right.   \n",
              "...                                                 ...   \n",
              "9984                                         You or me?   \n",
              "9985  I got it. Uh, Joey, women don't have Adam's ap...   \n",
              "9986               You guys are messing with me, right?   \n",
              "9987                                              Yeah.   \n",
              "9988  That was a good one. For a second there, I was...   \n",
              "\n",
              "                                             Clean_Text  \n",
              "0     also I was the point person on my companys tra...  \n",
              "1                        You mustve had your hands full  \n",
              "2                                 That I did That I did  \n",
              "3           So lets talk a little bit about your duties  \n",
              "4                                  My duties  All right  \n",
              "...                                                 ...  \n",
              "9984                                          You or me  \n",
              "9985      I got it Uh Joey women dont have Adams apples  \n",
              "9986                 You guys are messing with me right  \n",
              "9987                                               Yeah  \n",
              "9988  That was a good one For a second there I was l...  \n",
              "\n",
              "[9989 rows x 2 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# csv_file_path = r'D:\\College\\Fourth Year\\GP\\Meld\\train_sent_emo.csv'\n",
        "# df = pd.read_csv(csv_file_path)\n",
        "# df.isnull().sum()\n",
        "# # df['Emotion'].value_counts().plot(kind='bar')\n",
        "# df['Clean_Text'] = df['Utterance'].apply(nfx.remove_multiple_spaces)\n",
        "# df['Clean_Text'] = df['Utterance'].apply(nfx.remove_bad_quotes)\n",
        "# df['Clean_Text'] = df['Utterance'].apply(nfx.remove_special_characters)\n",
        "# df[['Utterance','Clean_Text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2v2ZYdNoi5V"
      },
      "outputs": [],
      "source": [
        "# # Split the data into features (X) and labels (y)\n",
        "# X = df['Clean_Text']\n",
        "# y = df['Emotion']\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZJy2DVToi5W"
      },
      "outputs": [],
      "source": [
        "# def update_class_weight(y_train):\n",
        "#     # Define the class labels\n",
        "#     class_labels = ['neutral', 'joy', 'surprise', 'anger', 'sadness', 'disgust', 'fear']\n",
        "\n",
        "#     # Map class labels to their corresponding indices\n",
        "#     class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "\n",
        "#     # Convert emotions to class indices\n",
        "#     class_indices_array = np.array([class_indices[emotion] for emotion in y_train])\n",
        "\n",
        "#     # Calculate class weights based on the inverse of class frequencies\n",
        "#     class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(class_indices_array), y=class_indices_array)\n",
        "#     label_encoder = LabelEncoder()\n",
        "#     integer_labels = label_encoder.fit_transform(y_train)\n",
        "#     class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "#     return class_weight_dict,class_indices_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRJ2iXOgoi5W"
      },
      "outputs": [],
      "source": [
        "# def tune_hyperparameters(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "#     grid_search = GridSearchCV(model, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
        "#     grid_search.fit(X_train, y_train)\n",
        "\n",
        "#     print(\"Best parameters found:\")\n",
        "#     print(grid_search.best_params_)\n",
        "\n",
        "#     best_model = grid_search.best_estimator_\n",
        "\n",
        "#     y_pred = best_model.predict(X_test)\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     print(\"Best model accuracy:\", accuracy)\n",
        "\n",
        "#     return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCWHUvtXoi5W",
        "outputId": "96c82713-29d0-44e5-e8bd-9b8028afc3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3958958958958959\n"
          ]
        }
      ],
      "source": [
        "# class_labels = ['neutral', 'joy', 'surprise', 'anger', 'sadness', 'disgust', 'fear']\n",
        "# class_weight,y_train_indices = update_class_weight(y_train)\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# # Transform the testing data\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# # Initialize the SVM classifier\n",
        "# svm_classifier = SVC(kernel='linear',class_weight=class_weight)\n",
        "\n",
        "# # Fit the SVM classifier on the TF-IDF transformed training data\n",
        "# svm_classifier.fit(X_train_tfidf, y_train_indices)\n",
        "\n",
        "# # Predict the labels for the testing data\n",
        "# y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "\n",
        "# # Convert emotions to class indices\n",
        "# class_indices_array = np.array([class_indices[emotion] for emotion in y_test])\n",
        "\n",
        "# # Calculate the accuracy of the model\n",
        "# accuracy = accuracy_score(class_indices_array, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO1SZ8q7oi5W",
        "outputId": "e8450431-dda6-497d-d5de-d5ac32ae68b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best parameters found:\n",
            "{'C': 10.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Best model accuracy: 0.5145145145145145\n",
            "Accuracy: 0.3958958958958959\n"
          ]
        }
      ],
      "source": [
        "# param_grid = {\n",
        "#     'C': [0.01, 0.1, 1.0, 10.0],\n",
        "#     'kernel': ['linear', 'rbf'],\n",
        "#     'gamma': ['scale', 'auto']\n",
        "# }\n",
        "# best_model = tune_hyperparameters(svm_classifier, param_grid, X_train_tfidf, y_train_indices, X_test_tfidf, class_indices_array)\n",
        "# best_model.fit(X_train_tfidf, y_train_indices)\n",
        "# # Predict the labels for the testing data\n",
        "# y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "\n",
        "# # Convert emotions to class indices\n",
        "# class_indices_array = np.array([class_indices[emotion] for emotion in y_test])\n",
        "\n",
        "# # Calculate the accuracy of the model\n",
        "# accuracy = accuracy_score(class_indices_array, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ_nBe92oi5W",
        "outputId": "0154b5a2-f209-4960-b8f7-040fe09e1874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.36436436436436437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# logistic_regression = LogisticRegression(class_weight=class_weight)\n",
        "# logistic_regression.fit(X_train_tfidf, y_train_indices)\n",
        "# y_pred2 = logistic_regression.predict(X_test_tfidf)\n",
        "# accuracy = accuracy_score(class_indices_array, y_pred2)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqM7CbgWoi5X",
        "outputId": "a4373554-aa2e-4e36-ff03-28f6167dfdde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters found:\n",
            "{'C': 10.0, 'penalty': 'l2'}\n",
            "Best model accuracy: 0.3953953953953954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# # Define the parameter grid for hyperparameter tuning\n",
        "# param_grid = {\n",
        "#     'C': [0.01, 0.1, 1.0, 10.0],\n",
        "#     'penalty': ['l2']\n",
        "# }\n",
        "\n",
        "# # Perform hyperparameter tuning\n",
        "# best_model = tune_hyperparameters(logistic_regression, param_grid, X_train_tfidf, y_train_indices, X_test_tfidf, class_indices_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgTNFiOaoi5X",
        "outputId": "1b5c7e23-f337-45d8-c99a-a1a8cd4aae82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3813813813813814\n"
          ]
        }
      ],
      "source": [
        "# decision_tree = DecisionTreeClassifier(class_weight=class_weight)\n",
        "# decision_tree.fit(X_train_tfidf, y_train_indices)\n",
        "# y_pred3 = decision_tree.predict(X_test_tfidf)\n",
        "# accuracy = accuracy_score(class_indices_array, y_pred3)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqqzicAPoi5X",
        "outputId": "afcec106-8031-442d-a0d5-79d90bed2404"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.534034034034034"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])\n",
        "# pipe_lr.fit(X_train,y_train)\n",
        "# pipe_lr.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUa9KSZroi5X"
      },
      "source": [
        "**DAILY DIALOGUE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6a3HZ4Coi5X",
        "outputId": "23a18212-3e1e-477e-aae4-93c9f03fdad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spellchecker\n",
            "  Using cached spellchecker-0.4-py3-none-any.whl\n",
            "Collecting setuptools (from spellchecker)\n",
            "  Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting inexactsearch (from spellchecker)\n",
            "  Using cached inexactsearch-1.0.2-py3-none-any.whl\n",
            "Collecting soundex>=1.0 (from inexactsearch->spellchecker)\n",
            "  Using cached soundex-1.1.3-py3-none-any.whl\n",
            "Collecting silpa-common>=0.3 (from inexactsearch->spellchecker)\n",
            "  Using cached silpa_common-0.3-py3-none-any.whl\n",
            "Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "Installing collected packages: silpa-common, setuptools, soundex, inexactsearch, spellchecker\n",
            "  Attempting uninstall: silpa-common\n",
            "    Found existing installation: silpa_common 0.3\n",
            "    Uninstalling silpa_common-0.3:\n",
            "      Successfully uninstalled silpa_common-0.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 69.5.1\n",
            "    Uninstalling setuptools-69.5.1:\n",
            "      Successfully uninstalled setuptools-69.5.1\n",
            "  Attempting uninstall: soundex\n",
            "    Found existing installation: soundex 1.1.3\n",
            "    Uninstalling soundex-1.1.3:\n",
            "      Successfully uninstalled soundex-1.1.3\n",
            "  Attempting uninstall: inexactsearch\n",
            "    Found existing installation: inexactsearch 1.0.2\n",
            "    Uninstalling inexactsearch-1.0.2:\n",
            "      Successfully uninstalled inexactsearch-1.0.2\n",
            "  Attempting uninstall: spellchecker\n",
            "    Found existing installation: spellchecker 0.4\n",
            "    Uninstalling spellchecker-0.4:\n",
            "      Successfully uninstalled spellchecker-0.4\n",
            "Successfully installed inexactsearch-1.0.2 setuptools-69.5.1 silpa-common-0.3 soundex-1.1.3 spellchecker-0.4\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade --force-reinstall spellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNUFKBjBoi5X",
        "outputId": "98f0e6ce-7b24-4e0a-ae1d-c46ff91bf406"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\emans\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Tuple, Callable, Dict\n",
        "import argparse\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import torch.autograd\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
        "                             recall_score, classification_report, confusion_matrix)\n",
        "import numpy as np\n",
        "import contractions\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "import emoji\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0xeXlnznoi5Y"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"D:/College/Fourth Year/GP/dailydialog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "25cf3hVpoi5Y"
      },
      "outputs": [],
      "source": [
        "train_data_path = f\"{dataset_path}/train/dialogues_train.txt\"\n",
        "train_label_path = f\"{dataset_path}/train/dialogues_emotion_train.txt\"\n",
        "test_data_path = f\"{dataset_path}/test/dialogues_test.txt\"\n",
        "test_label_path = f\"{dataset_path}/test/dialogues_emotion_test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uu7ZAmi2oi5Y"
      },
      "outputs": [],
      "source": [
        "def lower_sentence(sentence: str) -> str:\n",
        "    '''\n",
        "    Lowercase the sentence.\n",
        "    :param data: The sentence to lowercase.\n",
        "    :return: The lowercased sentence\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return sentence.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gw209eUsoi5Y"
      },
      "outputs": [],
      "source": [
        "def remove_emails(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove emails from the sentence.\n",
        "    :param sentence: The sentence to remove emails from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without emails.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(r\"\\S*@\\S*\\s?\", \"\", sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DO9o0n5hoi5Y"
      },
      "outputs": [],
      "source": [
        "def remove_nonascii_diacritic(sentence: str) -> str:\n",
        "    '''\n",
        "\n",
        "    Remove diacritics from the sentence.\n",
        "\n",
        "    :param sentence: The sentence to remove diacritics from.\n",
        "\n",
        "    :type sentence: str\n",
        "\n",
        "    :return: The sentence without diacritics.\n",
        "\n",
        "    :rtype: str\n",
        "    '''\n",
        "\n",
        "    return unicodedata.normalize(\"NFKD\", sentence).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oopTPMoKoi5Y"
      },
      "outputs": [],
      "source": [
        "def clean_html(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove HTML tags from the sentence.\n",
        "    :param sentence: The sentence to remove HTML tags from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without HTML tags.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return BeautifulSoup(sentence, \"html.parser\").get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mZAhdj_Zoi5Y"
      },
      "outputs": [],
      "source": [
        "def replace_repeated_chars(sentence: str) -> str:\n",
        "    '''\n",
        "    Replace repeated characters in the sentence.\n",
        "    :param sentence: The sentence to replace repeated characters in.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with replaced repeated characters.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Replace consecutive occurrences of ',', '!', '.', and '?' with a single occurrence\n",
        "    return re.sub(r'([,!?.])\\1+', r'\\1', sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S2oM7pFnoi5Y"
      },
      "outputs": [],
      "source": [
        "def translate_emojis_to_text(sentence: str) -> str:\n",
        "    '''\n",
        "    Translate emojis in the sentence to text.\n",
        "    :param sentence: The sentence to translate emojis to text.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with translated emojis to text.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Translate emojis to text codes\n",
        "    translated_text = emoji.demojize(sentence)\n",
        "    # Remove colons from the translated text\n",
        "    translated_text = re.sub(r':', '', translated_text)\n",
        "    return translated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5BW58YLjoi5Y"
      },
      "outputs": [],
      "source": [
        "def expand_sentence(sentence: str) -> str:\n",
        "    '''\n",
        "    Expand the contractions in the sentence.\n",
        "    :param sentence: The sentence to expand contractions in.\n",
        "    :type sentence: str\n",
        "    :return: The sentence with expanded contractions.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return contractions.fix(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BYrMyBQToi5Z"
      },
      "outputs": [],
      "source": [
        "def remove_url(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove URLs from the sentence.\n",
        "    :param sentence: The sentence to remove URLs from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without URLs.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\", '', sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QmhjCm7toi5Z"
      },
      "outputs": [],
      "source": [
        "def remove_possessives(sentence: str) -> str:\n",
        "    '''\n",
        "    Strip possessives from the sentence.\n",
        "    :param sentence: The sentence to strip possessives from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without possessives.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    # Stripping the possessives\n",
        "    sentence = sentence.replace(\"'s\", '')\n",
        "    sentence = sentence.replace('’s', '')\n",
        "    sentence = sentence.replace('s’', 's')\n",
        "    sentence = sentence.replace(\"s'\", 's')\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Iius6YyYoi5Z"
      },
      "outputs": [],
      "source": [
        "def remove_extra_space(sentence: str) -> str:\n",
        "    '''\n",
        "    Remove extra spaces from the sentence.\n",
        "    :param sentence: The sentence to remove extra spaces from.\n",
        "    :type sentence: str\n",
        "    :return: The sentence without extra spaces.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return re.sub(r'\\s+', ' ', sentence).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ehtsg2PLoi5Z"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence: str) -> list[str]:\n",
        "    '''\n",
        "    Tokenize the sentence.\n",
        "    :param sentence: The sentence to tokenize.\n",
        "    :type sentence: str\n",
        "    :return: The tokenized sentence.\n",
        "    :rtype: str\n",
        "    '''\n",
        "    return nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HO79DcDnoi5Z"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(sentence: list[str]) -> list[str]:\n",
        "    '''\n",
        "    Remove stop words from the sentence.\n",
        "    :param sentence: The sentence to remove stop words from.\n",
        "    :type sentence: list[str]\n",
        "    :return: The sentence without stop words.\n",
        "    :rtype: list[str]\n",
        "    '''\n",
        "    return [word for word in sentence if word not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zy1YVsUloi5Z"
      },
      "outputs": [],
      "source": [
        "def lemm_sentence(sentence: list[str]) -> list[str]:\n",
        "    '''\n",
        "    Lemmatize the sentence.\n",
        "    :param sentence: The sentence to lemmatize.\n",
        "    :type sentence: list[str]\n",
        "    :return: The lemmatized sentence.\n",
        "    :rtype: list[str]\n",
        "    '''\n",
        "    # Perform POS tagging\n",
        "    pos_tags = pos_tag(sentence)\n",
        "    # Lemmatize each word based on its POS tag\n",
        "    lemmatized_words = []\n",
        "    for word, pos in pos_tags:\n",
        "        # Map Penn Treebank POS tags to WordNet POS tags\n",
        "        if pos.startswith('N'):  # Nouns\n",
        "            pos = 'n'\n",
        "        elif pos.startswith('V'):  # Verbs\n",
        "            pos = 'v'\n",
        "        elif pos.startswith('J'):  # Adjectives\n",
        "            pos = 'a'\n",
        "        elif pos.startswith('R'):  # Adverbs\n",
        "            pos = 'r'\n",
        "        else:\n",
        "            pos = 'n'  # Default to noun if POS tag not found\n",
        "\n",
        "        # Lemmatize the word using the appropriate POS tag\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "        lemmatized_words.append(lemma)\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OnpBWs8voi5Z"
      },
      "outputs": [],
      "source": [
        "def clean_train(line: str) -> list[str]:\n",
        "    '''\n",
        "    Clean the line and return it as a list of tokens\n",
        "    :param line: the line to clean\n",
        "    :type line: str\n",
        "    :return: the cleaned line as a list of tokens\n",
        "    :rtype: list\n",
        "    '''\n",
        "    # translate emojis\n",
        "    line = translate_emojis_to_text(line)\n",
        "    # lower the line\n",
        "    line = lower_sentence(line)\n",
        "    # remove non ascii\n",
        "    line = remove_nonascii_diacritic(line)\n",
        "    # remove emails\n",
        "    line = remove_emails(line)\n",
        "    # remove html\n",
        "    line = clean_html(line)\n",
        "    # remove urls\n",
        "    line = remove_url(line)\n",
        "    # replace repeated chars\n",
        "    line = replace_repeated_chars(line)\n",
        "    # expand\n",
        "    line = expand_sentence(line)\n",
        "    # remove possessives\n",
        "    line = remove_possessives(line)\n",
        "    # remove extra spaces\n",
        "    line = remove_extra_space(line)\n",
        "    # tekonize\n",
        "    line = tokenize_sentence(line)\n",
        "    # remove stopwords\n",
        "    line = remove_stop_words(line)\n",
        "    # lemmetization\n",
        "    line = lemm_sentence(line)\n",
        "    if len(line) == 0:\n",
        "        return ['Normal']\n",
        "    return line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PGOhAPYPoi5a"
      },
      "outputs": [],
      "source": [
        "def read_dialogue_data(file_path: str) -> List[List[str]]:\n",
        "    '''\n",
        "    Read the dialogue data from the file path.\n",
        "    :param file_path: The path of the file.\n",
        "    :type file_path: str\n",
        "    :return: A list of dialogues, where each dialogue is a list of sentences,\n",
        "             and each sentence is a string.\n",
        "    :rtype: list\n",
        "    '''\n",
        "    # define dialogues list\n",
        "    dialogues = []\n",
        "    # read data file\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as file_data:\n",
        "        dialogues = [\n",
        "            [\n",
        "                sentence.replace(\".\", \" . \").replace(\"?\", \" ? \").replace(\"!\", \" ! \").replace(\n",
        "                    \";\", \" ; \").replace(\":\", \" : \").replace(\",\", \" , \").strip()\n",
        "                for sentence in line.split(\"__eou__\") if sentence.strip()\n",
        "            ]\n",
        "            for line in file_data\n",
        "        ]\n",
        "    return dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1u_TgyDQoi5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_dataset_dailyDialog(data_path: str, label_path: str) -> Tuple[List[List[List[str]]], List[List[int]]]:\n",
        "    '''\n",
        "    Take the data path and the label path and read them.\n",
        "    It then splits the conversations and extracts each conversation, sentence, and words of each sentence.\n",
        "    It reads the labels of each sentence in the conversation\n",
        "\n",
        "    :param data_path: The path of the conversations.\n",
        "    :type data_path: str\n",
        "    :param label_path: The path of the labels for the conversations.\n",
        "    :type label_path: str\n",
        "    :return: A tuple containing inputs and targets.\n",
        "             inputs: List of conversations, where each conversation is a list of sentences,\n",
        "                     and each sentence is a list of words.\n",
        "             targets: List of labels for each conversation.\n",
        "    :rtype: tuple\n",
        "    '''\n",
        "    # define targets list\n",
        "    targets = []\n",
        "    # read labels file\n",
        "    with open(label_path, \"r\", encoding=\"utf8\") as file_data:\n",
        "        targets = [[int(label) for label in line.strip(\n",
        "            \"\\n\").strip(\" \").split(\" \")] for line in file_data]\n",
        "        # for loop version\n",
        "        # for line in file_data:\n",
        "        #     labels = [int(label) for label in line.strip(\"\\n\").strip(\" \").split(\" \")]\n",
        "        #     targets.append(labels)\n",
        "\n",
        "    # read data file\n",
        "    dialogues = read_dialogue_data(data_path)\n",
        "    # define inputs list\n",
        "    inputs = [\n",
        "        [\n",
        "            clean_train(sentence) for sentence in dialogue\n",
        "        ]\n",
        "        for dialogue in dialogues\n",
        "    ]\n",
        "    return (inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl2MdnxUoi5a",
        "outputId": "4f3f2a54-ed44-4ab8-c004-370d905d79fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\temp\\ipykernel_12020\\964458276.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11118\n",
            "11118\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = read_dataset_dailyDialog(train_data_path, train_label_path)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9lwwCkQoi5a",
        "outputId": "cbeaef37-9550-4aa4-ceeb-5c04163322d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\temp\\ipykernel_12020\\964458276.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  return BeautifulSoup(sentence, \"html.parser\").get_text()\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = read_dataset_dailyDialog(test_data_path, test_label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OX95PFnBoi5b"
      },
      "outputs": [],
      "source": [
        "flattened_X_train = [' '.join(sentence) for dialog in X_train for sentence in dialog]\n",
        "# Flatten X_train into a list of strings\n",
        "flattened_X_test = [' '.join(sentence) for dialog in X_test for sentence in dialog]\n",
        "\n",
        "# Flatten y_train into a single list\n",
        "y_train_flat = [label for dialog_labels in y_train for label in dialog_labels]\n",
        "y_test_flat = [label for dialog_labels in y_test for label in dialog_labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDv8qTOCoi5b",
        "outputId": "309f8f4f-03a1-4906-90d6-f15710a90f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "[[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]]\n"
          ]
        }
      ],
      "source": [
        "print(type(X_train))\n",
        "print(y_train[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5201gzwwqg78",
        "outputId": "9c48208e-2aee-418f-f513-889fad8bef98"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn xgboost\n",
        "# !pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Mp4S8PiASnOo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression with Count Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression with Count Vectorizer: {'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.64      0.76      6321\n",
            "           1       0.13      0.45      0.20       118\n",
            "           2       0.07      0.26      0.11        47\n",
            "           3       0.12      0.59      0.20        17\n",
            "           4       0.40      0.64      0.49      1019\n",
            "           5       0.12      0.54      0.20       102\n",
            "           6       0.10      0.50      0.17       116\n",
            "\n",
            "    accuracy                           0.63      7740\n",
            "   macro avg       0.27      0.52      0.30      7740\n",
            "weighted avg       0.81      0.63      0.69      7740\n",
            "\n",
            "============================================================\n",
            "Training Logistic Regression with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression with TF-IDF Vectorizer: {'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.62      0.74      6321\n",
            "           1       0.12      0.45      0.18       118\n",
            "           2       0.09      0.30      0.14        47\n",
            "           3       0.12      0.47      0.19        17\n",
            "           4       0.38      0.65      0.48      1019\n",
            "           5       0.12      0.57      0.20       102\n",
            "           6       0.10      0.53      0.17       116\n",
            "\n",
            "    accuracy                           0.61      7740\n",
            "   macro avg       0.26      0.51      0.30      7740\n",
            "weighted avg       0.81      0.61      0.68      7740\n",
            "\n",
            "============================================================\n",
            "Training Logistic Regression with Word2Vec\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression with Word2Vec: {'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.42      0.58      6321\n",
            "           1       0.06      0.20      0.09       118\n",
            "           2       0.02      0.32      0.03        47\n",
            "           3       0.01      0.47      0.03        17\n",
            "           4       0.37      0.49      0.43      1019\n",
            "           5       0.07      0.55      0.12       102\n",
            "           6       0.09      0.56      0.15       116\n",
            "\n",
            "    accuracy                           0.43      7740\n",
            "   macro avg       0.22      0.43      0.20      7740\n",
            "weighted avg       0.81      0.43      0.54      7740\n",
            "\n",
            "============================================================\n",
            "Training Logistic Regression with FastText\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression with FastText: {'classifier__C': 0.1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.42      0.58      6321\n",
            "           1       0.07      0.23      0.11       118\n",
            "           2       0.02      0.38      0.04        47\n",
            "           3       0.01      0.29      0.02        17\n",
            "           4       0.37      0.45      0.41      1019\n",
            "           5       0.05      0.54      0.09       102\n",
            "           6       0.08      0.51      0.14       116\n",
            "\n",
            "    accuracy                           0.42      7740\n",
            "   macro avg       0.22      0.40      0.20      7740\n",
            "weighted avg       0.81      0.42      0.53      7740\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Naive Bayes with Count Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Naive Bayes with Count Vectorizer: {'classifier__alpha': 1.5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.96      0.90      6321\n",
            "           1       0.00      0.00      0.00       118\n",
            "           2       0.00      0.00      0.00        47\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.59      0.32      0.42      1019\n",
            "           5       0.00      0.00      0.00       102\n",
            "           6       0.12      0.01      0.02       116\n",
            "\n",
            "    accuracy                           0.83      7740\n",
            "   macro avg       0.22      0.19      0.19      7740\n",
            "weighted avg       0.77      0.83      0.79      7740\n",
            "\n",
            "============================================================\n",
            "Training Naive Bayes with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Naive Bayes with TF-IDF Vectorizer: {'classifier__alpha': 0.5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.99      0.90      6321\n",
            "           1       0.00      0.00      0.00       118\n",
            "           2       0.00      0.00      0.00        47\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.68      0.17      0.27      1019\n",
            "           5       0.00      0.00      0.00       102\n",
            "           6       0.60      0.03      0.05       116\n",
            "\n",
            "    accuracy                           0.83      7740\n",
            "   macro avg       0.30      0.17      0.17      7740\n",
            "weighted avg       0.78      0.83      0.77      7740\n",
            "\n",
            "============================================================\n",
            "Training Naive Bayes with FastText\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 759, in fit\n    self._count(X, Y)\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 881, in _count\n    check_non_negative(X, \"MultinomialNB (input X)\")\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1650, in check_non_negative\n    raise ValueError(\"Negative values in data passed to %s\" % whom)\nValueError: Negative values in data passed to MultinomialNB (input X)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Train the model with hyperparameter tuning\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Predict and print classification report\u001b[39;00m\n\u001b[0;32m    103\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    945\u001b[0m     )\n\u001b[1;32m--> 947\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 759, in fit\n    self._count(X, Y)\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 881, in _count\n    check_non_negative(X, \"MultinomialNB (input X)\")\n  File \"d:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1650, in check_non_negative\n    raise ValueError(\"Negative values in data passed to %s\" % whom)\nValueError: Negative values in data passed to MultinomialNB (input X)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    # 'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training K-Nearest Neighbors with Count Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for K-Nearest Neighbors with Count Vectorizer: {'classifier__n_neighbors': 3}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88      6321\n",
            "           1       0.50      0.11      0.18       118\n",
            "           2       0.45      0.11      0.17        47\n",
            "           3       1.00      0.12      0.21        17\n",
            "           4       0.44      0.41      0.42      1019\n",
            "           5       0.46      0.06      0.10       102\n",
            "           6       0.28      0.26      0.27       116\n",
            "\n",
            "    accuracy                           0.80      7740\n",
            "   macro avg       0.57      0.28      0.32      7740\n",
            "weighted avg       0.78      0.80      0.79      7740\n",
            "\n",
            "============================================================\n",
            "Training K-Nearest Neighbors with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for K-Nearest Neighbors with TF-IDF Vectorizer: {'classifier__n_neighbors': 3}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90      6321\n",
            "           1       0.49      0.17      0.25       118\n",
            "           2       0.55      0.13      0.21        47\n",
            "           3       0.75      0.18      0.29        17\n",
            "           4       0.52      0.33      0.40      1019\n",
            "           5       0.43      0.09      0.15       102\n",
            "           6       0.46      0.22      0.30       116\n",
            "\n",
            "    accuracy                           0.82      7740\n",
            "   macro avg       0.58      0.29      0.36      7740\n",
            "weighted avg       0.79      0.82      0.80      7740\n",
            "\n",
            "============================================================\n",
            "Training K-Nearest Neighbors with Word2Vec\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.81396123 0.82643111        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for K-Nearest Neighbors with Word2Vec: {'classifier__n_neighbors': 5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90      6321\n",
            "           1       0.24      0.08      0.12       118\n",
            "           2       0.25      0.02      0.04        47\n",
            "           3       0.33      0.06      0.10        17\n",
            "           4       0.53      0.31      0.39      1019\n",
            "           5       0.60      0.06      0.11       102\n",
            "           6       0.34      0.18      0.24       116\n",
            "\n",
            "    accuracy                           0.82      7740\n",
            "   macro avg       0.45      0.24      0.27      7740\n",
            "weighted avg       0.79      0.82      0.79      7740\n",
            "\n",
            "============================================================\n",
            "Training K-Nearest Neighbors with FastText\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for K-Nearest Neighbors with FastText: {'classifier__n_neighbors': 7}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.96      0.90      6321\n",
            "           1       0.07      0.01      0.02       118\n",
            "           2       1.00      0.02      0.04        47\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.54      0.28      0.37      1019\n",
            "           5       0.30      0.03      0.05       102\n",
            "           6       0.36      0.18      0.24       116\n",
            "\n",
            "    accuracy                           0.82      7740\n",
            "   macro avg       0.45      0.21      0.23      7740\n",
            "weighted avg       0.78      0.82      0.79      7740\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest with Count Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Random Forest with Count Vectorizer: {'classifier__n_estimators': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.86      6321\n",
            "           1       0.24      0.29      0.26       118\n",
            "           2       0.23      0.21      0.22        47\n",
            "           3       0.38      0.29      0.33        17\n",
            "           4       0.48      0.51      0.50      1019\n",
            "           5       0.26      0.25      0.25       102\n",
            "           6       0.12      0.42      0.19       116\n",
            "\n",
            "    accuracy                           0.76      7740\n",
            "   macro avg       0.37      0.40      0.37      7740\n",
            "weighted avg       0.80      0.76      0.78      7740\n",
            "\n",
            "============================================================\n",
            "Training Random Forest with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Random Forest with TF-IDF Vectorizer: {'classifier__n_estimators': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87      6321\n",
            "           1       0.29      0.30      0.30       118\n",
            "           2       0.35      0.19      0.25        47\n",
            "           3       0.57      0.24      0.33        17\n",
            "           4       0.53      0.46      0.49      1019\n",
            "           5       0.31      0.22      0.26       102\n",
            "           6       0.12      0.33      0.17       116\n",
            "\n",
            "    accuracy                           0.79      7740\n",
            "   macro avg       0.44      0.37      0.38      7740\n",
            "weighted avg       0.80      0.79      0.79      7740\n",
            "\n",
            "============================================================\n",
            "Training Random Forest with Word2Vec\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Random Forest with Word2Vec: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89      6321\n",
            "           1       0.46      0.28      0.35       118\n",
            "           2       0.43      0.19      0.26        47\n",
            "           3       0.80      0.24      0.36        17\n",
            "           4       0.56      0.39      0.46      1019\n",
            "           5       0.28      0.15      0.19       102\n",
            "           6       0.19      0.32      0.24       116\n",
            "\n",
            "    accuracy                           0.81      7740\n",
            "   macro avg       0.51      0.35      0.39      7740\n",
            "weighted avg       0.80      0.81      0.80      7740\n",
            "\n",
            "============================================================\n",
            "Training Random Forest with FastText\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Random Forest with FastText: {'classifier__n_estimators': 50}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89      6321\n",
            "           1       0.47      0.26      0.34       118\n",
            "           2       0.43      0.19      0.26        47\n",
            "           3       0.80      0.24      0.36        17\n",
            "           4       0.55      0.38      0.45      1019\n",
            "           5       0.27      0.14      0.18       102\n",
            "           6       0.19      0.32      0.24       116\n",
            "\n",
            "    accuracy                           0.81      7740\n",
            "   macro avg       0.51      0.35      0.39      7740\n",
            "weighted avg       0.80      0.81      0.80      7740\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        " \n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost with Count Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for XGBoost with Count Vectorizer: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.97      0.91      6321\n",
            "           1       0.70      0.06      0.11       118\n",
            "           2       0.00      0.00      0.00        47\n",
            "           3       1.00      0.06      0.11        17\n",
            "           4       0.65      0.32      0.43      1019\n",
            "           5       0.54      0.07      0.12       102\n",
            "           6       0.33      0.05      0.09       116\n",
            "\n",
            "    accuracy                           0.84      7740\n",
            "   macro avg       0.58      0.22      0.25      7740\n",
            "weighted avg       0.81      0.84      0.80      7740\n",
            "\n",
            "============================================================\n",
            "Training XGBoost with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost with TF-IDF Vectorizer: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91      6321\n",
            "           1       0.83      0.08      0.15       118\n",
            "           2       0.67      0.04      0.08        47\n",
            "           3       1.00      0.06      0.11        17\n",
            "           4       0.65      0.35      0.46      1019\n",
            "           5       0.47      0.08      0.13       102\n",
            "           6       0.53      0.16      0.25       116\n",
            "\n",
            "    accuracy                           0.84      7740\n",
            "   macro avg       0.72      0.25      0.30      7740\n",
            "weighted avg       0.82      0.84      0.81      7740\n",
            "\n",
            "============================================================\n",
            "Training XGBoost with Word2Vec\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost with Word2Vec: {'classifier__n_estimators': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      6321\n",
            "           1       0.57      0.20      0.30       118\n",
            "           2       0.57      0.17      0.26        47\n",
            "           3       0.75      0.18      0.29        17\n",
            "           4       0.64      0.36      0.46      1019\n",
            "           5       0.48      0.14      0.21       102\n",
            "           6       0.45      0.20      0.28       116\n",
            "\n",
            "    accuracy                           0.84      7740\n",
            "   macro avg       0.62      0.32      0.39      7740\n",
            "weighted avg       0.82      0.84      0.82      7740\n",
            "\n",
            "============================================================\n",
            "Training XGBoost with FastText\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
            "d:\\Python\\Lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost with FastText: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.96      0.91      6321\n",
            "           1       0.54      0.22      0.31       118\n",
            "           2       0.64      0.15      0.24        47\n",
            "           3       0.75      0.18      0.29        17\n",
            "           4       0.64      0.35      0.45      1019\n",
            "           5       0.39      0.11      0.17       102\n",
            "           6       0.48      0.27      0.34       116\n",
            "\n",
            "    accuracy                           0.84      7740\n",
            "   macro avg       0.61      0.32      0.39      7740\n",
            "weighted avg       0.82      0.84      0.82      7740\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        " \n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Decision Tree with Count Vectorizer\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters for Decision Tree with Count Vectorizer: {'classifier__max_depth': 20}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86      6321\n",
            "           1       0.12      0.03      0.04       118\n",
            "           2       0.12      0.06      0.08        47\n",
            "           3       0.21      0.24      0.22        17\n",
            "           4       0.42      0.33      0.37      1019\n",
            "           5       0.18      0.38      0.25       102\n",
            "           6       0.13      0.27      0.17       116\n",
            "\n",
            "    accuracy                           0.77      7740\n",
            "   macro avg       0.29      0.31      0.29      7740\n",
            "weighted avg       0.76      0.77      0.76      7740\n",
            "\n",
            "============================================================\n",
            "Training Decision Tree with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters for Decision Tree with TF-IDF Vectorizer: {'classifier__max_depth': None}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.82      6321\n",
            "           1       0.16      0.31      0.21       118\n",
            "           2       0.17      0.26      0.21        47\n",
            "           3       0.15      0.24      0.19        17\n",
            "           4       0.40      0.54      0.46      1019\n",
            "           5       0.16      0.28      0.21       102\n",
            "           6       0.11      0.48      0.18       116\n",
            "\n",
            "    accuracy                           0.71      7740\n",
            "   macro avg       0.29      0.41      0.32      7740\n",
            "weighted avg       0.79      0.71      0.74      7740\n",
            "\n",
            "============================================================\n",
            "Training Decision Tree with Word2Vec\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters for Decision Tree with Word2Vec: {'classifier__max_depth': None}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.82      0.85      6321\n",
            "           1       0.25      0.30      0.27       118\n",
            "           2       0.24      0.23      0.24        47\n",
            "           3       0.19      0.24      0.21        17\n",
            "           4       0.39      0.46      0.42      1019\n",
            "           5       0.16      0.22      0.18       102\n",
            "           6       0.15      0.36      0.21       116\n",
            "\n",
            "    accuracy                           0.74      7740\n",
            "   macro avg       0.32      0.37      0.34      7740\n",
            "weighted avg       0.78      0.74      0.76      7740\n",
            "\n",
            "============================================================\n",
            "Training Decision Tree with FastText\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters for Decision Tree with FastText: {'classifier__max_depth': None}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.82      0.85      6321\n",
            "           1       0.26      0.29      0.27       118\n",
            "           2       0.18      0.19      0.19        47\n",
            "           3       0.24      0.24      0.24        17\n",
            "           4       0.39      0.48      0.43      1019\n",
            "           5       0.14      0.19      0.16       102\n",
            "           6       0.15      0.36      0.21       116\n",
            "\n",
            "    accuracy                           0.74      7740\n",
            "   macro avg       0.32      0.37      0.33      7740\n",
            "weighted avg       0.78      0.74      0.76      7740\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    # 'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        " \n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SVM with Count Vectorizer\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    # 'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    # 'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    # 'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    # 'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    # 'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]})\n",
        "    'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "# classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "#                                                        ('rf', classifiers['Random Forest']),\n",
        "#                                                        ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        " \n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmuYHmhHq6ne",
        "outputId": "94cad1f1-a541-4c13-e83d-ab6407a71849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression with TF-IDF Vectorizer: {'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.61      0.74      6321\n",
            "           1       0.12      0.45      0.18       118\n",
            "           2       0.09      0.30      0.14        47\n",
            "           3       0.12      0.47      0.19        17\n",
            "           4       0.38      0.65      0.48      1019\n",
            "           5       0.12      0.56      0.20       102\n",
            "           6       0.10      0.55      0.17       116\n",
            "\n",
            "    accuracy                           0.61      7740\n",
            "   macro avg       0.26      0.51      0.30      7740\n",
            "weighted avg       0.81      0.61      0.67      7740\n",
            "\n",
            "============================================================\n",
            "Training Naive Bayes with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for Naive Bayes with TF-IDF Vectorizer: {'classifier__alpha': 0.5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.99      0.90      6321\n",
            "           1       0.00      0.00      0.00       118\n",
            "           2       0.00      0.00      0.00        47\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.68      0.17      0.27      1019\n",
            "           5       0.00      0.00      0.00       102\n",
            "           6       0.60      0.03      0.05       116\n",
            "\n",
            "    accuracy                           0.83      7740\n",
            "   macro avg       0.30      0.17      0.17      7740\n",
            "weighted avg       0.78      0.83      0.77      7740\n",
            "\n",
            "============================================================\n",
            "Training K-Nearest Neighbors with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for K-Nearest Neighbors with TF-IDF Vectorizer: {'classifier__n_neighbors': 5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.96      0.90      6321\n",
            "           1       0.55      0.10      0.17       118\n",
            "           2       0.75      0.06      0.12        47\n",
            "           3       0.00      0.00      0.00        17\n",
            "           4       0.57      0.29      0.39      1019\n",
            "           5       0.83      0.05      0.09       102\n",
            "           6       0.40      0.18      0.25       116\n",
            "\n",
            "    accuracy                           0.83      7740\n",
            "   macro avg       0.56      0.24      0.27      7740\n",
            "weighted avg       0.80      0.83      0.80      7740\n",
            "\n",
            "============================================================\n",
            "Training Random Forest with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Random Forest with TF-IDF Vectorizer: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      6321\n",
            "           1       0.31      0.29      0.30       118\n",
            "           2       0.35      0.19      0.25        47\n",
            "           3       0.57      0.24      0.33        17\n",
            "           4       0.53      0.46      0.49      1019\n",
            "           5       0.34      0.24      0.28       102\n",
            "           6       0.12      0.34      0.18       116\n",
            "\n",
            "    accuracy                           0.79      7740\n",
            "   macro avg       0.44      0.38      0.39      7740\n",
            "weighted avg       0.80      0.79      0.79      7740\n",
            "\n",
            "============================================================\n",
            "Training XGBoost with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters for XGBoost with TF-IDF Vectorizer: {'classifier__n_estimators': 150}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91      6321\n",
            "           1       0.67      0.08      0.15       118\n",
            "           2       0.67      0.04      0.08        47\n",
            "           3       1.00      0.06      0.11        17\n",
            "           4       0.65      0.34      0.45      1019\n",
            "           5       0.53      0.09      0.15       102\n",
            "           6       0.49      0.15      0.23       116\n",
            "\n",
            "    accuracy                           0.84      7740\n",
            "   macro avg       0.69      0.25      0.30      7740\n",
            "weighted avg       0.82      0.84      0.81      7740\n",
            "\n",
            "============================================================\n",
            "Training Decision Tree with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameters for Decision Tree with TF-IDF Vectorizer: {'classifier__max_depth': None}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.76      0.82      6321\n",
            "           1       0.16      0.31      0.21       118\n",
            "           2       0.17      0.26      0.21        47\n",
            "           3       0.17      0.29      0.21        17\n",
            "           4       0.40      0.54      0.46      1019\n",
            "           5       0.17      0.27      0.21       102\n",
            "           6       0.12      0.51      0.19       116\n",
            "\n",
            "    accuracy                           0.71      7740\n",
            "   macro avg       0.30      0.42      0.33      7740\n",
            "weighted avg       0.79      0.71      0.74      7740\n",
            "\n",
            "============================================================\n",
            "Training SVM with TF-IDF Vectorizer\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Custom transformer for FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return np.array([np.mean([self.model.wv[word] for word in sentence.split() if word in self.model.wv]\n",
        "                                 or [np.zeros(self.size)], axis=0) for sentence in X])\n",
        "\n",
        "# Example dataset\n",
        "X_train, X_test, y_train, y_test = flattened_X_train,flattened_X_test,y_train_flat,y_test_flat\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': (LogisticRegression(), {'classifier__C': [0.1, 1, 10]}),\n",
        "    'Naive Bayes': (MultinomialNB(), {'classifier__alpha': [0.5, 1.0, 1.5]}),\n",
        "    'K-Nearest Neighbors': (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7]}),\n",
        "    'Random Forest': (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {'classifier__n_estimators': [50, 100, 150]}),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), {'classifier__max_depth': [None, 10, 20, 30]}),\n",
        "    'SVM': (SVC(probability=True), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']})\n",
        "}\n",
        "\n",
        "# Ensemble Classifier\n",
        "classifiers['Ensemble'] = VotingClassifier(estimators=[('lr', classifiers['Logistic Regression']),\n",
        "                                                       ('rf', classifiers['Random Forest']),\n",
        "                                                       ('svc', classifiers['SVM'])], voting='soft')\n",
        "\n",
        "# List of vectorizers\n",
        "vectorizers = {\n",
        "    # 'Count Vectorizer': CountVectorizer(),\n",
        "    'TF-IDF Vectorizer': TfidfVectorizer(),\n",
        "    'Word2Vec': Word2VecTransformer(),\n",
        "    'FastText': FastTextTransformer()\n",
        "}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Iterate over each classifier and vectorizer\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, (clf, param_grid)  in classifiers.items():\n",
        "        print(f\"Training {clf_name} with {vec_name}\")\n",
        "\n",
        "        # Create a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        if hasattr(clf, 'class_weight'):\n",
        "            clf.set_params(class_weight=class_weight_dict)\n",
        "\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "        # Train the model with hyperparameter tuning\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and print classification report\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        print(f\"Best parameters for {clf_name} with {vec_name}: {grid_search.best_params_}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eT-1umYoi5b",
        "outputId": "eac93376-4f9e-4863-e9e1-67e66b298e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.63      0.75      6321\n",
            "           1       0.12      0.47      0.19       118\n",
            "           2       0.08      0.30      0.12        47\n",
            "           3       0.09      0.59      0.16        17\n",
            "           4       0.41      0.64      0.50      1019\n",
            "           5       0.13      0.57      0.21       102\n",
            "           6       0.10      0.56      0.17       116\n",
            "\n",
            "    accuracy                           0.62      7740\n",
            "   macro avg       0.26      0.54      0.30      7740\n",
            "weighted avg       0.81      0.62      0.69      7740\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
        "\n",
        "# # Convert class weights to a dictionary\n",
        "# class_weight_dict = dict(zip(np.unique(y_train_flat), class_weights))\n",
        "# pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression(class_weight=class_weight_dict))])\n",
        "# pipe_lr.fit(flattened_X_train,y_train_flat)\n",
        "\n",
        "# y_pred = pipe_lr.predict(flattened_X_test)\n",
        "# report = classification_report(y_test_flat, y_pred)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWy0cdE4oi5b",
        "outputId": "a882c506-ee95-496d-9ad3-112cace166c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.842764857881137\n"
          ]
        }
      ],
      "source": [
        "# # Assuming X_train and X_test are lists of lists\n",
        "# # Flatten X_train into a list of strings\n",
        "# flattened_X_train = [' '.join(sentence) for dialog in X_train for sentence in dialog]\n",
        "# # Flatten X_train into a list of strings\n",
        "# flattened_X_test = [' '.join(sentence) for dialog in X_test for sentence in dialog]\n",
        "\n",
        "# # Flatten y_train into a single list\n",
        "# y_train_flat = [label for dialog_labels in y_train for label in dialog_labels]\n",
        "# y_test_flat = [label for dialog_labels in y_test for label in dialog_labels]\n",
        "\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
        "\n",
        "# # Transform the testing data\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
        "\n",
        "# # Initialize the SVM classifier\n",
        "# svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# # Fit the SVM classifier on the TF-IDF transformed training data\n",
        "# svm_classifier.fit(X_train_tfidf, y_train_flat)\n",
        "\n",
        "# # Predict the labels for the testing data\n",
        "# y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6yKqUnPoi5g"
      },
      "outputs": [],
      "source": [
        "# def save_model(model, save_path):\n",
        "#     \"\"\"\n",
        "#     Save a trained model to a file using pickle.\n",
        "\n",
        "#     Parameters:\n",
        "#     - model: Trained model object\n",
        "#     - save_path: File path to save the model\n",
        "#     \"\"\"\n",
        "#     with open(save_path, 'wb') as f:\n",
        "#         pickle.dump(model, f)\n",
        "#     print(\"Model saved to:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7uXXEeVoi5g",
        "outputId": "f8e0703e-4e16-4dc5-eb27-b55bbf6c5404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML\n"
          ]
        }
      ],
      "source": [
        "# save_model(svm_classifier,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLjN_1Zmoi5g"
      },
      "outputs": [],
      "source": [
        "# no emotion (0), anger (1), disgust (2), fear (3), happiness (4), sadness (5) and surprise (6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m20JWeSDoi5g",
        "outputId": "8983ac66-a8fe-4c2d-fb3b-749cfb1247eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted label: [0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# # Example of a new sentence\n",
        "# new_sentence = \"I don't know . My life is a big mess .\"\n",
        "\n",
        "# preprocessed_text = clean_train(new_sentence)\n",
        "\n",
        "# # Transform the preprocessed sentence into TF-IDF representation\n",
        "# tfidf_vectorized_sentence = tfidf_vectorizer.transform(preprocessed_text)\n",
        "\n",
        "# # Predict the label for the TF-IDF transformed sentence\n",
        "# predicted_label = svm_classifier.predict(tfidf_vectorized_sentence)\n",
        "\n",
        "# print(\"Predicted label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ypVgvfRoi5g",
        "outputId": "f68bbea4-2f14-476c-e777-a82073d4347e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6600775193798449\n"
          ]
        }
      ],
      "source": [
        "# # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
        "\n",
        "# # Convert class weights to a dictionary\n",
        "# class_weight_dict = dict(zip(np.unique(y_train_flat), class_weights))\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
        "\n",
        "# # Transform the testing data\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
        "\n",
        "# # Initialize the SVM classifier\n",
        "# svm_classifier = SVC(kernel='linear', class_weight=class_weight_dict)\n",
        "\n",
        "# # Fit the SVM classifier on the TF-IDF transformed training data\n",
        "# svm_classifier.fit(X_train_tfidf, y_train_flat)\n",
        "\n",
        "# # Predict the labels for the testing data\n",
        "# y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcn2fLpRoi5h",
        "outputId": "dc6e85f3-599f-40e3-ac0f-823ff0272c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML_weighted\n"
          ]
        }
      ],
      "source": [
        "# save_model(svm_classifier,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_ML_weighted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcDP0qE3oi5h",
        "outputId": "a206fdb4-8ab0-4a0f-c3d9-c93afb2d3510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sad']\n",
            "Predicted label: [5]\n"
          ]
        }
      ],
      "source": [
        "# # Example of a new sentence\n",
        "# new_sentence = \"sad\"\n",
        "\n",
        "# preprocessed_text = clean_train(new_sentence)\n",
        "# print(preprocessed_text)\n",
        "# # Transform the preprocessed sentence into TF-IDF representation\n",
        "# tfidf_vectorized_sentence = tfidf_vectorizer.transform(preprocessed_text)\n",
        "\n",
        "# # Predict the label for the TF-IDF transformed sentence\n",
        "# predicted_label = svm_classifier.predict(tfidf_vectorized_sentence)\n",
        "\n",
        "# print(\"Predicted label:\", predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgnXvSkooi5h",
        "outputId": "1bf084d6-8eea-48e9-e520-33e3fc6c0a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6076227390180878\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.61      0.73      6321\n",
            "           1       0.11      0.50      0.18       118\n",
            "           2       0.08      0.32      0.13        47\n",
            "           3       0.08      0.59      0.14        17\n",
            "           4       0.40      0.64      0.49      1019\n",
            "           5       0.12      0.58      0.20       102\n",
            "           6       0.10      0.55      0.18       116\n",
            "\n",
            "    accuracy                           0.61      7740\n",
            "   macro avg       0.26      0.54      0.29      7740\n",
            "weighted avg       0.81      0.61      0.67      7740\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# # Flatten X_train into a list of strings\n",
        "# flattened_X_train = [' '.join(sentence) for dialog in X_train for sentence in dialog]\n",
        "# # Flatten X_train into a list of strings\n",
        "# flattened_X_test = [' '.join(sentence) for dialog in X_test for sentence in dialog]\n",
        "\n",
        "# # Flatten y_train into a single list\n",
        "# y_train_flat = [label for dialog_labels in y_train for label in dialog_labels]\n",
        "# y_test_flat = [label for dialog_labels in y_test for label in dialog_labels]\n",
        "\n",
        "# # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
        "\n",
        "# # Convert class weights to a dictionary\n",
        "# class_weight_dict = dict(zip(np.unique(y_train_flat), class_weights))\n",
        "\n",
        "# logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(flattened_X_train)\n",
        "\n",
        "# # Transform the testing data\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(flattened_X_test)\n",
        "\n",
        "# logistic_regression.fit(X_train_tfidf, y_train_flat)\n",
        "# y_pred2 = logistic_regression.predict(X_test_tfidf)\n",
        "# accuracy = accuracy_score(y_test_flat, y_pred2)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# report = classification_report(y_test_flat, y_pred2)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf9I3QYKoi5h"
      },
      "outputs": [],
      "source": [
        "# emotion_map = {\n",
        "#     0: 'neutral',\n",
        "#     1: 'anger',\n",
        "#     2: 'disgust',\n",
        "#     3: 'fear',\n",
        "#     4: 'happiness',\n",
        "#     5: 'sadness',\n",
        "#     6: 'surprise'\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxRkeoz2oi5i",
        "outputId": "b0499400-8dd5-4b4d-8b89-eb11c9a859fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['okay', ',', 'give', 'try', '.', 'still', 'scar', 'doubtful', '.']\n",
            "Probability of neutral: 0.06441659914938573\n",
            "Probability of anger: 0.007847991876326342\n",
            "Probability of disgust: 0.005675938055684583\n",
            "Probability of fear: 0.8110976504209492\n",
            "Probability of happiness: 0.06154204997979236\n",
            "Probability of sadness: 0.020639569339246187\n",
            "Probability of surprise: 0.028780201178615725\n",
            "Predicted emotion: fear\n"
          ]
        }
      ],
      "source": [
        "# # Example of a new sentence\n",
        "# new_sentence = \"Okay, I'll give it a try. But I'm still scared and doubtful.\"\n",
        "\n",
        "# preprocessed_text = clean_train(new_sentence)\n",
        "# print(preprocessed_text)\n",
        "# # Transform the preprocessed sentence into TF-IDF representation\n",
        "# tfidf_vectorized_sentence = tfidf_vectorizer.transform([\" \".join(preprocessed_text)])\n",
        "\n",
        "# # Predict the label for the TF-IDF transformed sentence\n",
        "# predicted_emotion = logistic_regression.predict(tfidf_vectorized_sentence)[0]\n",
        "\n",
        "# class_probabilities = logistic_regression.predict_proba(tfidf_vectorized_sentence)[0]\n",
        "\n",
        "\n",
        "# emotion_probabilities = {emotion_map[i]: prob for i, prob in enumerate(class_probabilities)}\n",
        "\n",
        "# # Output the probabilities for each class\n",
        "# for emotion, probability in emotion_probabilities.items():\n",
        "#     print(f\"Probability of {emotion}: {probability}\")\n",
        "\n",
        "# # Get the predicted emotion (the one with the highest probability)\n",
        "# # predicted_emotion = max(emotion_probabilities, key=emotion_probabilities.get)\n",
        "# print(\"Predicted emotion:\", emotion_map[predicted_emotion])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qltrLU3Moi5i",
        "outputId": "fc3b82c3-931a-45ce-fa79-17abf917113d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression\n"
          ]
        }
      ],
      "source": [
        "# save_model(logistic_regression,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoMw45F_oi5i"
      },
      "outputs": [],
      "source": [
        "with open('D:/College/Fourth Year/GP/glove_vectors.pkl', 'rb') as f:\n",
        "    glove_vectors = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCRLLFVKoi5i",
        "outputId": "612d3904-5867-4a69-8e3b-a71e6661fa05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with GloVe embeddings: 0.4904392764857881\n",
            "Classification Report with GloVe embeddings:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.47      0.63      6321\n",
            "           1       0.08      0.39      0.13       118\n",
            "           2       0.04      0.43      0.07        47\n",
            "           3       0.02      0.41      0.04        17\n",
            "           4       0.38      0.59      0.46      1019\n",
            "           5       0.08      0.52      0.14       102\n",
            "           6       0.09      0.60      0.16       116\n",
            "\n",
            "    accuracy                           0.49      7740\n",
            "   macro avg       0.23      0.49      0.23      7740\n",
            "weighted avg       0.82      0.49      0.58      7740\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Define your preprocessing function using GloVe embeddings\n",
        "# def preprocess_with_glove(sentence, glove_vectors):\n",
        "#     vectorized_sentence = []\n",
        "#     for word in sentence.split():\n",
        "#         if word in glove_vectors:\n",
        "#             vectorized_sentence.append(glove_vectors[word])\n",
        "#     return np.mean(vectorized_sentence, axis=0) if vectorized_sentence else np.zeros_like(list(glove_vectors.values())[0])\n",
        "\n",
        "# # Transform the training data using GloVe embeddings\n",
        "# X_train_glove = [preprocess_with_glove(sentence, glove_vectors) for sentence in flattened_X_train]\n",
        "# X_train_glove = np.vstack(X_train_glove)\n",
        "\n",
        "# # Transform the testing data using GloVe embeddings\n",
        "# X_test_glove = [preprocess_with_glove(sentence, glove_vectors) for sentence in flattened_X_test]\n",
        "# X_test_glove = np.vstack(X_test_glove)\n",
        "\n",
        "# # Define logistic regression model with class weights\n",
        "# logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
        "\n",
        "# # Fit logistic regression model\n",
        "# logistic_regression.fit(X_train_glove, y_train_flat)\n",
        "\n",
        "# # Predict on testing data\n",
        "# y_pred_glove = logistic_regression.predict(X_test_glove)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy_glove = accuracy_score(y_test_flat, y_pred_glove)\n",
        "# print(\"Accuracy with GloVe embeddings:\", accuracy_glove)\n",
        "\n",
        "# # Calculate classification report\n",
        "# report_glove = classification_report(y_test_flat, y_pred_glove)\n",
        "# print(\"Classification Report with GloVe embeddings:\")\n",
        "# print(report_glove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C0EZOr8oi5i",
        "outputId": "f6c9b221-086c-41cf-e8d3-a38a22aa9b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted label: sadness\n",
            "Probability of neutral: 0.18763722940773414\n",
            "Probability of anger: 0.030659054586403434\n",
            "Probability of disgust: 0.11225866478299383\n",
            "Probability of fear: 0.11656221603375616\n",
            "Probability of happiness: 0.15791877457306758\n",
            "Probability of sadness: 0.35241141926781683\n",
            "Probability of surprise: 0.042552641348227956\n"
          ]
        }
      ],
      "source": [
        "# # Example of a new sentence\n",
        "# new_sentence = \" I don't know. I guess I can try. I don't have anything to lose.\"\n",
        "\n",
        "# # Preprocess the new sentence using GloVe embeddings\n",
        "# preprocessed_sentence = preprocess_with_glove(new_sentence, glove_vectors)\n",
        "\n",
        "# # Reshape the preprocessed sentence to match the expected input shape\n",
        "# preprocessed_sentence = preprocessed_sentence.reshape(1, -1)\n",
        "\n",
        "# # Predict the label for the preprocessed sentence\n",
        "# predicted_label = logistic_regression.predict(preprocessed_sentence)[0]\n",
        "\n",
        "# # Predict the probabilities for each class for the preprocessed sentence\n",
        "# class_probabilities = logistic_regression.predict_proba(preprocessed_sentence)[0]\n",
        "\n",
        "# # Create a dictionary mapping emotions to probabilities\n",
        "# emotion_probabilities = {emotion_map[i]: prob for i, prob in enumerate(class_probabilities)}\n",
        "\n",
        "# # Output the predicted label\n",
        "# print(\"Predicted label:\", emotion_map[predicted_label])\n",
        "\n",
        "# # Output the probabilities for each class\n",
        "# for emotion, probability in emotion_probabilities.items():\n",
        "#     print(f\"Probability of {emotion}: {probability}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4q-YEWVoi5j",
        "outputId": "52841128-7182-443e-c383-3a9391c0060c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression_glv\n"
          ]
        }
      ],
      "source": [
        "# save_model(logistic_regression,\"D:/College/Fourth Year/GP/Soul-AI/models/mental_health_logistic_regression_glv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN85J140oi5j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
