{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the location for the data set and Glove for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder of the data set\n",
    "dataset_path = '../data/raw/suicidal_detection/'\n",
    "# The folder for the models\n",
    "model_path = '../models'\n",
    "# The folder where Glove is installed\n",
    "TORCHNLP_CACHEDIR = f'{model_path}/Glove/pytorch-nlp_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isTrain = True\n",
    "seed = 2\n",
    "data_path = f\"{dataset_path}/train/dialogues_train.txt\"\n",
    "train_preprocess = f\"{dataset_path}/train/dialogues_train_preprocess.pkl\"\n",
    "dev_preprocess = f\"{dataset_path}/validation/dialogues_validation_preprocess.pkl\"\n",
    "test_preprocess = f\"{dataset_path}/test/dialogues_test_preprocess.pkl\"\n",
    "model_save_path = f\"{model_path}/dailyDialog/model_test_new_code.pt\"\n",
    "batch_size = 16\n",
    "embedding_size = 300\n",
    "lstm_hidden_size = 500\n",
    "hidden_layer_size = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-nlp --quiet\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install pyspellchecker --quiet\n",
    "!pip install contractions --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install emoji --quiet\n",
    "!pip install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable, Dict\n",
    "import argparse\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, classification_report, confusion_matrix)\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "import contractions\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# define Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# define Glove\n",
    "pretrained_wv = GloVe(cache=TORCHNLP_CACHEDIR)\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_tuple(filename: str, data: tuple) -> None:\n",
    "    '''\n",
    "    Dump the tuple to a file.\n",
    "    :param filename: The name of the file to dump the tuple to.\n",
    "    :type filename: str\n",
    "    :param data: The tuple to dump.\n",
    "    :type data: tuple\n",
    "    '''\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "\n",
    "def load_tuple(filename: str) -> tuple:\n",
    "    '''\n",
    "    Load the tuple from the file.\n",
    "    :param filename: The name of the file to load the tuple from.\n",
    "    :type filename: str\n",
    "    :return: The loaded tuple.\n",
    "    :rtype: tuple\n",
    "    '''\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_sentence(sentence: str) -> str:\n",
    "    '''\n",
    "    Lowercase the sentence.\n",
    "    :param data: The sentence to lowercase.\n",
    "    :return: The lowercased sentence\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove emails from the sentence.\n",
    "    :param sentence: The sentence to remove emails from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without emails.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(r\"\\S*@\\S*\\s?\", \"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii_diacritic(sentence: str) -> str:\n",
    "    '''\n",
    "\n",
    "    Remove diacritics from the sentence.\n",
    "\n",
    "    :param sentence: The sentence to remove diacritics from.\n",
    "\n",
    "    :type sentence: str\n",
    "\n",
    "    :return: The sentence without diacritics.\n",
    "\n",
    "    :rtype: str\n",
    "    '''\n",
    "\n",
    "    return unicodedata.normalize(\"NFKD\", sentence).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove HTML tags from the sentence.\n",
    "    :param sentence: The sentence to remove HTML tags from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without HTML tags.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return BeautifulSoup(sentence, \"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeated_chars(sentence: str) -> str:\n",
    "    '''\n",
    "    Replace repeated characters in the sentence.\n",
    "    :param sentence: The sentence to replace repeated characters in.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with replaced repeated characters.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Replace consecutive occurrences of ',', '!', '.', and '?' with a single occurrence\n",
    "    return re.sub(r'([,!?.])\\1+', r'\\1', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emojis_to_text(sentence: str) -> str:\n",
    "    '''\n",
    "    Translate emojis in the sentence to text.\n",
    "    :param sentence: The sentence to translate emojis to text.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with translated emojis to text.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Translate emojis to text codes\n",
    "    translated_text = emoji.demojize(sentence)\n",
    "    # Remove colons from the translated text\n",
    "    translated_text = re.sub(r':', '', translated_text)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_sentence(sentence: str) -> str:\n",
    "    '''\n",
    "    Expand the contractions in the sentence.\n",
    "    :param sentence: The sentence to expand contractions in.\n",
    "    :type sentence: str\n",
    "    :return: The sentence with expanded contractions.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return contractions.fix(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove URLs from the sentence.\n",
    "    :param sentence: The sentence to remove URLs from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without URLs.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\", '', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_possessives(sentence: str) -> str:\n",
    "    '''\n",
    "    Strip possessives from the sentence.\n",
    "    :param sentence: The sentence to strip possessives from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without possessives.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    # Stripping the possessives\n",
    "    sentence = sentence.replace(\"'s\", '')\n",
    "    sentence = sentence.replace('’s', '')\n",
    "    sentence = sentence.replace('s’', 's')\n",
    "    sentence = sentence.replace(\"s'\", 's')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_space(sentence: str) -> str:\n",
    "    '''\n",
    "    Remove extra spaces from the sentence.\n",
    "    :param sentence: The sentence to remove extra spaces from.\n",
    "    :type sentence: str\n",
    "    :return: The sentence without extra spaces.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return re.sub(r'\\s+', ' ', sentence).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_spelling(sentence: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Check the spelling of the words in the sentence.\n",
    "    :param sentence: The sentence to check the spelling of.\n",
    "    :type sentence: list\n",
    "    :return: The sentence with corrected spelling.\n",
    "    :rtype: list\n",
    "    '''\n",
    "    spell = SpellChecker()\n",
    "    corrected_sentence = []\n",
    "    for word in sentence:\n",
    "        if word != '':\n",
    "            correction = spell.correction(word)\n",
    "            if correction is not None:\n",
    "                corrected_sentence.append(correction)\n",
    "            else:\n",
    "                corrected_sentence.append(word)\n",
    "        else:\n",
    "            corrected_sentence.append('')\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence: str) -> list[str]:\n",
    "    '''\n",
    "    Tokenize the sentence.\n",
    "    :param sentence: The sentence to tokenize.\n",
    "    :type sentence: str\n",
    "    :return: The tokenized sentence.\n",
    "    :rtype: str\n",
    "    '''\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Remove stop words from the sentence.\n",
    "    :param sentence: The sentence to remove stop words from.\n",
    "    :type sentence: list[str]\n",
    "    :return: The sentence without stop words.\n",
    "    :rtype: list[str]\n",
    "    '''\n",
    "    return [word for word in sentence if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_sentence(sentence: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Lemmatize the sentence.\n",
    "    :param sentence: The sentence to lemmatize.\n",
    "    :type sentence: list[str]\n",
    "    :return: The lemmatized sentence.\n",
    "    :rtype: list[str]\n",
    "    '''\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(sentence)\n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        # Map Penn Treebank POS tags to WordNet POS tags\n",
    "        if pos.startswith('N'):  # Nouns\n",
    "            pos = 'n'\n",
    "        elif pos.startswith('V'):  # Verbs\n",
    "            pos = 'v'\n",
    "        elif pos.startswith('J'):  # Adjectives\n",
    "            pos = 'a'\n",
    "        elif pos.startswith('R'):  # Adverbs\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            pos = 'n'  # Default to noun if POS tag not found\n",
    "\n",
    "        # Lemmatize the word using the appropriate POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemmatized_words.append(lemma)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train(line: str) -> list[str]:\n",
    "    '''\n",
    "    Clean the line and return it as a list of tokens\n",
    "    :param line: the line to clean\n",
    "    :type line: str\n",
    "    :return: the cleaned line as a list of tokens\n",
    "    :rtype: list\n",
    "    '''\n",
    "    # translate emojis\n",
    "    line = translate_emojis_to_text(line)\n",
    "    # lower the line\n",
    "    line = lower_sentence(line)\n",
    "    # remove non ascii\n",
    "    line = remove_nonascii_diacritic(line)\n",
    "    # remove emails\n",
    "    line = remove_emails(line)\n",
    "    # remove html\n",
    "    line = clean_html(line)\n",
    "    # remove urls\n",
    "    line = remove_url(line)\n",
    "    # replace repeated chars\n",
    "    line = replace_repeated_chars(line)\n",
    "    # expand\n",
    "    line = expand_sentence(line)\n",
    "    # remove possessives\n",
    "    line = remove_possessives(line)\n",
    "    # remove extra spaces\n",
    "    line = remove_extra_space(line)\n",
    "    # tekonize\n",
    "    line = tokenize_sentence(line)\n",
    "    # remove stopwords\n",
    "    line = remove_stop_words(line)\n",
    "    # lemmetization\n",
    "    line = lemm_sentence(line)\n",
    "    if len(line) == 0:\n",
    "        return ['Normal']\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line: str) -> list[str]:\n",
    "    '''\n",
    "    Clean the line and return it as a list of tokens\n",
    "    :param line: the line to clean\n",
    "    :type line: str\n",
    "    :return: the cleaned line as a list of tokens\n",
    "    :rtype: list\n",
    "    '''\n",
    "    # translate emojis\n",
    "    line = translate_emojis_to_text(line)\n",
    "    # lower the line\n",
    "    line = lower_sentence(line)\n",
    "    # remove non ascii\n",
    "    line = remove_nonascii_diacritic(line)\n",
    "    # remove emails\n",
    "    line = remove_emails(line)\n",
    "    # remove html\n",
    "    line = clean_html(line)\n",
    "    # remove urls\n",
    "    line = remove_url(line)\n",
    "    # replace repeated chars\n",
    "    line = replace_repeated_chars(line)\n",
    "    # expand\n",
    "    line = expand_sentence(line)\n",
    "    # remove possessives\n",
    "    line = remove_possessives(line)\n",
    "    # remove extra spaces\n",
    "    line = remove_extra_space(line)\n",
    "    # tekonize\n",
    "    line = tokenize_sentence(line)\n",
    "    # check spelling\n",
    "    line = check_sentence_spelling(line)\n",
    "    # remove stopwords\n",
    "    line = remove_stop_words(line)\n",
    "    # lemmetization\n",
    "    line = lemm_sentence(line)\n",
    "    if len(line) == 0:\n",
    "        return ['Normal']\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mapping from 's' to 1 and 'u' to 0\n",
    "forward_label_mapping = {'s': 1, 'u': 0}\n",
    "\n",
    "# define the reverse mapping from 0 to 'u' and 1 to 's'\n",
    "reverse_label_mapping = {0: 'u', 1: 's'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_suicidal_detection(data_path: str) -> Tuple[List[List[List[str]]], List[List[int]]]:\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
